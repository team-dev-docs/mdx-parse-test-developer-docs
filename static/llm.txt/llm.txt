Below is all the contents of our docs: 



 This is the content for the doc README.md 

 # developer-docs 🚧

Our open source documentation is under construction. Refer to our [website](https://www.speakeasy.com/docs) for up-to-date docs!


 This is the content for the doc api-design/caching.mdx 

 ---
description: Learn how to use HTTP caching to make APIs more efficient and sustainable.
---

# Caching API Responses

import { Callout } from "~/components";

API caching can save servers some serious work, cut down on costs, and even help reduce the carbon impact of an API. However, it is often considered an optimization rather than what it truly is: an integral part of API design. 

A fundamental part of REST is APIs declaring the "cacheability" of resources. When working with HTTP there are many amazing caching options available through HTTP Caching; a series of standards that power how the entire internet functions. This can be used to design more useful APIs, as well as being faster, cheaper, and more sustainable.

## What is HTTP caching?

HTTP caching tells API clients (like browsers, mobile apps, or other backend systems) if they need to ask for the same data over and over again, or if they can use data they already have. This is done with HTTP headers on responses that tell the client how long they can "hold onto" that response, or how to check if it's still valid.

This works very differently from server-side caching tools like Redis or Memcached, which cache data on the server. 

HTTP caching happens on client-side or on intermediary proxies like Content Delivery Networks (CDNs), acting as a proxy between the client and the server and storing responses for reuse whenever possible.

Think of server-side caching as a way to skip application work like database calls or outgoing HTTP requests, by fetching precomputed results from Redis or Memcached. HTTP caching reduces traffic and computational load further, by reducing the number of requests that even reach the server, and by reducing the number of responses that need to be generated.

## How does it work?

HTTP caching is driven by cache headers. In its most simple form, when an API sends a response, it includes instructions that tell the client and other network components like CDNs if they are allowed to cache the response, and if so for how long.

The guide on [API responses](/api-design/responses) briefly introduced the `Cache-Control` header:

```http
HTTP/2 200 OK
Content-Type: application/json
Cache-Control: public, max-age=18000

{
  "message": "I am cached for five minutes!"
}
```

Here the server is telling the client (and any cache proxies) that they can cache this response for 5 minutes, and they can share it with other clients too. This means that a client can use this data for up to 5 minutes without checking back with the server, and when that time has expired it will make a new request.

Fetching data, processing it, and sending it back to the client takes time and resources. Even when all of those processes are as optimized as possible, if the data hasn't changed, why bother repeating these requests? Instead of wasting resources answering the same requests over and over again, the server could be processing more useful requests, saving energy, and save money by scaling down unnecessary server capacity.

### Cache-Control

Defined in [RFC 9111: HTTP Caching](https://www.rfc-editor.org/rfc/rfc9111), this header sets out the rules. It tells clients what to do with the response:

- `Cache-Control: max-age=3600` — The client can use this data for up to an hour (3600 seconds) without checking with the server.
- `Cache-Control: no-cache` — The client must check with the server before using the cached copy.
- `Cache-Control: public` or `private` — Defines whether just the client or everyone (like proxies) can cache it.

These directives can be combined in various combinations for more control, with handy advanced options like `s-maxage` for setting how long data should live on shared caches like CDNs.

Some simple APIs will only use `Cache-Control` to manage caching, but there's another powerful tool in the cache toolbox: `ETag`.

### ETag

ETags (short for "Entity Tags") are like a fingerprint for a particular version or instance of a resource. When the resource changes, the ETag will change. No two versions of a resource should have the same ETag, and the ETag is unique to the URL of the resource.

When a server sends a response, it can include an ETag header to identify that version of the resource:

```http
HTTP/2 200 OK
Content-Type: application/json
ETag: "abc123"

{
  "message": "Hello, world!"
}
```

Then when a request is reattempted for whatever reason, the client sends a request with the ETag in the `If-None-Match` header. Doing this basically says "Only download the response if the ETag is different to this".

```http
GET /api/resource HTTP/2
If-None-Match: "abc123"
```

- If the server responds with `304 Not Modified`, it tells the client, "That response is still good. Nothing has changed since then, so no need to download it again."
- If the data has changed, the server returns the new data with a new ETag.

This is especially helpful for large responses that don't change often, especially when combined with `Cache-Control`. Sending `Cache-Control` and `ETag` lets the client confidently reuse the data for a while without even needing to send a HTTP request to the server, then after that time it can switch to doing a check for changes instead of downloading the whole response again.

All of this is done without the client needing to know anything about the data, or how it's stored, or how it's generated. The server will handle it all, and the client will just keep requesting the data, allowing the cache-aware HTTP client to do the heavy lifting.

## Using Cache-Control and ETags in code

Let's add these headers to a basic Express.js API to see how it might look on the server-side.

```js
const express = require('express');
const app = express();

app.get('/api/resource', (req, res) => {
    const data = { message: "Hello, world!" }; // Simulated data
    const eTag = `"${Buffer.from(JSON.stringify(data)).toString('base64')}"`;

    if (req.headers['if-none-match'] === eTag) {
        // Client has the latest version
        res.status(304).end();
    } else {
        // Serve the resource with cache headers
        res.set({
            'Cache-Control': 'max-age=3600', // Cache for 1 hour
            'ETag': eTag
        });
        res.json(data);
    }
});

app.listen(3000, () => console.log('API running on http://localhost:3000'));
```

The ETag is generated by hashing the data, then the server checks if the client has the latest version. If it does, it sends a `304 Not Modified` response, otherwise it sends the data with the `ETag` and `Cache-Control` headers.

In a real codebase, would be doing something like fetching from a datasource, or computing something that takes a while, so waiting for all of that to happen just to make an ETag is not ideal. Yes, it avoids turning that data in JSON and sending it over the wire, but if the API is going to ignore it and send an `304 Not Modified` header with no response, the data was loaded and hashed for no reason. 

Instead, an ETag can be made from metadata, like the last updated timestamp of a database record.

```js
const crypto = require('crypto');

function sha1(data) {
  const crypto.createHash('sha1').update(data).digest('hex');
}

const trip = Trips.get(1234);

const eTag = `"${sha1(trip.updated_at)}"`;
```

This example creates a SHA1 hash of the updated time, which will automatically change each time the record is updated. No need to specify the name of the Trip resource, or even mention the trip ID, because an ETag is unique to the URL and that is already a unique identifier.

When working with resources that have their own concept of versioning, why not use that version number as an ETag instead of creating one from something else.

```js
const trip = Trips.get(1234);

const eTag = `"${trip.version}"`;
```

```http
HTTP/2 200 OK
Content-Type: application/json
ETag: "v45.129"
```

Regardless, ETags are brilliant and easy to reconcile. If clients don't use them, it doesn't have any effect, but if they do use a HTTP client with [cache middleware](https://apisyouwonthate.com/blog/http-client-response-caching/) enabled then both the client and the server can save a lot of time and resources.

## Public, private, and shared caches

Using `Cache-Control` headers its possible to specify whether the response can be cached by everyone, just the client, or just shared caches. This is important for security and privacy reasons, as well as cache efficiency.

- `public` — The response can be cached by everyone, including CDNs.
- `private` — The response can only be cached by the client.
- `no-store` — The response can't be cached at all.

<Callout title="NOTE" variant="info">
  <p>
    When a response contains an `Authorization` header, it's automatically marked as `private` to prevent sensitive data from being cached by shared caches. This is another reason to use standard auth headers instead of using custom headers like `X-API-Key`.
  </p>
</Callout>

## Which resources should be cached?

Some people think none of the data in their API data is cacheable because "things might change." It's rare that all data is so prone to change that HTTP caching cannot help. All data is inherently out of date before the server has even finished sending it, but the question is how out of date is acceptable?

For example, a user profile is not likely to change particularly often, and how up to date does it really need to be? Just because one user changes their biography once in a year doesn't mean that all user profiles need to be fetched fresh on every single request. It could be cached for several hours, or even every day.

When talking about more real-time systems, one common example is a stock trading platform. In reality, most trading platforms publish a new public price every 15 minutes. A request to `/quotes/ICLN` might return a header like `Cache-Control: max-age=900`, indicating the data is valid for 900 seconds. Even when clients are "polling" every 30 seconds, the network cache will still be able to serve the response for 15 minutes, and the server will only need to respond to 1 in 30 requests.

Some resources might genuinely change every second, and depending on the traffic patterns network caching could still be helpful. If 1,000 users are accessing it simultaneously then network caching will help significantly reduce the load. Instead of responding to 1,000 individual requests per second, the system can reuse a single response per second. This would be a 99.9% reduction in server load, and a 99.9% reduction in bandwidth usage.

A safe default for most data is to apply some level of `max-age` caching (such as 5 minutes, an hour, a day, or a week, before it needs to be refreshed) paired with an ETag to check for fresh data past that time if the response is large or slow to generate. The introduction of ETags to an API can increase confidence in using longer cache expiry times.

## Designing cacheable resources

All new APIs should be designed with cachability in mind, which means thinking about how to structure resources to make them more cacheable. The changes needed to make an API more cacheable are often the same changes that make an API more efficient and easier to work with.

### Resource composition

One of the largest problems API designers face is how to sensibly group data into resources. There's a temptation to make fewer resources so that there are fewer endpoints, with less to document. However, this means larger resources, which become incredibly inefficient to work with (especially when some of the data is more prone to change than the rest).

```http
GET /invoices/645E79D9E14
```

```json
{
  "id": "645E79D9E14",
  "invoiceNumber": "INV-2024-001",
  "customer": "Acme Corporation",
  "amountDue": 500.00,
  "amountPaid": 250.00,
  "dateDue": "2024-08-15",
  "dateIssued": "2024-08-01",
  "datePaid": "2024-08-10",
  "items": [
    {
      "description": "Consulting Services",
      "quantity": 10,
      "unitPrice": 50.00,
      "total": 500.00
    }
  ],
  "customer": {
    "name": "Acme Corporation",
    "address": "123 Main St",
    "city": "Springfield",
    "state": "IL",
    "zip": "62701",
    "email": "acme@example.org",
    "phone": "555-123-4567"
  },
  "payments": [
    {
      "date": "2024-08-10",
      "amount": 250.00,
      "method": "Credit Card",
      "reference": "CC-1234"
    }
  ]
}
```

This is a very common pattern, but it's not very cacheable. If the invoice is updated, the whole invoice is updated, and the whole invoice needs to be refreshed. If the customer is updated, the whole invoice is updated, and the whole invoice needs to be refreshed. If the payments are updated, the whole invoice is updated, and the whole invoice needs to be refreshed. 

We can increase the cachability of most of this information by breaking it down into smaller resources:

```http
GET /invoices/645E79D9E14
```

```json
{
  "id": "645E79D9E14",
  "invoiceNumber": "INV-2024-001",
  "customer": "Acme Corporation",
  "amountDue": 500.00,
  "dateDue": "2024-08-15",
  "dateIssued": "2024-08-01",
  "items": [
    {
      "description": "Consulting Services",
      "quantity": 10,
      "unitPrice": 50.00,
      "total": 500.00
    }
  ],
  "links": {
    "self": "/invoices/645E79D9E14",
    "customer": "/customers/acme-corporation",
    "payments": "/invoices/645E79D9E14/payments"
  }
}
```

Instead of mixing in payment information with the invoice, this example moves the fields related to payment into the payments sub-collection. This is not only makes the invoice infinitely more cacheable, but it also makes space for features that are often used in an invoice system like payment attempts (track failed payments) or partial payments. All of that can be done in the Payments sub-collection, and each of those collections can be cached separately.

The customer data is also moved out of the invoice resource, because the `/customers/acme-corporation` resource already exists and reusing it avoids code duplication and maintenance burden. Considering the user flow of the application, the resource is likely already in the browser/client cache, which reduces load times for the invoice.

This API structure works regardless of what the data structure looks like. Perhaps all of the payment data are in an `invoices` SQL table, but still have `/invoices` and `/invoices/{id}/payments` endpoints. Over time as common extra functionality like partial payments is requested, these endpoints can remain the same, but the underlying database structure can be migrated to move payment-specific fields over to a `payments` database table. 

Many would argue this is a better separation of concerns, it's easier to control permissions for who is allowed to see invoices and/or payments, and the API has drastically improved cachability by splitting out frequently changing information from rarely changing information. 

### Avoid mixing public and private data

Breaking things down into smaller, more manageable resources can separate frequently changing information from more stable data, but there are other design issues that can effect cachability: mixing public and private data. 

Take the example of a train travel booking API. There could be a Booking resource, specific to a single user with private data nobody else should see. 

```http
GET /bookings/1234
```

```json
{
  "id": 1234,
  "departure": "2025-08-15T08:00:00",
  "arrival": "2025-08-15T12:00:00",
  "provider": "ACME Express",
  "seat": "A12"
}
```

In order for a user to pick their seat on the train, there could be a sub-resource for seating:

```http
GET /bookings/:my_booking_ref/seating
```

```
{
  "my_seat": "A12",
  "available_seats": [
    "A1", "A2", "A3", "A4", "A5", "A6", ...
  ]
}
```

Creating the seating sub-resource like this will make a unique seating chart for every single user, because "all the seats" and "this users seat" have been mixed together. These responses could still be cached, but it would have to be a `private` cache because the generic information has been "tainted" with data unique to each user. 10,000 users would have 10,000 cache entries, and the chance/impact of them being reused would be rather small, so there isn't much benefit to filling the entire cache with all this.

Consider breaking this down into two resources:

- `GET /bookings/:my_booking_ref` - See booking details, including current seat.
- `GET /trips/:trip_id/seats` - List seat availability on the train.
- `PUT /bookings/:my_booking_ref` - Update booking (eg to reserve a seat).

By moving the seat information to the booking resource, the seating availability becomes generic. With nothing personalized about it at all, the resource can be cached for everyone who is trying to book a seat on this train.

There is no downside to caching this data, because it is the same for everyone. Even if it changes, it's easy to grab the latest data from the server and suggest the user select another seat if it's no longer available. This allows the seat availability to be cached for a long time, and only worry about refreshing the plan if the `PUT` request fails because a seat is no longer available.

## Content Delivery Networks (CDNs)

HTTP caching works well when clients use it, and many do automatically, like web browsers or systems with caching middleware. But it becomes even more powerful when combined with tools like [Fastly](https://www.fastly.com/) or [Varnish](https://www.varnish-software.com/products/varnish-cache/). 

These tools sit between the server and the client, acting like intelligent gatekeepers:

![A sequence diagram showing a Client, Cache Proxy, and Server. A web request travels from client to proxy, then is sent on to the server, showing a "cache miss". The response then travels back from the server to the cache proxy, and then is sent to the client](./assets/httpcachemiss.png)

![A sequence diagram showing a Client, Cache Proxy, and Server. A web request travels from client to proxy, but does not go to the server, showing show a "cache hit". The response is served from the cache proxy to the client without involving the server](./assets/httpcachehit.png)

Client-caching like this is certainly useful, but the real power of caching comes when API web traffic is routed through a caching proxy. Using hosted solutions like Fastly or AWS CloudFront, this could be a case of changing DNS settings. For self-hosted options like Varnish, instead of pointing DNS settings to a hosted solution somebody will need to spin up a server to act as the cache proxy. 

Many API gateway tools like Tyk and Zuplo have caching built in, so this functionaity may already be available in the ecosystem and just need enabling.

## Save emissions (and money) with HTTP caching

The Internet (and it's infrastructure) is responsible for [4% of global CO2 emissions](https://www.bbc.com/future/article/20200305-why-your-internet-habits-are-not-as-clean-as-you-think), and with [83% of web traffic coming from APIs](https://www.akamai.com/newsroom/press-release/state-of-the-internet-security-retail-attacks-and-api-traffic), it becomes critical to consider the carbon impact of new APIs.

Each unnecessary API request costs server resources, bandwidth, and energy. That energy comes with a carbon footprint, whether it's from a data center powered by renewable energy or not.

## Summary

By reducing redundant requests, HTTP caching can:

- Cut down on server load (lowering hosting costs).
- Reduce network traffic (lowering bandwidth fees).
- Minimize energy consumption (a win for the environment).

Imagine millions of users no longer making unnecessary requests for unchanged data. Designing APIs to be cache-friendly from the start not only benefits the environment but also leads to faster, more efficient, and user-friendly APIs. It's a win-win: better performance for users, lower operational costs for providers, and a positive impact on the planet.

Next time a new API is being designed, ask the question: How much of this data do I really need to serve fresh each time, and how much of this can be cached with a combination of `Cache-Control` and `ETag` headers?

## Further Reading

- [MDN: HTTP Caching](https://developer.mozilla.org/en-US/docs/Web/HTTP/Caching)
- [ETags: What are they and how to use them?](https://www.fastly.com/blog/etags-what-they-are-and-how-to-use-them)
- [What is Cache Control?](https://www.fastly.com/blog/cache-control-wild)


 This is the content for the doc api-design/collections.md 

 ---
description: Learn how to use resources and collections for a REST API, getting stuck into some real world examples, using links to get between them all, and some pitfalls to avoid.
---

# Returning resources & collections of data

In the context of REST/HTTP APIs, a resource represents a specific piece of data or object that can be accessed via a unique URI (Uniform Resource Identifier). This could be anything: a user, a blog post, a product, or an order. Whereas a collection is a group of resources. It’s a list or set of all the items of a particular type.

## Structuring URLs for resources & collections

Retrieval of resources and collections both use the `GET` operation. Established convention is to have a unique base URL for each type of resource in an API: `/invoices`, `/transactions`, etc. 

To retrieve the entire collection of resources, a `GET` request is made to that URL: `GET /invoices`. 

```http
GET /invoices
```

```json
[
  {
    "id": "645E79D9E14",
    "invoiceNumber": "INV-2024-001",
    "customer": "Acme Corporation",
    "amountDue": 500.00,
    "dateDue": "2024-08-15"
  },
  {
    "id": "646D15F7838",
    "invoiceNumber": "INV-2024-002",
    "customer": "Monsters Inc.",
    "amountDue": 750.00,
    "dateDue": "2024-08-20"
  }
]
```

To retrieve a specific resource within that collection, a slightly different
endpoint exists: `GET /invoices/645E79D9E14`. In this case, the ID `645E79D9E14`
is a unique identifier for a specific invoice.

```http
GET /invoices/645E79D9E14
```

```json
{
  "id": "645E79D9E14",
  "invoiceNumber": "INV-2024-001",
  "customer": "Acme Corporation",
  "amountDue": 500.00,
  "dateDue": "2024-08-15",
  "dateIssued": "2024-08-01",
  "items": [
    {
      "description": "Consulting Services",
      "quantity": 10,
      "unitPrice": 50.00,
      "total": 500.00
    }
  ],
  "links": {
    "self": "/invoices/645E79D9E14",
    "customer": "/customers/acme-corporation",
    "payments": "/invoices/645E79D9E14/payments"
  }
}
```

The resource contains loads of data, including the customer name, an array of
items on the invoice, various dates, and how much of the invoice is left to be
paid.

It also has `"links"`, which can be related resources, collections, which could
be related data, or could be available actions foe the resource. For example, a
`"pay"` link which signals a payment can be made, a `"send"` link which helps
consumers send an invoice, or the example here which contains a `"payments"`
link to also allows for payments to be made, but also supports viewing a list of
partial and failed payments.

## What is a collection

A collection can be considered a special type of resource, which is a list of a
specific type of resource. For example, a collection of invoices, a collection
of products, or a collection of users.

Usually the API returns some basic information about each resource in the
collection, like title, id, and the sort of information an application might
want to show on a webpage showing the list. Then the links allow a client to
load up more data for each resource it's interested in.

## How do HTTP methods fit in

REST APIs typically use standard HTTP methods to interact with resources and
collections:

**GET:** Retrieve data.

- `/posts` - Get a collection of all blog posts.
- `/posts/abc1` - Get a single blog post by its ID.

**POST:** Create a new resource.

- `/posts` - Add a new blog post to the collection.

**PUT:** Replace an entire existing resource.

- `/posts/abc1` - Update the blog post with ID abc1.

**PATCH:** Update part an existing resource.

- `/posts/abc1` - Update the blog post with ID abc1.

**DELETE:** Remove a resource.

- `/posts/abc1` - Delete the blog post with ID abc1.

APIs are about a whole lot more than just CRUD, but when thinking about
collections and resources this is a simple way it really helps to think about
how these operations map to the data and actions available.

## Best practices

### URI structure

The structure of URIs in REST APIs is crucial for consistency and readability.
Here are some common conventions.

**Nouns over Verbs**: URIs typically use nouns (like /posts) rather than verbs
(like `/getPosts`), because HTTP methods (GET, POST, etc.) already imply the
action.

**Pluralization:** Collections are usually plural (e.g.: `/posts`), while
resources are identified with a unique identifier (e.g.: `/posts/abc1`).

**Minimal Data in Collections:** When retrieving a collection, APIs often return
minimal information about each resource to save bandwidth and speed up
responses. This allows consumers to quickly scan the collection and then retrieve more
detailed information if needed.

```http
GET /posts
```

``` json
[
    {
        "id": "abc1",
        "title": "Understanding REST APIs",
        "author": "Bob Doe",
        "link": "/posts/abc1"
    },
    {
        "id": "def2",
        "title": "Introduction to HTTP Methods",
        "author": "Sally Smith",
        "link": "/posts/def2"
    }
]
```

There's plenty of debate about how much detail to put in collections and how
much to put in resources, but the key is to keep it simple and consistent.

Putting everything in the collection would bloat the list view horrendously,
wasting time, money, and carbon emissions stressing the infrastructure passing
around massive JSON payloads with content that may not even be needed right now.

Trimming them down to a bare minimum could then force consumers to make an
unreasonable number of requests to get even the most basic data.

Some API designers go as far as putting no information at all in their
collections, because it can all be fetched directly from the resources. This
helps make the responses a lot more [cachable](/api-design/caching) because if
any of the data does change for any of the resources then the collections do not
need to be purged from the cache to maintain consistency.

```http
GET /posts
```

```json
[
    {
        "link": "/posts/abc1"
    },
    {
        "link": "/posts/def2"
    }
]
```

There is no one simple answer here, but using a bit of common sense and talking
to consumers about their use cases should usually help find the right balance.

In general, it's a sensible default to aim for a reasonable middle-ground, where
summary data is in the collection: name, ID, status, and a few key bits of
data that consumers are the most likely to need when they're building an index of data.

Then if consumers need more data, they can go fetch it, and with modern day
HTTP/2 & HTTP/3 this does not have as many performance burdens as it used to.
Especially when API caching is implemented with quality API design then slimming down collections can
even lead to better performance than trying to squash everything into the collection.

### Linking to related resources

Collections linking to resources is helpful, letting clients follow various
links throughout an API like a user browsing a website. Resources can link
to other related resources and collections, which might be data but could also
be considered "actions", all handled through the same conventions.

```http
GET /posts/abc1
```

```json
{
    "id": "abc1",
    "title": "Understanding REST APIs",
    "author": "Jane Doe",
    "content": "This is a detailed tutorial on REST APIs...",
    "datePublished": "2023-10-01",
    "links": {
        "self": "/posts/abc1",
        "author": "/authors/jane-doe",
        "comments": "/posts/abc1/comments"
    }
}
```

In this response:

- The `self` link points to the resource itself, like a canonical URL, which is
  a handy convention for knowing where something came from, whether that's a
  JSON blob that has been saved in a database without the headers, or providing
  one location to call back to if this was a temporary action which cannot be
  repeated. 

- The `author` link points to the resource representing the author of the post
  because it's quite likely clients will want to load that. Nobody will need to
  load every author for every post because HTTP caching will kick in, and makes
  no sense to squash that data into the post resource.

- The `comments` link points to a collection of comments related to this post if
  consumers want to load that, and any application loading that up is going to want to
  do it after it's got the post showing to users, so it doesn't matter if it
  loads later.

Splitting up API data into multiple endpoints that can be grabbed if needed is
really handy, upgrading a REST API from basically a set of functions which grab
some data, into an Object-Relational Mapping (ORM) where relationships can be
navigated easily, but going one step further and focusing on actions turns the API into 
essentially a state machine over HTTP.

## Don't confuse resource design & database design

A key aspect of API design is not tying API resources and collections directly
to the underlying database. Database needs to change and adapt rapidly as data
structures change, but APIs needs to evolve slowly (or not at all).

The more tied an API becomes to an internal database structure, the more they're
going to more often API consumers are going to have to rewrite their
applications.

**Normalization will change over time:** An invoice resource might contain a
`customer` object, even though it is in a separate database table. That could be
INNER JOIN'ed in the background (for those using SQL). Then if that query starts
to get really slow, the database could reduce the level of normalization and
bring that customer name directly into the `invoices` table (which is going to
help maintain proper historical accuracy if the customer changes their name).

**There could be pivot tables involved which don't need to be exposed:** Linking
tree planting `sites` to all of the tree `species` might involve a
`sites_species` database but that doesn't mean the API should have a
`/sites_species` table.

There's lots to think about, but the quick point here is to avoid letting
database design influence resource design too heavily. Clients should always
come first.

## Real-World Examples

**GitHub API**

When retrieving a list of repositories, each repository item includes a url
field that links to the full details of that repository.

```http
GET /users/octocat/repos
```

```json
[
  {
      "id": 1296269,
      "name": "Hello-World",
      "url": "https://api.github.com/repos/speakeasy-api/Hello-World"
  }
]
```

**Twitter API**

When retrieving a user's timeline, each tweet includes a URL that links to the specific tweet details.

```http
GET /statuses/user_timeline.json?screen_name=speakeasydev
```

```json
[
  {
      "created_at": "Wed Oct 10 20:19:24 +0000 2018",
      "id": 1050118621198921728,
      "text": "Just setting up my Twitter. #myfirstTweet",
      "url": "https://api.twitter.com/1.1/statuses/show/1050118621198921728.json"
  }
]
```

**Stripe API**

Stripe has a collection which is a bit different, instead of returning a JSON array directly in the response, it wraps it in an object with a data property:

```http
GET /v1/charges
```

```json
{
  "object": "list",
  "url": "/v1/charges",
  "has_more": false,
  "data": [
    {
      "id": "ch_3MmlLrLkdIwHu7ix0snN0B15",
      "object": "charge",
      "amount": 1099,
      "amount_captured": 1099,
      "amount_refunded": 0,
      "application": null,
      "application_fee": null,
      "application_fee_amount": null,
      "balance_transaction": "txn_3MmlLrLkdIwHu7ix0uke3Ezy",
      "billing_details": {
        "address": {
          "city": null,
          "country": null,
          "line1": null,
          "line2": null,
          "postal_code": null,
          "state": null
        },
        "email": null,
        "name": null,
        "phone": null
      },
      "calculated_statement_descriptor": "Stripe",
      "captured": true,
      "created": 1679090539,
      "currency": "usd",
      "customer": null,
      ... snip because its HUGE...
    }
    {...}
    {...}
  ],
}
```

They do this so they can add in various other bits of metadata, but much of this
metadata comes down to pagination which can be handled other ways (like popping
pagination into Links headers), so this practice is somewhat dying out.

## Best Practices

Returning resources and collections in a logical and consistent way is tough at
first, but there are standards and best practices that can help avoid common
mistakes.

## Using a "Data Envelope"

One common convention used by many popular APIs (like the Stripe example above)
is to wrap data in some sort of "envelope", which is a common term for putting
it into another object so there's a bit of room for metadata.

```json
{
  "data": [
    {
      "id": 123,
      "name": "High Wood",
      "lat": 50.4645697,
      "lon": -4.4865975
      "created_at": "2022-10-24T12:00:00Z"
    },
    {
      "id": 456,
      "name": "Goytre Hill",
      "lat": 52.1356114,
      "lon": -3.5975258
      "created_at": "2024-12-01T09:00:00Z"
    }
  ],
  "meta": {
    "rate-limit": 100,
    "next": "/places?page=2"
  }
}
```

This was really popular for a long time, but we don't need to do this anymore,
because most of that metadata would be better off in a response header.

The move to headers may in part be down to HTTP/2 adding [HPAK header
compression](https://blog.cloudflare.com/hpack-the-silent-killer-feature-of-http-2),
meaning it is more efficient to use headers for anything that's sensible to use
them for, and more standards are popping up to move these concepts out of custom
implementations in JSON and elsewhere, and move them into headers.

For example, instead of putting rate limiting data into `meta`, the [`RateLimit`
header](https://www.ietf.org/archive/id/draft-ietf-httpapi-ratelimit-headers-08.html),
can be used, and instead of putting `pagination` into the response, why not use
the `Links` header.

```http
HTTP/2 200 OK
Content-Type: application/json
Cache-Control: public, max-age=18000
RateLimit: "default";r=100;t=60
Link: <https://api.example.com/places?page=1&size=10>; rel="first",
      <https://api.example.com/places?page=3&size=10>; rel="next",
      <https://api.example.com/places?page=100&size=10>; rel="last"

[
  {
    "id": 123,
    "name": "High Wood",
    "lat": 50.4645697,
    "lon": -4.4865975
    "created_at": "2022-10-24T12:00:00Z"
  },
  {
    "id": 456,
    "name": "Goytre Hill",
    "lat": 52.1356114,
    "lon": -3.5975258
    "created_at": "2024-12-01T09:00:00Z"
  }
]
```

This probably looks easier to work with in some ways, and harder to work with in
some ways, but it's more performant, and any complexity can be deferred to
standard libraries and packages which handle all of this for API consumers
automatically.

## Data Format Standards

Instead of creating custom formats it may be easier for API developers and
consumers alike to use an existing "data format" standard.

- [CollectionJSON](http://amundsen.com/media-types/collection/format/)
- [HAL](http://stateless.co/hal_specification.html)
- [JSON:API](https://jsonapi.org/)
- [OData](https://www.odata.org/)
- [Siren](https://github.com/kevinswiber/siren)

Using any of these can avoid the "bikeshedding" (arguments about pros and cons
of each minor choice), and more importantly it will open the doors to more
standard tooling on both the client-side and server-side.

## Summary

**Use Consistent Naming:** Stick to conventions like using plural nouns for
collections. It shouldn't matter, but it drives people mad.

**Keep it Simple:** Start with basic endpoints and add complexity only when
necessary. It's easier to add things to an API if they're needed later, than
take them away once they're in production.

**API model is not a database model:** Do not try and recreate the database
model over HTTP because it will be a big waste of time, and be almost
immediately wrong anyway, which will make clients upset.


 This is the content for the doc api-design/data-formats.md 

 # Formatting API data

A request body (and a response body) will have a `Content-Type`, and that
content type will tell tools how the data is formatted so it can be converted
into something meaningful in whichever programming language is being used.

## JSON: The Modern Standard

JSON has become the de facto standard for API requests because it:

- Supports native data types (numbers, booleans, null)
- Allows nested structures
- Is human-readable
- Has excellent tooling support

Example of complex JSON request:

```json
{
  "place": {
    "name": "Central Park",
    "location": {
      "lat": 40.785091,
      "lon": -73.968285
    },
    "features": ["park", "landmark"],
    "isAccessible": true,
    "capacity": null
  }
}
```

## XML: Ye Oldé Standard

Any modern API will support JSON. Occasionally they will support XML as well.

XML is relict of the early internet. It dominated web APIs in the 2000s with
standards like SOAP and XML-RPC, but was largely displaced by JSON in the 2010s
due to JSON's simplicity and natural fit with JavaScript. Today, XML persists
mainly in legacy systems, enterprise SOAP services, and specific domains like
publishing (DocBook), feed syndication (RSS/Atom), and configuration files
(Maven, Android manifests).

JSON is a lot easier to work with than XML, and it is a lot easier to read. It
is also more compact, which is important when sending data over the wire.

An example of a bunch of different data types in JSON.

```json
{
  "place": {
    "id": 1,
    "name": "This is a bunch of text.",
    "is_true": false,
    "maybe": null,
    "empty_string": ""
  }
}
```

This same data in XML.

```xml
<places>
    <place>
        <id>1</id>,
        <name>This is a bunch of text.</name>
        <is_true>0</is_true>
        <maybe />
        <empty_string />
    </place>
</places>
```

Basically, in XML, _everything_ is considered a string, meaning integers,
booleans, and nulls can be confused. Both `<maybe />` and `<empty_string />`
have the same value, because there is no way to denote a `null` value either.
Gross.

## Form Data: Legacy Format

Form Data uses the `application/x-www-form-urlencoded` mime type, and is helpful
when accepting web forms from a browser using the `<form>` HTML tag. This was
very popular decades ago, but with modern web applications using more
single-page applications (SPAs) and mobile apps to speak JSON natively, it is
something most people just don't bother with anymore.

It's not just that it's old, it's cumbersome to work with, and suffers from a
lack of data types like XML but with even more awkward syntax.

Everything is a string. To handle a boolean a client has to send `1` or `0`,
which will be read as `"1"` or `"0"`. Some would suggest sending `property=true`, but that is
a literal `"true"` string which can be confusing to deal with.

```http
POST /checkins HTTP/1.1
Host: api.example.org
Content-Type: application/x-www-form-urlencoded

place_id=1&message=This%20is%20a%20bunch%20of%20text.&with_friends[]=1&with_friends[]=2&with_friends[]=3
```

This is a bit of a mess, as the message needs to be "URL encoded" and the
`with_friends` is an array with awkward syntax. On top of that it's not clear
what the data types are. It is also a bit of a pain to work with on the
server-side, as the data needs to be parsed and spit up properly, then converted
to the correct data types.

For comparison, the same data in JSON is a lot easier to work with.

```http
POST /checkins HTTP/1.1
Host: api.example.org
Content-Type: application/json

{
  "place_id": 1,
  "message": "This is a bunch of text.",
  "with_friends": [1, 2, 3]
}
```

This is a JSON object, and it is easy to see what is going on. The `place_id` is
an integer, the `message` is a string, and `with_friends` is an array of
integers.

## Multipart Form Data: An Occasionally Helpful Nightmare


Multipart forms are a way to send data in multiple parts as a single HTTP request, often used in REST APIs for handling mixed types of data, such as JSON and binary files (e.g., images or documents). Unlike standard form submission, where data is encoded as application/x-www-form-urlencoded, multipart forms use the multipart/form-data encoding, which allows for the inclusion of both text and file content in the same request.

This is particularly useful for endpoints that need to process metadata (e.g., JSON) alongside uploaded files. Each part of the form is separated by a boundary string and includes headers that describe the content type and disposition of the part.

```http
POST /checkins HTTP/1.1
Host: api.example.org
Content-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW

------WebKitFormBoundary7MA4YWxkTrZu0gW
Content-Disposition: form-data; name="metadata"
Content-Type: application/json

{
  "place_id": 1,
  "message": "This is a bunch of text.",
  "with_friends": [1, 2, 3]
}
------WebKitFormBoundary7MA4YWxkTrZu0gW
Content-Disposition: form-data; name="file"; filename="example.jpg"
Content-Type: image/jpeg

[Binary data of the image file]
------WebKitFormBoundary7MA4YWxkTrZu0gW--
```

This is either confusing or brilliant depending on perspective, but it's
generally a massive pain to work with. 


## Best Practices

### 1. Stick to JSON wherever possible

Work out which content type (or types) are actually needed, and _stick to that_.
95% of the time, that's JSON.

Some want to add CSV or HTML "just in case", and others want to add all the fun
new formats like BSON or MessagePack because they're "quicker" (without doing
basic optimizations on their code/database which would likely yield more
meaningful performance gains). That might be a bit of fun, but it's all adding a
maintenance burden and expecting too much of clients.

Start with JSON and wait for a big client to ask for a specific format, then
weigh it up against the cost of supporting it.

### 2. Avoid multipart forms

There are a few reasons to avoid this. It's hard to document, weird to handle
partial errors, and generally confuses beginners trying to work with an API. 

An SDK can hide some of the complexity, but that won't solve the awkward race
conditions that pop up creating something from the first "part", then the second
or third part fails, rolling back database transactions after emails have
already gone out. 

Designing an API for the least experienced user is not necessarily the goal, but
making things unnecessarily complex isn't the plan either, so stick with "one
endpoint does one thing". File uploads can be handled with other more targeted
endpoints.


 This is the content for the doc api-design/errors.mdx 

 ---
description: "Design useful API errors to save clients time."
---

import { Callout } from "~/components";

# Returning informative API Errors

When building an API it's natural to put most of the focus into building a
beautiful "happy path" where nothing goes wrong. Developers often don't like to
consider the failure cases, because of course everything is going to work out
just fine, so errors are often not designed with the same care as the rest of
the API.

Errors in an API are not just an edge-case, they are a crucial part of the
functionality, and should be treated like a core feature to be proudly shared
and documented with users. Failing clearly and concisely is arguably more
important than any other aspect of API design.

Errors should:

- Be as detailed as possible.
- Provide context as to exactly what went wrong, and why.
- Help humans find out more information about the problem.
- Help computers decide what to do next.
- Be consistent across the API.

## HTTP Status Codes

The journey to great errors starts with [status
codes](/api-design/status-codes). Status code conventions exist to specify what
category of error has occurred, and they are a great way to help developers
make decisions automatically based on the status code, like automatically
refreshing access tokens on a `403`, or retrying the request on a `500`.

Learn more about [HTTP Status Codes](/api-design/status-codes), and how to use
them effectively.

## Application Errors

HTTP status codes only set the scene for the category of issue that has
occurred. An error like `400 Bad Request` is generally used as a vague catch-all
error that covers a whole range of potential issues. 

More information will be required to help developers understand what went wrong,
and how to fix it, without having to dig through logs or contact the support
team. 

Error details are useful for:

1. humans - so that the developer building the integration can understand the issue.
2. software - so that client applications can automatically handle more situations correctly.

Imagine building a carpooling app, where the user plans a trip between two
locations. What happens if the user inputs coordinates that which are not
possible to drive between, say England and Iceland? Below is a series of
responses from the API with increasing precision:

```http
HTTP/1.1 400 Bad Request
```

A not very helpful error response, the user will have no idea what they did
incorrectly.

```http
HTTP/1.1 400 Bad Request

"error": {
    "message": "Trip is not possible, please check start/stop coordinates and try again."
}
```

This message could be passed back to the user which will allow them to figure
out how to address the issue, but it would be very difficult for an application
to programmatically determine what issue occurred and how to respond.

```http
HTTP/1.1 400 Bad Request

"error": {
    "code": "trip_not_possible",
    "message": "Trip is not possible, please check start/stop coordinates and try again."
}
```

Now this includes data that can help our users know what's going on, as well as
an error code which let's them handle the error programmatically if they would
like to.

So, we should always include both API error messages, as well as API error
codes. Let's take a closer look at the best practices for each of these.

## API error messages

API error messages should be clear, concise, and actionable. They should provide
enough information for the developer to understand what went wrong, and how to
fix it.

Here are a few best practices for API error messages:

- **Be Specific**: The error message should clearly explain what went wrong.
- **Be Human-Readable**: The error message should be easy to understand.
- **Be Actionable**: The error message should provide guidance on how to fix the issue.
- **Be Consistent**: Error messages should follow a consistent format across the API.

## API error codes

The use of an error code is well established in the API ecosystem. However,
unlike status codes, error codes are specific to an API or organization. That
said, there are conventions to follow to give error codes a predictable
format.

![Screenshot of Stripe.com API documentation's "Error Codes" page, which explains how "error codes" are added to provide extra information on top of HTTP status codes.](./assets/stripe-error-codes.png)

Stripe's error codes have a nice easy to understand structure. Each error has a
code which is a string, and a message which is a string, and that string is
documented online so it can be understood, or reported to support.

```http
HTTP/1.1 400 Bad Request

{
  "error": {
    "code": "trip_too_short",
    "message": "This trip does not meet the minium threshold for a carpool or 2 kilometers (1.24 miles)."
  }
}
```

This makes it easy for developers to react programatically to the error too:

```typescript
if (error.code === 'trip_too_short')
```

## Complete Error Objects

Include a `code` and a `message` puts an error message off to a great start, but
there's more to be done to turn errors into a handy feature instead of just a
red flag.

Here's the full list of what an API error should include:

- **Status Code**: Indicating the general category of the error (4xx for client errors, 5xx for server errors).
- **Short Summary**: A brief, human-readable summary of the issue (e.g., "Cannot checkout with an empty shopping cart").
- **Detailed Message**: A more detailed description that offers additional context (e.g., "It looks like you have tried to check out but there is nothing in your cart").
- **Application-Specific Error Code**: A unique code that helps developers programmatically handle the error (e.g., `cart-empty`, `ERRCARTEMPTY`).
- **Links to Documentation**: Providing a URL where users or developers can find more information or troubleshooting steps.

Some folks will build their own custom format for this, but let's leave that to
the professionals and use existing standards: [RFC 9457 - Problem Details for
HTTP APIs](https://www.rfc-editor.org/rfc/rfc9457.html). This is being used by
more and more API teams.

```json
{
  "type": "https://signatureapi.com/docs/v1/errors/invalid-api-key",
  "title": "Invalid API Key",
  "status": 401,
  "detail": "Please provide a valid API key in the X-Api-Key header."
}
```

This example of an error from the [Signature
API](https://signatureapi.com/docs/errors) includes a `type`, which is basically
the same as an error code, but instead of an arbitrary string like
`invalid-api-key` the standard suggests a URI which is unique to the API (or
ecosystem): `https://signatureapi.com/docs/v1/errors/invalid-api-key`. This does
not have to resolve to anything (doesn't need to go anywhere if someone loads it
up) but it _can_, and that covers the "link to documentation" requirement too.

![API Documentation for the SignatureAPI, with an explanation of what the error is, what happened, and how to fix it](./assets/errors-documentation.png)

Why have both a `title` and a `description`? This allows the error to be used in
a web interface, where certain errors are caught and handled internally, but
other errors are passed on to the user to help errors be considered as
functionality instead of just "Something went wrong, erm, maybe try again or
phone us". This can reduce incoming support requests, and allow applications to
evolve better when handling unknown problems before the interface can be
updated.

Here's a more complete usage including some optional bits of the standard and
some extensions.

```json
HTTP/1.1 403 Forbidden
Content-Type: application/problem+json

{
 "type": "https://example.com/probs/out-of-credit",
 "title": "Not enough credit.",
 "detail": "The current balance is 30, but that costs 50.",
 "instance": "/account/12345/msgs/abc",
 "balance": 30,
 "accounts": ["/account/12345", "/account/67890"]
}
```

This example shows the same `type`, `title`, and `detail`, but has extra bits.

The `instance` field allows the server to point to a specific resource (or endpoint)
which the error is relating to. Again URI could resolve (it's a relative path to
the API), or it could just be something that does not necessarily exist on the
API but makes sense to the API, allowing clients/users to report a specific instance
of a problem with more information that "it didn't work...?".

The `balance` and `account` fields are not described by the specification, they
are "extensions", which can be extra data which helps the client application
report the problem back to the user. This is extra helpful if they would rather
use the variables to produce their own error messages instead of directly
inserting the strings from `title` and `details`, opening up more options for
customization and internationalization.

## Best Practices

Handling errors in API design is about more than just choosing the right HTTP
status code. It's about providing clear, actionable information that both
developers, applications, and end-users of those applications can understand and
act upon.

Here are a few more things to think about when designing errors.

### 200 OK and Error Code

HTTP 4XX or 5XX codes alert the client, monitoring systems, caching systems, and
all sorts of other network components that something bad happened.

**The folks over at CommitStrip.com know what's up.**

![This monster has got his API responding with HTTP Status 200 OK despite the request failing.](./assets/errors-200-ok.jpeg)

Returning an HTTP status code of 200 with an error code confuses every single
developer and every single HTTP standards-based tool that may ever come into
contact with this API. now or in the future.

Some folks want to consider HTTP as a "dumb pipe" that purely exists to move data up and
down, and part of that thinking suggests that so long as the HTTP API was able to respond then thats a 200 OK.

This is fundamentally problematic, but the biggest issue is that it delegates
all of the work of detecting success or failure to the client code. Caching tools will cache the error. Monitoring tools 
will not know there was a problem. Everything will look absolutely fine despite mystery weirdness happening throughout the system. Don't do this!

### Single or Multiple Errors?

Should an API return a single error for a response, or multiple errors?

Some folks want to return multiple errors, because the idea of having to fix one
thing, send a request, fail again, fix another thing, maybe fail again, etc.
seems like a tedious process.

This usually comes down to a definition of what an error is. Absolutely, it
would be super annoying for a client to get one response with an error saying
"that email is in a bad format" and then when they resubmit they get another
error with "the name you sent has unsupported characters". Both those validation
messages could have been sent at once, but an API doesn't need multiple errors
to do that.

The error there is that "the resource is invalid", and that can be a single
error. The validation messages are just extra information added to that single
error.

```json
{
  "type": "https://example.com/probs/invalid-payload",
  "title": "The payload is invalid",
  "details": "The payload has one or more validation errors, please fix them and try again.",
  "validation": [
    {
      "message": "Email address is not properly formatted",
      "field": "email"
    },
    {
      "message": "Name contains unsupported characters",
      "field": "name"
    }
  ]
}
```

This method is preferred because it's impossible to preempt things that might go
wrong in a part of the code which has not had a chance to execute yet. For
instance, that email address might be valid, but the email server is down, or
the name might be valid, but the database is down, or the email address is
already registered, all of which are different types of error with different
status codes, messages, and links to documentation to help solve each of them
where possible.

### Custom or standard error formats

When it comes to standards for error formats, there are two main contenders:

**RFC 9457 - Problem Details for HTTP APIs**

The latest and greatest standard for HTTP error messages. There only reason not
to use this standard is not knowing about it. It is technically new, released in
2023, but is replacing the RFC 7807 from 2016 which is pretty much the same
thing.

It has a lot of good ideas, and it's being adopted by more and more
tooling, either through web application frameworks directly, or as "middlewares"
or other extensions.

This helps avoid reinventing the wheel, and it's strongly recommended to use it
if possible.

**JSON:API Errors**

[JSON:API](https://jsonapi.org/) is not so much a standard, but a popular
specification used throughout the late 2010s. It focuses on providing a common
response format for resources, collections, and relationships, but it also has a
decent [error format](https://jsonapi.org/format/#errors) which a lot of people
like to replicate even if they're not using the entire specification.

**Pick One**

There has been a long-standing stalemate scenario where people do not implement
standard formats until they see buy-in from a majority of the API community, or
wait for a large company to champion it, but seeing as everyone is waiting for
everyone else to go first nobody does anything. The end result of this is
everyone rolling their own solutions, making a standard less popular, and the
vicious cycle continues.

Many large companies are able to ignore these standards because they can create
their own effective internal standards, and have enough people around with
enough experience to avoid a lot of the common problems around.

Smaller teams that are not in this privileged position can benefit from
deferring to standards written by people who have more context on the task at
hand. Companies the size of Facebook can roll their own error format and brute
force their decisions into everyones lives with no pushback, but everyone on
smaller teams should stick to using simple standards like RFC 9457 to keep
tooling interoperable and avoid reinventing the wheel.

### Retry-After

API designers want their API to be as usable as possbile, so whenever it makes
sense, let consumers know when and if they should come back and try again., and if so, when. The
`Retry-After` header is a great way to do this.

```http
HTTP/1.1 429 Too Many Requests
Retry-After: 120
```

This tells the client to wait two minutes before trying again. This can be a
timestamp, or a number of seconds, and it can be a good way to avoid a client
bombarding your API with requests when it's already struggling.

Learn more about [Retry-After on MDN](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Retry-After).

<Callout title="NOTE" variant="info">
  <p>
    Help your API consumers out by{" "}
    <a href="/docs/runtime/retries">enabling retry logic</a> in your Speakeasy
    SDK.
  </p>
</Callout>


 This is the content for the doc api-design/file-uploads.mdx 

 ---
description: > 
  Learn how to design file uploads in an API.
---

# File Uploads

File uploads can be confusing to work with at first because it takes a bit of a
mental shift to think about. 

Firstly, a file is usually not just a file, it also has metadata needs to go
with it and that can be hard to keep track of.

Secondly, it is not really a file upload, simply a resource or collection of
resources with a `Content-Type` of something other than the usual JSON or XML.

## URL design 

To visualize how file uploads could be designed into an API, let's see how
images could be added for two different use-cases.

A user could have an avatar sub-resource, which might look like this:

```
/users/<username>/avatar
```

This can then be uploaded and retrieved on the same URL, making it a consistent
API experience with any other type of resource. 

Multiple images could be needed for product thumbnails, and that can be a
sub-collection of the product.

```
/product/<uuid>/thumbnails
```

A collection of resources could be available, and a particular thumbnail could
be retrieved or deleted using regular semantics like `GET` and `DELETE` on the
particular resource URL.

```
/product/<uuid>/thumbnails/<image-uuid>
```

## POST or PUT

There is no particular [HTTP method](/api-design/http-methods) specific to file
uploads, instead we use the appropriate hTTP method for the resource or
collection being worked with.

For the example of a single avatar for each user, the URL is already known, and
it does not make any difference whether this is the first avatar they have
uploaded, or they have remade the same request 10 times in a row after an
intermitted internet connection messed up the first few. This should use a
`PUT`, because that means "The end result should be this, regardless of what is
there right now."

```
PUT /users/<username>/avatar
```

When working with a collection, the URL of the resource is not known until it
has been created. For this reason a `POST` would be more appropriate.

```
POST /product/<uuid>/thumbnails
```

How these uploads work could vary depending on the use case, so let's look at
the most popular methods.

## Different methods of file upload

There are a few popular approaches to file uploads in APIs:

1. Uploading a file by itself, like adding an avatar for an existing user.
2. Uploading a file with metadata in the same request, like a video file with a title, description, and geodata.
3. Importing a file from a URL, like a user's avatar from Facebook.

It's not entirely unreasonable to consider an API using all of these approaches
for different use cases throughout the API depending on the specifics. Lets
learn how these things work, and talk about when to use one over the other.

### Method A: Direct file uploads

When no metadata is needed to be uploaded with a request, a direct file upload
is beautifully simple.

- Uploading a CSV of emails being imported to send a tree sponsorship email to.
- A new logo for a funding partner.
- A replacement avatar for a user profile.

In all of these situations, the file is the only thing that needs to be uploaded
and they also have a handy content type that can go right into the HTTP request
to let the API know what's coming.

```http
PUT /users/philsturgeon/image HTTP/2
Authentication: Bearer <token>
Content-Type: image/jpeg
Content-Length: 284

<raw image content>
```

Any file can be uploaded this way, and the API can infer the content type from
the `Content-Type` header. The API can also infer the user from the token, so 
the request does not need to include any user information.

The API will then save the file, and return a response with a URL to the file
that was uploaded. This URL can be used to access the file in the future, and
can be used to link the file to the user that uploaded it.

The response here will have a simple body:

```json
{
  "url": "https://cdn.example.org/users/philsturgeon.jpg",
  "links": {
    "self": "https://example.org/api/images/c19568b4-77b3-4442-8278-4f93c0dd078",
    "user": "https://example.org/api/users/philsturgeon"
  }
}
```

That `user` was inferred from the token, and the `url` is the resulting URL to
the avatar that has been uploaded. Normally this would be some sort of Content
Delivery Network (CDN) URL, but it could be a direct-to-S3 URL, or a URL to a Go
service that handles file uploads. It's up to you, but its good to split off
file uploads to a separate service to keep your API servers free to do more
impactful work than serving files.

### Method B: Upload from URL

Depending on how the client application works, uploading from a file might not
be the preferred approach. A common pattern is mobile clients uploading user
images directly from the photo libraries on the mobile device, and the web teams
were pulling avatars from Facebook or Twitter profiles after they have done a
"social login" flow.

This is common because its harder for the web application to access the raw
content of a file using just browser-based JavaScript. At some point a server
needs to be involved to read that, so whether they have uploaded via cloudinary
or some other upload service, the API server is going to need to take a URL and
download the file.

The same endpoint that handled the direct upload can serve this same logic, with
the `Content-Type` header changed to `application/json` and the body of the
request containing a URL to the file.

```http
PUT /users/philsturgeon/image HTTP/2
Authentication: Bearer <token>
Content-Type: application/json

{
  "url" : "https://facebook.com/images/dfidsyfsudf.png"
}
```

The API will then download the file from the URL, save it, and return a response
with a URL to the file that was uploaded. This URL can be used to access the file
in the future, and can be used to link the file to the user that uploaded it.

```json
{
  "url": "https://cdn.example.org/users/philsturgeon.jpg",
  "links": {
    "self": "https://example.org/api/images/c19568b4-77b3-4442-8278-4f93c0dd078",
    "user": "https://example.org/api/users/philsturgeon"
  }
}
```

Supporting both might not be necessary, but if they are, just support both the
image types you need and the JSON alternative of that. HTTP makes that
incredibly easy to do thanks to being able to switch `Content-Type`.

### Method 3: Separate metadata resource

The above examples are great for simple file uploads, but what if you need to
upload metadata with the file? This is where things get a bit more complex. 

One approach would be multipart forms, but they're pretty complex to work with
and not ideal for large files. If sending a massive video file, you don't want
to have to send the title, description, and tags in the same request as the
video file. If the video file upload fails, you'll have to re-upload the video
file and all of the metadata again.

The way YouTube handles uploads via API are an interesting examples of splitting
out metadata and a video file. They use a two-step process which focuses on
metadata first, which allows for the metadata to be saved and the video can then
be retried and uploaded without losing the metadata.

The YouTube Data API (v3) approach to [Resumable
Uploads](https://developers.google.com/youtube/v3/guides/using_resumable_upload_protocol)
works like this.

First, they make a POST request to the video upload endpoint with the metadata
in the body of the request:

```http
POST /upload/youtube/v3/videos?uploadType=resumable&part=snippet,status HTTP/1.1
Host: www.googleapis.com
Authorization: Bearer <token>
Content-Length: 278
Content-Type: application/json; charset=UTF-8

{
  "snippet": {
    "title": "My video title",
    "description": "This is a description of my video",
    "tags": ["cool", "video", "more keywords"],
    "categoryId": 22
  },
  "status": {
    "privacyStatus": "public",
    "embeddable": true,
    "license": "youtube"
  }
}
```

The response then contains a `Location` header with a URL to the video upload endpoint:

```http
HTTP/1.1 200 OK
Location: https://www.googleapis.com/upload/youtube/v3/videos?uploadType=resumable&upload_id=xa298sd_f&part=snippet,status,contentDetails
Content-Length: 0
```

Then to upload the video it's back to direct file uploads. The video file can be
uploaded to the URL provided in the `Location` header, with the content type set
to `video/*`:

```http
PUT https://www.googleapis.com/upload/youtube/v3/videos?uploadType=resumable&upload_id=xa298sd_f&part=snippet,status,contentDetails HTTP/1.1
Authorization: Bearer AUTH_TOKEN
Content-Length: <file length>
Content-Type: video/mp4

<BINARY_FILE_DATA>
```

What's cool about this approach, is that URL _could_ be part of your main API,
or it _could_ be a totally different service. It could be a direct-to-S3 URL,
Cloudinary, or some other service that handles file uploads. 

Larger companies will be more prone to building a service to handle such files
coming in, whilst smaller teams might want to keep things simple and let their
API do the heavy lifting. The larger the file, the more likely you'll want to
split that off, as having your API handle these huge files - even if the uploads
are chunked - will keep the HTTP workers busy. Maintaining those connections
might slow down a Rails-based API for a long time, for example, so having
another service would help there.

## Best practices

### Check Content-Type and Content-Length

It is worth noting that the `Content-Type` header is not always reliable, and
you should not trust it. If you're expecting an image, you should check the
first few bytes of the file to see if it is a valid image format. If you're
expecting a CSV, you should check the first few lines to see if it is a valid
CSV. **Never trust input.**

The only thing worth mentioning on that request is the addition of
`Content-Length`, which is basically the size of the image being uploaded. A
quick check of `headers['Content-Length'].to_i > 3.megabytes` will let us
quickly reply saying "This image is too large", which is better than waiting
forever to say that. Sure, malicious folks could lie here, so your backend code
will need to check the image size too. **Never trust input.**

Protecting against large files is important, as it can be a denial of service
attack. If you allow users to upload files, they could upload a 10GB file and
fill up your disk space. This is why it's important to check the size of the
file before writing it to disk. 

To make sure it seems to be the right type, and to make sure it's not too large,
you can read the file in chunks. This can be done with a simple `File.open` and
`File.read` in Ruby, or similar in other languages. The file is read in chunks,
and then written to a file on disk. This is a good way to handle large files, as
you're not trying to load the whole file into memory at once.

```ruby
def update
  if headers['Content-Type'] != 'image/jpeg'
    render json: { error: 'Invalid content type' }, status: 400
    return
  end

  if headers['Content-Length'].to_i > 3.megabytes
    render json: { error: 'File is too large' }, status: 400
    return
  end

  file = File.open("tmp/#{SecureRandom.uuid}.jpg", 'wb') do |f|
    f.write(request.body.read)
  end

  # Do something with the file
end
```

### Securing File Uploads

Allowing file uploads can introduce all sorts of new attack vectors, so it's worth being very careful about the whole thing. 

One of the main issues with file uploads is directory traversal attacks. If you allow users to upload files, they could upload a file with a name like `../../etc/passwd`, which could allow them to read sensitive files on your server.

Uploading from a URL could allow for [Server-Side Request Forgery (SSRF)](https://owasp.org/API-Security/editions/2023/en/0xa7-server-side-request-forgery/) attacks, where an attacker could upload a file from a URL that points to a sensitive internal resource, like an AWS metadata URL, or something like `localhost:8080` which allows them to scan for ports on the server.

The [OWASP File Upload Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/File_Upload_Cheat_Sheet.html) has a lot of good advice on how to secure file uploads, including:

- Limiting the types of files that can be uploaded.
- Limiting the size of files that can be uploaded.
- Storing files in a location that is not accessible via the web server.
- Renaming files to prevent directory traversal attacks.
- Checking the file type by reading the first few bytes of the file.
- Checking the file size before writing it to disk.
- Checking the file for viruses using a virus scanner.

## Summary

Think about what sort of file uploads are needed, how big the files are, where
they're going, and what sort of clients will be using the API. 

The YouTube approach is a bit complex, but a combination of 1 and 2 usually take care of the
job, and help avoid complicated multipart uploads. 

As always, build defensively, and never trust any user input at any point. 


 This is the content for the doc api-design/filtering-responses.mdx 

 ---
description: Learn how to filter collections in a REST API.
---

# Filtering Collections

When building a REST API, the ability to filter potentially large collections of
data is essential, and sorting can reduce a huge amount of work for both the
client and server.

## What is Filtering?

Filtering allows API users to request only the data they need, "filtering out"
irrelevant things on the server-side instead of making them do it themselves on
the client-side. Reducing the amount of data you're returning and transferring
reduces server resources and improves performance, which reduces carbon
emissions and saves money.

## How to Filter

The most straightforward way to filter resources is by using query parameters.
These are appended to the URL to refine the results of an API request. For
example:

```bash
GET /products?category=books&price_lt=20
```

In this case, the request filters products where the `category` is "books", and
the `price` field is less than 20. The query string is easy for both the API
designer and users to understand, making it a natural choice for filtering data.

Naming conventions and deciding if or how to use operators will vary depending on
the implementation, but there are a few common practices and standards to consider.

### Simple Filtering

Starting with the most basic, you can filter by a single parameter using a query
parameter with a sensible name.

```bash
GET /products?category=books&status=available
```

In these examples, the query parameter `category` or `status` is used to remove
any products that don't match those exact values. 

The query parameters in some APIs might be a little busy, as there could be not
just sorting and pagination, but people do things changing output structures,
selecting which properties should be returned, or all kinds of functionality
which are not filtering.

To avoid confusion, it's a good idea to use a consistent naming scheme, like
`filter_category` or better yet a "filter array", e.g.:

```bash
GET /products?filter[category]=books&filter[status]=available
```

This makes it clear that these are filtering parameters, keeping it separate from
pagination, sorting, or any response modifiers which may be present.

Sometimes, users want to combine multiple filters. This is generally done by
adding more parameters to the URL: 

```bash
GET /orders?filter[status]=shipped&filter[customer_id]=123
```

Using multiple filters is always considered a logical `AND` and the filters
should be combined. Supporting a logical `OR` is trickier to represent in a
query string, but one common convention is to allow multiple values for a single
parameter with a comma-separated list:

```bash
GET /products?category=books,electronics
```

This would return products in either the "books" or "electronics" categories.

### Declaring Operators

Simple value matching is the most common form of filtering, but it might not be
enough depending on the use-cases clients expect. For example, filtering for books with a
price of `20` will ignore any books that cost `19.99`, which is probably not
very helpful.

```bash
GET /products?filter[price]=20
```

To solve this you can use operators to specify the type of comparison, like
"less than", "greater than", or "not equal". These are usually implemented with
suffixes or specific words added to the parameter name. For example, `GET
/products?price_gt=50` would retrieve products where the price is greater than
50. Other common operators include:

- `_lt` for less than (e.g., `price_lt=20`)
- `_gt` for greater than (e.g., `price_gt=100`)
- `_gte` and `_lte` for greater than or equal to, and less than or equal to, respectively.

Some people are tempted to try and use operators as a prefix for the value, like
`GET /products?price=<20` but that gets fairly awkward if you try less than or
equal: `GET /products?price=<=20`, everything needs to be escaped, and its
impossible to read.

Sticking with the filter array approach, you can make this a little more
readable:

```bash
GET /products?filter[price][lt]=20
GET /products?filter[price][gt]=99
GET /products?filter[price][gte]=100
```

This is a little more verbose, but it's much easier to read and understand.

### Advanced Filtering

Instead of trying to invent your own approach, there are standards that can be
used to make this easier for everyone, like
[FIQL](https://datatracker.ietf.org/doc/html/draft-nottingham-atompub-fiql-00),
[RSQL](https://github.com/jirutka/rsql-parser), or
[OData](https://www.odata.org/getting-started/basic-tutorial/#queryData).

As an example, OData is a widely used standard that provides a consistent way to
query and manipulate data. It uses a specific syntax for filtering, which might
look like this:

```bash
GET /products?$filter=category eq 'books' and price lt 50
```

Here, `$filter` is the standard keyword for filtering, and `eq` is used for
equality, while `lt` means less than. You can combine multiple filters using
`and`, just like in the example above. 

FIQL is a compact, text-based query language used for filtering. It uses
operators such as `==` for equality, `!=` for not equal, `<` and `>` for less
than and greater than, and `;` for AND logic. For example, a FIQL filter might
look like this:

```bash 
GET /products?filter=category==books;price<20
```

This is a concise way to express complex filtering logic, making it useful for
more advanced APIs.

Another option is RSQL, which is a slightly more modern version of FIQL that is gaining popularity:

```bash
GET /products?filter=category==books,price<50
```

RSQL uses a comma to separate filters, which is a little more readable than the
semicolon and doesn't need to be URL encoded. It can make some amazing queries
like `last_name==foo*,(age=lt=55;age=gt=5)`.

Whichever of these formats you pick will have pros and cons, but the most
important thing is to pick a standard instead of reinventing the wheel so you
can leverage existing libraries and tools on both the client-side and the
server-side. It's important to reuse existing tools for things like this instead
of wasting infinite time building and maintaining your own custom solutions
instead of solving genuine problems for your users.

## What is Sorting?

What order should you return resources in a collection? 

- Oldest first or newest first? 
- Alphabetical based on the name?
- Highest price to lowest price?

Whatever you pick at first may be a sensible default, but it's likely that users
will want to change this.

For APIs, sorting is the process of arranging resources in a specific order
based on user inputs. 

## How to Sort

Sorting is usually done with a query parameter:

```bash
GET /products?sort=name
```

This sorts products by the `name` property, and by default that will be in ascending order.

Most APIs will also allow clients to specify the order, which is usually done with another query parameter:

```bash
GET /products?sort=price&order=desc
```

Here if we just had `sort=price` it would be reasonable to assume the client
wanted the cheapest results, but if we're looking for the most expensive 
products, we can add `order=desc` to return the most expensive first.

This convention is very closely related to the SQL `ORDER BY` clause, which
takes a database property and an order in exactly the same way. Unlike a
database query your API does not have to allow clients to sort by every single
property, you could restrict to a few common use-cases and make sure they are
well optimized.

## Best Practices

### Consistency and Documentation

When designing filters for your REST API, it's important to make sure they are
intuitive and consistent. Use clear, descriptive names for your parameters. For
example, `price_lt` is much easier to understand than something vague like
`lower_price`. Providing solid documentation is equally important—developers
should be able to quickly find information on the available filters and how to
use them.

### Validation and Error Handling

Validation is also critical. If a user tries to apply a filter with invalid data
(like `price=abc`), your API should return a helpful error message rather than
just failing silently or returning incorrect results. Be sure to handle edge
cases as well, such as empty values or invalid characters in the query string.

_Learn more about [error handling in REST APIs](/api-design/responses/errors)._

### Performance Considerations

The more you allow clients to customize their requests, the harder it becomes to
set up caching rules and optimize database queries that might be produced. 

Anyone using an SQL database will know that the more complex the query, the
harder it is to optimize. If you're allowing clients to send in completely
arbitrary queries, it's going to be very hard to optimize your database because
you wont know what indexes to create. You are left retroactively optimizing
popular usages, which might be ok for an internal API used by a limited number
of colleagues who can warn you, but is a nightmare for teams maintaining public
APIs where an API could be brought down by a single user launching a new
product.

Rate limiting can help, but it's worth questioning: what is the purpose of this
API? 

Generally an API is not meant to be a database-over-HTTP, so if you feel
like you're starting to recreate SQL or some other query language, you might be
going down the wrong path. There are databases which can be used over HTTP that
do not require you to create a database, like FaunaDB, Firebase, or DynamoDB,
which might be a better fit.

### URL Design

Sometimes a filter could or should have been a different endpoint, a
different parameter, or a different way of structuring the data. 

If the clients have asked for the ability to show off some "Hot Bargains",
instead of telling clients to pick numbers based on price with `GET
/products?price_lt=20&sort=price`, you could use `GET /products/bargains`.

Cachability is improved, because you can set a 24 hour network cache on that
which will be shared by all clients. 

Consistency is improved, because the web and iOS versions of the same
application aren't going to pick slightly different numbers for what is
considered a bargain.

## Conclusion

Filtering is a powerful tool for API designers, allowing users to request only
the data they need. By using query parameters, operators, and standard query
languages, you can create a flexible and intuitive filtering system that meets
the needs of your users, without going overboard and confusing everyone or
making the API wildlife inefficient and unstable. 

When in doubt, start simple, and add things later. It's always easier to add new
parameters, endpoints, and additional ways of doing things, than it is to take
them away later.


 This is the content for the doc api-design/http-methods.md 

 # Using HTTP methods

We learned about URLs in _[What is a URL?](./what-is-a-url.md)_ which is how an
API defines its "resources". If an API request were a sentence, the URL would be the noun. In this section, we'll cover the verbs of API requests: HTTP methods.

```curl focus:1[1:3]
GET /places?lat=40.759211&lon=-73.984638 HTTP/1.1
Host: api.example.org
```

Sometimes people will suggest HTTP methods are optional, or they they're just noise, and everything would be better off done as purely a `GET` or `POST`. This is a bit of a misunderstanding of how HTTP works, and how the web works, and leads to all sorts of trouble down the line.

Conventions may seem arbitrary, but they are important to follow because **your API does not exist in a vacuum**. Conventions allow disparate tools that do not have direct knowledge of each other to work together seamlessly. If you start breaking these conventions, you're going to need to build all of your own tools, and that's a lot of work.

With that in mind, let's dive into the core HTTP methods and best practices for using them in your API.

## `GET`: Fetching Resources

`GET` is the most common HTTP method, and is used for fetching data. It is the
default method for most browsers, as it is used for fetching web pages, images,
stylesheets, and scripts. It is also used for fetching data from APIs.

`GET` is "idempotent", meaning that if you made the same get call over and over
again, You can expect the same outcome every time. If you `GET` the resource, but the request fails or times out, and you `GET` it again, the end result is that you got it. Nothing was deleted, or removed, or changed in any lasting way, so if this thing is got a bunch of times it is the same as being got once.

```curl
GET /places?lat=40.759211&lon=-73.984638 HTTP/1.1
Host: api.example.org
```

### Best Practices for `GET` requests

- Idempotent: Identical requests get identical outputs.
- No request body: `GET` requests can *technically* have a body, but a lot of frameworks and tooling will start acting a bit wonky, so just avoid it. There's almost always a better way to do what you're trying to do.
- Caching: You can and should enable caching for the response of `GET` requests, but you need to be careful about how you do it. If the data changes frequently, you might want to set a short cache time, or use a cache-busting technique.
- Safe: `GET` requests should not change the state of the server. They should only be used to retrieve data.

## `POST`: Creating Resources

`POST` requests are typically used for creating new resources or triggering actions that change server state. Unlike `GET`, `POST` is not idempotent - sending the same POST request multiple times may create multiple resources or trigger multiple actions.

`POST` does not necessarily have to represent a resource creation. You could use to signify the triggering of an event: the sending of an email, paying an invoice, etc. But it should result in that action being executed every time you make a request.

### `POST` examples

Creating a new location resources:

```curl
POST /places HTTP/1.1
Host: api.example.org
Content-Type: application/json

{
  "name": "High Wood",
  "lat": 50.464569783,
  "lon": -4.486597585
}
```

Triggering an email send:

```curl
POST /emails/send HTTP/1.1
Host: api.example.org
Content-Type: application/json

{
  "to": "Phil",
  "subject": "Hello",
  "body": "Hi Phil, how are you?"
}

```

### Best Practices for `POST` requests

- Not idempotent
- Includes a request body
- Cannot be cached by default
- Used for resource creation and non-idempotent actions

## `PUT`: Complete Resource Updates

Often incorrectly associated with being an "edit" action, PUT is designed for idempotent updates where the request contains the complete resource representation. This means that if you `PUT` the same data multiple times, the resource will be in the same state each time.

Some developers get hung up about is this a "create or update" action. Their concern comes from a misplaced sense that HTTP verbs should correspond to a specific CRUD (create, read, update, delete)action. That is not the case. `PUT` is the REST equivalent of an upsert operation in a database. If the resource exists, it will be updated. If it does not exist, it will be created.

### `PUT` examples

An example of this would be an image upload. An API might have the ability to
upload an image for a user, which is probably a profile image. A request with
`PUT /users/jane/image` and a body of the image contents (or a JSON
payload with a URL) could then provide the image. It does not matter if the user
already had an image or not, if the request is a success they will have one. If
the upload fails that is fine, another request can be made, and it will be
overridden.

```curl
PUT /users/jane/image HTTP/1.1
Host: api.example.org
Content-Type: image/jpeg

[Binary image data]
```

### Best Practices for `PUT` requests

- Idempotent: Multiple identical requests result in the same final state
- Complete resources: Must include the complete resource representation
- Treat as upsert: Can create or update resources
- Useful for uploads and full resource replacements

## `PATCH`: Partial Resource Updates

`PATCH` allows clients to send partial modifications to a resource. Unlike `PUT`, which requires sending the complete resource, `PATCH` only needs to contain the changes to be applied.

`PATCH` is not idempotent, so if you `PATCH` a resource, and the request
fails, you cannot just retry the request as you could with a `PUT`. The server
might have already made some changes, and retrying the request could result in a
different outcome.

```curl
PATCH /users/phil

{
  "image_url": "https://cdn.example.org/fancy-headshot.png"
}
```

How exactly PATCH works can vary depending on which data format you're using. If it's `JSON` then there are two popular approaches: [JSON Patch](https://tools.ietf.org/html/rfc6902) and [JSON Merge Patch](https://tools.ietf.org/html/rfc7396).

JSON Merge Patch is what most people will want to use for general APIs, as it is
simple to use.

### `PATCH` Example

```json
{
  "title": "Goodbye!",
  "author" : {
    "givenName" : "John",
    "familyName" : "Doe"
  },
  "tags":[ "example", "sample" ],
  "content": "This will be unchanged"
}
```

A user agent wishing to change the value of the "title" member from
"Goodbye!" to the value "Hello!", add a new "phoneNumber" member,
remove the "familyName" member from the "author" object, and replace
the "tags" array so that it doesn't include the word "sample" would
send the following request:

```http
PATCH /my/resource HTTP/1.1
Host: example.org
Content-Type: application/merge-patch+json

{
  "title": "Hello!",
  "phoneNumber": "+01-123-456-7890",
  "author": {
    "familyName": null
  },
  "tags": [ "example" ]
}
```

The resulting JSON document would be:

```json
{
  "title": "Hello!",
  "author" : {
    "givenName" : "John"
  },
  "tags": [ "example" ],
  "content": "This will be unchanged",
  "phoneNumber": "+01-123-456-7890"
}
```

### Best Practices for `PATCH` requests

- Not idempotent
- Contains only the fields to be modified
- More flexible than PUT for updates
- Supports different patch formats (JSON Patch, JSON Merge Patch)

## `DELETE`: Removing Resources

Aptly named, the `DELETE` method is used to remove resources from the system. It's intended to be idempotent because deleting a resource multiple times should have the same effect as deleting it once. However, some APIs do not implement it that way so a second attempt to delete the same thing will get a 404. This is not ideal, but it is common.

### `DELETE` Example

```curl
DELETE /places/123 HTTP/1.1
Host: api.example.org
```

### Best Practices for `DELETE` requests

- Keep delete idempotent.
- Usually doesn't include a request body
- Should return appropriate status codes (204 No Content or 200 OK)

## Less Common HTTP Methods

While most APIs primarily use GET, POST, PUT, PATCH, and DELETE, several other HTTP methods serve specific purposes:

### HEAD

- Identical to GET but returns only headers, no body
- Perfect for checking if a resource exists or has been modified
- Useful for validating links or checking file sizes before download

Example:

```http
HEAD /articles/123 HTTP/1.1
Host: api.example.org

HTTP/1.1 200 OK
Last-Modified: Wed, 15 Oct 2024 12:00:00 GMT
Content-Length: 12345
```

### OPTIONS

- Returns information about available communication options
- Most commonly used for CORS preflight requests
- Can provide information about allowed methods

Example:

```http
OPTIONS /api/articles HTTP/1.1
Host: api.example.org

HTTP/1.1 200 OK
Allow: GET, POST, HEAD, OPTIONS
Access-Control-Allow-Methods: GET, POST
```

### TRACE

- Used for diagnostic purposes
- Returns the exact request received by the server
- Helpful for debugging proxy issues
- Often disabled for security reasons

### CONNECT

- Used to establish tunnel connections through HTTP proxies
- Primarily used for HTTPS connections through proxies
- Rarely implemented in standard APIs

These methods are less frequently used but can be valuable for specific use cases like debugging, CORS handling, and proxy management.

## Best Practices

### 1. Use Methods as Intended

Don't force everything through POST or GET. Each method has its purpose:

- GET for retrieval
- POST for creation and non-idempotent actions
- PUT for complete resource replacement (upsert)
- PATCH for partial resource updates
- DELETE for removal

### 2. Consider Caching Implications

- GET requests should be cacheable when appropriate
- Include proper cache headers
- Ensure POST, PUT, PATCH, and DELETE invalidate caches as needed

### 3. Handle Race Conditions

With `PUT` & `PATCH` requests, be aware of potential race conditions:

```
# Client A reads resource
GET /resources/123

# Client B reads resource
GET /resources/123

# Client B's update overwrites A's changes
PUT /resources/123 {...}

# Client A updates resource
PUT /resources/123 {...}
```

Both clients were trying to update a single instance of a resource, but little do they know they are overwriting one another. This is on the server to handle, and there are a few ways to do it.

- Use timestamps for last modified
- Use optimistic locking with version numbers
- Implement ETags for concurrent updates

### 4. `PUT` vs. `PATCH`

Oftentimes an API will only support one of these methods. We **strongly** recommend supporting both, unless you have a very specific reason not to. `PUT` is great for full updates, but `PATCH` is more flexible and can be more efficient for partial updates.

## Remember

HTTP methods aren't just syntax - they're core to how the web works. Using them correctly makes your API:

- More predictable for clients
- Easier to cache
- Compatible with existing tools
- Easier to maintain and scale

Your choice of HTTP method communicates intent and behavior to both developers and tools, so choose wisely and consistently.


 This is the content for the doc api-design/index.mdx 

 ---
title: API design guide
description: "Learn how to design an API that is easy to use, easy to understand, and easy to maintain."
---

import { APIDesignCards } from "~/features/api-design/recipes";

<APIDesignCards />

 This is the content for the doc api-design/pagination.mdx 

 ---
description: "API Pagination is a common pattern for managing large data sets in APIs. This guide covers the basics of API pagination and best practices."
---

import { Callout } from '~/components'

# Paginating API responses

Pagination is a crucial concept that needs to be understood and designed into a
REST API before its built. It is often forgotten about until it's too late and
API consumers are already integrating with the API, so it's important to get
stuck into doing things the right way early on.

## What is API Pagination?

At first it's easy to image that collections only have a few hundred records.
That not be too taxing for the server to fetch from the database, turn into
JSON, and send back to the client, but as soon as the collection is getting into
thousands of records things start to fall apart in wild and unexpected ways. 

For example, a coworking company that expected to mostly host startups of 10-50
people, but then Facebook and Amazon rock up with ~10,000 employees each, and
every time somebody loads that data the entire API server crashes, along with
every application that uses it, and every application that uses that.

Breaking down a large dataset into smaller chunks helps to solve this, and it
works a lot like pagination does in the browser: when searching on a functioning
search engine like Duck Duck Go or Ecosia, the results are broken down into page
1, page 2... page 34295. It doesn't just throw every single result into the
browser in the worlds longest slowest web response, forcing computer fans to
whir until they snap out as it tries to render infinite HTML to the screen.

This is pagination in action, and pagination in an API is exactly the same idea.
Much like web pages it is done with query string parameters on a GET request.

```
GET /items?page=2
```

The main difference is that the client is not seeing a list of buttons in HTML,
instead they are getting metadata or links in the JSON/XML response. How exactly
that looks depends on which pagination strategy is picked, and there are a few to
choose from with their own pros and cons.

## Choosing a Pagination Strategy

To help pick a pagination strategy, let's look at some examples and talk through
the pros and cons.

1. Page-Based Pagination
2. Offset-Based Pagination
3. Cursor-Based Pagination

### Page-Based Pagination

Page-based pagination uses `page` and `size` parameters to navigate through pages of data.

```
GET /items?page=2&size=10
```

This request fetches the second page, with each page containing 10 items maximum.

There are two main ways to show pagination data in the response.

```json
{
  "data": [
    ...
  ],
  "page": 2,
  "size": 10,
  "total_pages": 100
}
```

This is pretty common, but forces the client to know a whole lot about the
pagination implementation, which could mean some guesswork (which could be
guessed wrong), or reading a whole lot of documentation about which bit goes
where and what is multiplied by whom.

The best way to help the client is to give them links, which at first seems
confusing but it's just
[HATEOAS](https://apisyouwonthate.com/blog/rest-and-richardson-maturity-model/)
(Hypermedia As The Engine Of Application State), also known as Hypermedia
Controls. 

It's a fancy way of saying "give them links for things they can do
next" and in the context of pagination that means "give them links to the next
page, the previous page, the first page, and the last page."

```json
{
  "data": [
    ...
  ],
  "meta": {
    "page": 2,
    "size": 10,
    "total_pages": 100
  },
  "links": {
    "self": "/items?page=2&size=10",
    "next": "/items?page=3&size=10",
    "prev": "/items?page=1&size=10",
    "first": "/items?page=1&size=10",
    "last": "/items?page=100&size=10"
  }
}
```

Whenever there is a `next` link, an API consumer can show a `next` button, or
start loading the next page of data to allow for auto-scrolling. 

If the `next` response returns data, it will give a 200 OK response and they can
show the data. 

If there is no data then it will still be a 200 OK but there will be an empty
array, showing that everything was fine, but there is no data on that page right
now. 

**Ease of Use**

- Pro: Simple to implement and understand.
- Pro: Easy for users to navigate through pages.
- Pro: UI can show page numbers and know exactly how many pages there are.
- Pro: Can optionally show a next/previous link to show consumers if there are more pages available.

**Performance**

- Con: Involves counting all records in the dataset which can be slow and hard to cache depending on how many variables are involved in the query.
- Con: Becomes exponentially slower with more records. Hundreds are fine. Thousands are rough. Millions are horrendous.

**Consistency**

- Con: When a consumer loads the latest 10 records, then a new record is added
to the database, then a user loads the second page, they'll see one of those
records twice. This is because there is no such concept as a "page" in the
database, just saying "grab me 10, now the next 10" does not differentiate which
records they actually were.

### Offset-Based Pagination

Offset-based pagination is a more straightforward approach. It uses `offset` and
`limit` parameters to control the number of items returned and the starting
point of the data, which avoids the concept of counting everything and dividing
by the limit, and just focuses on using offsets to grab another chunk of data.

```
GET /items?offset=10&limit=10
```

This request fetches the second page of items, assuming each page contains a
maximum of 10 items, and does not worry itself with how many pages there are.
This can help with infinite scrolls or automatically "importing" lots of data
one chunk at a time.

There are two main ways to show pagination data in the response.

```json
{
  "data": [
    ...
  ],
  "meta": {
    "total": 1000,
    "limit": 10,
    "offset": 10
  }
}

```

Or with hypermedia controls in the JSON:

```json
{
  "data": [
    ...
  ],
  "meta": {
    "total": 1000,
    "limit": 10,
    "offset": 10
  },
  "links": {
    "self": "/items?offset=10&limit=10",
    "next": "/items?offset=20&limit=10",
    "prev": "/items?offset=0&limit=10",
    "first": "/items?offset=0&limit=10",
    "last": "/items?offset=990&limit=10"
  }
}
```

**Ease of Use**

- Pro: Simple to implement and understand.
- Pro: Easily integrates with SQL `LIMIT` and `OFFSET` clauses.
- Pro: Like page-based pagination this approach can also show next/previous buttons dynamically when it's clear there are more records available.
- Con: Does not help the UI build a list of pages if they want to show "Page 1, 2, ... 20." They can awkwardly do maths on the total / limit but it's a bit weird.

**Performance**

- Con: Can become inefficient with large datasets due to the need to scan through all previous records.
- Con: Performance degradation is significant as the offset increases.

**Consistency**

- Con: The same problems exist for offset pagination as page pagination, if
more data has been added between the first request and second being made, the same record could show up in both pages.

**See this in action**

- [YouTube Data API](https://developers.google.com/youtube/v3/guides/implementation/pagination)
- [Reddit API](https://www.reddit.com/dev/api/)

### Cursor-Based Pagination

Cursor-based pagination uses an opaque string (often a unique identifier) to
mark the starting point for the next subsection of resources in the collection.
It's often more efficient and reliable for large datasets.

```
GET /items?cursor=abc123&limit=10
```

Here, `abc123` represents the last item's unique identifier from the previous
page, this could be a UUID, but it can be more dynamic than that.

APIs like Slack will base64 encode information with a field name and a value,
even adding sorting logic, all wrapped up in an opaque string. For example,
`dXNlcjpXMDdRQ1JQQTQ=` would represent `user:W07QCRPA4`. 

Obfuscating the information like this aims to stop API consumers hard-coding
values for the pagination, which allows for the API to change pagination logic
over time without breaking integrations. The consumers can simply pass the
cursor around to do the job, without worrying about what it actually involves.

It can look a bit like this:

```json
{
  "data": [...],
  "next_cursor": "xyz789",
  "limit": 10
}
```

To save the client even having to think about cursors (or knowing the name of
the query parameters for cursor or limit), links can once again save the day:

```json
{
  "data": [
    ...
  ],
  "links": {
    "self": "/items?cursor=abc123&limit=10",
    "next": "/items?cursor=xyz789&limit=10",
    "prev": "/items?cursor=prevCursor&limit=10",
    "first": "/items?cursor=firstCursor&limit=10",
    "last": "/items?cursor=lastCursor&limit=10"
  }
}
```

**Ease of Use**

- Pro: API consumers don't have to think about anything and the API can change the cursor logic.
- Con: Slightly more complex to implement than offset-based pagination.
- Con: API does not know if there are more records available after the last one in the dataset so has to show a next/previous link which may return no data. (You can grab limit+1 number of records to see if it's there, but that's a bit of a hack which could end up being slower. Benchmarks are your friend.)

**Performance**

- Pro: Generally more efficient than offset-based pagination depending on the data source being used.
- Pro: Avoids the need to count records to perform any sort of maths which means larger data sets can be paginated without suffering exponential slowdown.

**Consistency**

- Pro: Cursor-based pagination data remains consistent in more scenarios, even if new data is added or removed, because the cursor acts as a stable merker identifying a specific record in the dataset instead of "the 10th one" which might change between requests.

**See it in action**

- [Twitter API](https://developer.twitter.com/en/docs/twitter-api)
- [Instagram Graph API](https://developers.facebook.com/docs/instagram-api/)
- [Slack API](https://slack.engineering/evolving-api-pagination-at-slack/)

### Choosing a strategy

Choosing the right pagination strategy depends on the specific use case and
dataset size.

Offset-based pagination is simple but may suffer from performance issues with
large datasets. 

Cursor-based pagination offers better performance and consistency for large
datasets but come with added complexity. 

Page-based pagination is user-friendly but shares similar performance concerns
with offset-based pagination.

Using links instead of putting metadata in the response allows for more
flexibility over time with little-to-no impact on clients.

## Where Should Pagination Go?

In all of these examples there's been the choice between sending some metadata
back for the client to construct their own pagination controls, or sending them
links in JSON to avoid the faff.

Using links is probably the best approach, but they don't have to go in the
JSON response. Instead use the more modern approach: [RFC 8299: Web
Linking](https://www.rfc-editor.org/rfc/rfc8288).

```http
Link: <https://api.example.com/items?page=1&size=10>; rel="first",
      <https://api.example.com/items?page=3&size=10>; rel="next",
      <https://api.example.com/items?page=100&size=10>; rel="last"
```

Popping them into HTTP headers seems like the cleaner choice instead of
littering responses with metadata. It's also a slight performance putting
this into headers because HTTP/2 adds [header compression via
HPAK](https://blog.clou4986dflare.com/hpack-the-silent-killer-feature-of-http-2).

As this is a common standard instead of a convention, [generic HTTP clients like
Ketting](https://apisyouwonthate.com/blog/ketting-v5-hypermedia-controls/) can
pick this information up to provide a more seamless client experience.

Either way, pick the right pagination strategy for the data source, document it
well with a dedicated guide in API documentation, and make sure it scales up
with a realistic dataset instead of testing with a handful of records as assuming it scales

Adding or drastically changing pagination later could be a whole mess of
backwards compatibility breaks.

<Callout title="NOTE" variant="info">
  <p>Pagination can be tricky to work with for API clients, but Speakeasy SDKs can help out. Learn about <a href="/docs/runtime/pagination">adding pagination</a> to your Speakeasy SDK.</p>
</Callout>


 This is the content for the doc api-design/parameters.md 

 # Sending request parameters

There are all sorts of options that can be sent to an API endpoint and figuring out where to put things can be tricky at first.

Let's look at the four options.

## Path Parameters

Given the URL `/collections/shoes/products?sort=date&size=10&color=red` the
"path" is the `/collections/shoes/products`, and slashes separate bits of a URL,
some of which are static and some are variables. In this example `shoes` is a
variable that could be swapped for another collection on the e-commerce store,
e.g: `hats`. 

There is no concept of a "path parameter" in HTTP RFCs, this is purely a
convention, but it's common as API design terminology.

## Query Parameters

That same URL example has "query parameters" defined in the query string:
`/collections/shoes/products?sort=date&size=10&color=red`, which starts at, and
includes, the `?`: `?sort=date&size=10&color=red`. 

This string can be parsed into `[sort: 'date', size: '10', color: 'red']`. 

These options generally cover filtering, sorting, and pagination.

- Filtering: `?brand=Samsung&inStock=true`.
- Sorting: `?sort=date&sortBy=desc`.
- Pagination: `?page=2&limit=10` or `?cursor=abc123`.

They should never be used for anything destructive, or break the rules of the
method they're being used in (e.g. modify data on a GET), but they can have
other uses.

Some people use query strings for other purposes like changing the response data
that will come back, like using `GET /articles/123?include=comments` to squeeze
a bunch of comments data into the article response. This was considered best
practice in early 2010s but much like "image sprites" and "CSS combination" it's
now generally a bad practice, aided with improvements to HTTP/2 and HTTP/3.

Everything is a string because the whole URL is a string. Typed languages often
allow you to define types that these values should be converted to, so `size=10`
can become `10` instead of `"10"`, and `inStock=true` can become a proper `true`
instead of a literal string `"true"`.

**HTTP Headers**

HTTP Headers (also known as HTTP Header Fields) are metadata for a request, that
can have a wide variety of impacts across the API and various network components
along the way. You may have spotted this in the same code above.

```
Content-Type: application/json
```

That lets the API know that the message contains JSON.

```
Accept: application/json
```

That lets the API know we'd like JSON back too, if it can, we're ready for it.

```
If-Modified-Since: Wed, 30 Oct 2024 10:58:31 GMT
```

Only bother returning any data if its changed since then, otherwise just let us
know it's the same and save server resources.

These standard request headers are defined by Internet Engineering Task Force
(IETF) in [RFC 9110](https://www.rfc-editor.org/rfc/rfc9110) and [RFC
9111](https://www.rfc-editor.org/rfc/rfc9111), and various other complimentary
RFCs. They cover an amazingly wide functionality set, including: authorization,
caching, CORS, security, redirects, compression, and localization.

You _can_ define your own headers, but it's important to avoid replicating
standard functionality with custom headers. People do odd things like the
`MyCompany-API-Key` when they could use the standard `Authorization` header.
Various network components like cache proxies will know what to do with an
`Authorization` and will respond accordingly (changing the way caching works to
avoid leaking data to others), but you will need to somehow teach those network
components what to do with a custom `MyCompany-API-Key`.

Custom headers should be limited to handy but non-vital information, like trying
out a new beta feature hiding behind a feature flag.

```
Acme-Feature-Toggle: beta-feature=true
```

For everything else, there's the request body.

 This is the content for the doc api-design/rate-limiting.mdx 

 ---
description: Keep APIs running smoothly by controlling how many requests clients can make.
---

Rate limiting is the art of trying to protect the API by telling "overactive" API
consumers to calm down a bit, telling clients to reduce the frequency of their
requests, or take a break entirely and come back later to avoid overwhelming the
API. 

## Why bother with rate limiting

The main reason for rate limiting is to keep an API running smoothly and fairly.
If all clients could fire off requests as fast as they like, it's only a matter
of time before something breaks. A spike in traffic (whether accidental or
malicious) can overwhelm servers, leading to slowdowns, crashes, or unexpected
high infrastructure costs.

Rate limiting is also about fairness. If there are loads of users accessing an
API, it's important to make sure one consumers mistakes do not affect another. For
public APIs, it's about making sure no one user can hog all the resources. For
businesses, this could be different limits for free and various paid tiers to
make sure profit margins are maintained.

## How does API rate limiting work?

How can an API know when a client is making too many requests? That's where rate
limiting comes in. Rate limiting is a system that tracks the number of requests
made by a particular target (based on IP address, API key, user ID, or other
headers), within a defined time window.

The way this is implemented can vary, but the general process is the same:

- *Request Received* - A client makes a request to the API, asking for some data or
  to perform an action.
- *Identify the client* - The system identifies the client making the request,
  usually by looking at the IP address, API key, or other identifying
  information.
- *Check usage history* - The system checks how many requests the client has
  made in the current time window, and compares it to the limit.
- *Allow or deny the request* - If the client has made too many requests, the
  system denies the request with a `429 Too Many Requests` [status
  code](/api-design/status-codes). If the client is within the limit, the
  request is processed as normal.

## Different rate limiting strategies

There are a few different strategies for rate limiting, each with its own
advantages and disadvantages. 

![](./assets/rate-limiting-strategies.gif)

- **Token bucket:** the system has a bucket of tokens, and each request consumes
  a token. Tokens are added to the bucket at regular intervals, 100 tokens a
  minute, or 1,000 tokens per hour. If there are no tokens left, the request is
  denied. Clients are rewarded for taking time out and accrue more tokens as they
  do. This can lead to a lot of sudden bursts of activity, but should generally
  keep an average amount of traffic going through the system.

- **Fixed window:** the system sets a fixed limit for a specific time window. For
  example, "Make 100 requests per minute." This is the most common approach, but
  it can lead to very "lumpy" API traffic, where many clients are making the
  maximum number of requests at the start of a minute. This means an API can be stressed at the
  start of each minute and bored for the rest of it.

- **Sliding log:** instead of using the same time windows for all clients, the
  system sets a maximum number of requests for any 60 second period. This avoids
  the lumpy traffic concerns of many clients all maxing out at the start of the
  window, then doing nothing for the rest of it, as they would all have their
  own windows starting and stopping at different times depending on their usage.

- **Sliding window:** is a dynamic approach, adjusting limits based on real-time
  traffic patterns to optimize system performance and ensure fair access for
  all. This can be more complex to implement, but it can lead to a more
  efficient use of resources and a better experience for API consumers.

## Different limit targets

There are a lot of choices to be made when it comes to rate limiting, and the
first is: who or what are we trying to limit? Here are a few common targets
for rate limiting:

- **User-specific rate limits:** Identifying a user by their API key or user ID and
  setting a rate limit for that user. This is useful for ensuring that no single
  user can overwhelm the API and slow it down for others.

- **Application-specific rate limits:** Identifying an application by its API
  key and setting a rate limit for that application. This is useful for ensuring
  that a misconfigured application cannot affect stability for other
  applications.

- **Regional rate limits:** Manage traffic from different geographic regions, to
  make sure an API can continue to service critical regions, whilst still
  allowing other regions to access the API.

### Implementing rate limiting in HTTP

Rate limiting can be implemented at various levels, from the network layer to the
application layer. For HTTP APIs, the most common approach is to implement rate
limiting at the application layer with HTTP "middlewares" that keep track of these things, 
or API gateways which handle rate limiting like Zuplo, Kong, Tyk, etc. 

Wherever the rate limiting is implemented, there are a few standards that can be leveraged to 
avoid reinventing the wheel. 

The first is to return a HTTP error with a [status
code](/api-design/status-code) of `429 Too Many Requests` (as defined in [RFC
6585](https://www.rfc-editor.org/rfc/rfc6585.html)). This tells the client that
they've exceeded the rate limit and should back off for a while.

```http
HTTP/2 429 Too Many Requests
```

Instead of leaving the client to guess when they should try again (likely leading to lots of poking and prodding adding more traffic to the API), the `Retry-After` header can be added to a response with a number of seconds, or a specific time and date of when the next request should be made.

```http
HTTP/2 429 Too Many Requests
Retry-After: 3600
```

Why not also add some [proper error response](/api-design/errors) to explain why
the request was rejected, for any API consumer developers not familiar with
these concepts.

```http
HTTP/2 429 Too Many Requests
Content-Type: application/json
Retry-After: 3600

{
  "error": { 
    "message": "Rate limit exceeded",
    "code": "rate_limit_exceeded",
    "details": "You have exceeded the rate limit for this API. Please try again in 1 hour."
  }
}
```

Doing all of this makes it clear to the client that they have entered a rate
limit, and give them the information they need to know when they can try again,
but there is more that can be done to make this more user friendly.

### Rate limit headers

Documenting the rate limit in the response headers can help API consumers to
understand what's going on. There are various conventions for headers to help
consumers understand more about what the rate limiting policy is, how much of
the limit has been used, and what is remaining.

GitHub for example uses the `X-RateLimit-Limit`, `X-RateLimit-Remaining`, and
`X-RateLimit-Reset`.

Twitter uses `X-Rate-Limit-Limit`, `X-Rate-Limit-Remaining`, and
`X-Rate-Limit-Reset`. 

Similar but different, which causes all sorts of confusion. Designing an API to
be the most user friendly means relying on standards instead of conventions, so
it's worth looking at the [RateLimit header draft
RFC](https://datatracker.ietf.org/doc/draft-ietf-httpapi-ratelimit-headers/)
which outlines one new `RateLimit` header to cover all those use cases and a few more.

The following example shows a `RateLimit` header with a policy named "default",
which has another 50 requests allowed in the next 30 seconds.

```
RateLimit: "default";r=50;t=30
```

The `RateLimit` header focuses on the current state of the various quotes, but
it doesn't provide information about the policy itself. The same draft RFC also
outlines a `RateLimit-Policy` header which can be used to provide information
about how the policy works. 

This example shows two policies, "default" and "daily". The "default" policy has
a quota of 100 requests and a window of 30 seconds, while the "daily" policy has
a quota of 1000 requests and a window of 86400 seconds (24 hours).

```http
RateLimit-Policy: "default";q=100;w=30,"daily";q=1000;w=86400
```

Combining these two headers can provide a lot of information to API consumers to
know what the rate limits are, how much they have used, and when they can make
more requests. 

This can be a bit of work to set up, but it allows API consumers to interact
with an API more effectively, with less frustration, and keep everything running
smoothly. 

### Alternatives to Rate Limiting  

Some people argue that rate limiting is a blunt tool. It can be frustrating for
users who hit the limit when they're trying to get work done.

Poorly configured rate limiting can be fairly arbitrary. 

Consider an API that could theoretically handle 1000 requests per second. 

If there are 1000 users, each with a rate limit of 1 request per second, the API
would be maxed out. 

If that same API with 1000 users and only two of them are using up the their
maximum quotas, then the API could absolutely handle the load, and the API is
sitting their underutilized sitting around waiting for potential activity which
wont come. 

Not only is that a waste of server resources (hardware, electricity, CO2
emissions), but it's also frustrating for those users who are constantly being
told to calm down when they could be using the API to handle more activity;
activity which could be profitable.

One alternative approach is known as **backpressure**. This is a more dynamic
system which tells clients to ease up when the system is under strain, with a
`503 Service Unavailable` response with a `Retry-After` header. This could be
applied to the entire API, to specific users, or even specific endpoints that
are more resource intensive.

Quota-based systems are another alternative. Instead of measuring requests per
second or minute, users are assigned a monthly allowance. This works well for
subscription-based APIs, where users pay for a certain amount of access. If they
make a mistake and use up their quota too quickly, they can buy more, and other
API consumers can still access the API. This lends itself better to auto-scaling
up (and back down) based on number of active users and usage.

### Final Thoughts  

Rate limiting begins as a technical safeguard for an API (which makes managing
it easier) but ensures nobody is hogging resources (which keeps users happily
using the product). 

It's worth thinking about where and how to implement it, how to communicate it,
and how to make it as user-friendly as possible. It's not always simple for
junior developers to figure out how to work with rate limiting and they might
not know all the HTTP status codes and headers. The more tooling you can provide
to assist your users with responding to your rate limiting, the better. 



 This is the content for the doc api-design/request-body.mdx 

 ---
description: "Best practices and common patterns for API requests."
---

import { CodeWithTabs } from "~/components/codehike/CodeTabs";

# Sending request data

Understanding how to properly structure and handle request data is crucial for building robust APIs. This guide covers best practices and common patterns for working with API request data.

## URL Structure

Every API request starts with a URL that identifies the resource and any query parameters:

```http
GET /api/v1/places?lat=40.759211&lon=-73.984638 HTTP/1.1
Host: api.example.org
```

Key components:

- Resource path: Identifies what you're interacting with.
- Query parameters: Filter, sort, or modify the request.
- API version: Often included in the path.

## Request Bodies

Requests can also have a ' request body', which is a payload of data being sent
the API for processing. It is very frowned upon for `GET` but expected for
`POST`, `PUT`, and `PATCH`. The request body can be in a variety of formats, but
the most common are JSON, XML, and form data.

```http
POST /places HTTP/1.1
Host: api.example.org
Content-Type: application/json

{
  "name": "High Wood",
  "lat": 50.464569783,
  "lon": -4.486597585
}
```

This `POST` request to the `/places` endpoint is trying to add a new place to
the API, once again using a JSON body. The `Content-Type` header lets the server
know to expect JSON data, so it can parse the body and create a new place with
the name "High Wood" and the coordinates `50.464569783, -4.486597585`.

So far the examples of HTTP requests and responses have been using text, but in
reality they are just a series of bytes. The text is just a human-readable
representation of the bytes, and the tools that interact with the API will
convert the text to bytes before sending it, and convert the bytes back to text
when receiving it.

Most of you will interact with APIs using a programming language, and the code
to send a request will look something like this:


<CodeWithTabs>
    ```typescript !!tabs main.js
    import axios from 'axios';

    const response = await axios.post('https://api.example.org/places', {
      name: 'High Wood',
      lat: 50.464569783,
      lon: -4.486597585,
    });
    ```

    ```python !!tabs main.py
    import json
    import requests

    headers = {
        'Content-Type': 'application/json',
    }
    payload = {
        'name': 'High Wood',
        'lat': 50.464569783,
        'lon': -4.486597585,
    }
    req = requests.post(
      'https://api.example.org/places',
      data=json.dumps(payload),
      headers=headers
    )
    ```

    ```php !!tabs main.php
    $client = new Guzzle\Http\Client('https://api.example.org');

    $headers = [
        'Content-Type' => 'application/json',
    ];
    $payload = [
        'name' => 'High Wood',
        'lat' => 50.464569783,
        'lon' => -4.486597585,
    ];
    $req = $client->post('/places', [
      'headers' => $headers,
      'json' => $payload,
    ]);
    ```

    ```ruby !!tabs main.rb
    conn = Faraday.new(
      url: 'https://api.example.org',
      headers: { 'Content-Type' => 'application/json' }
    )

    response = conn.post('/places') do |req|
      req.body = {
        name: 'High Wood',
        lat: 50.464569783,
        lon: -4.486597585,
      }.to_json
    end
    ```
</CodeWithTabs>

HTTP tooling is essentially the same thing no matter the language. It's all
about URLs, methods, body, and headers. This makes REST API design a lot easier,
as you have a "uniform interface" to work with, whether everything is following
all these set conventions already.

Requests like `POST`, `PUT`, and `PATCH` typically include data in their body.

```http
POST /places HTTP/1.1
Host: api.example.org
Content-Type: application/json

{
  "name": "High Wood",
  "lat": 50.464569783,
  "lon": -4.486597585
}
```

This area is yours to do with as you want. There's a lot of freedom in how you
structure your data, but there are some best practices to follow which you can
learn more about in the [API Collections & Resources](/api-design
api-collections) guide.

### Request Body

The request is body is where the majority of "domain specific" data (things
specifically about your organization or API) will go. To understand more about
request bodies, we need to learn about data formats.

## Best Practices

### 1. Keep Request & Response Data Consistent

Maintain the same structure for data going in and out of your API. You want to
strive for predictability and consistency in your API design. When a user sends
a `POST` request to create a new resource, they should get back a response that
looks like the resource they just created. If a user updates a resource, the
response should return the new state of the updated resource.

```json
// POST Request
{
  "name": "High Wood",
  "lat": 50.464569783,
  "lon": -4.486597585
}

// Response
{
  "id": 123,
  "name": "High Wood",
  "lat": 50.464569783,
  "lon": -4.486597585,
  "created_at": "2024-10-24T12:00:00Z"
}
```

More on this in the [API Responses](/api-design/responses) guide.


 This is the content for the doc api-design/responses.mdx 

 ---
description: Creating clear, consistent API responses is crucial for building usable APIs. This guide covers essential patterns and best practices for API responses.
---

import { Callout } from '~/components'

# Designing API responses

The API response is the most important part of the entire API. 

- Did the API consumer ask a question? They need that answer.
- Did the API consumer ask to transfer £1,000,000? They need to be confident
that went well.
- Did the API consumer make a mistake? Tell them how to fix it so they can get
back to making you money.

Creating clear, consistent API responses is crucial for building usable APIs.
This guide covers essential patterns and best practices for API responses.

## Anatomy of an API Response

An API response is primarily made up of a status code, headers, and a response body,
so let's look at each of those parts in turn.

### Headers

Just like requests allow API consumers to add HTTP headers to act as metadata for the request, APIs and other network components can add headers to a response.

```http
HTTP/2 200 OK
Content-Type: application/json
Cache-Control: public, max-age=18000
RateLimit: "default";r=50;t=30

{
  "title": "something"
}
```

This is a successful request, with some JSON data as highlighted by the `Content-Type` header. The API has also alerted the API consumer that this is cacheable so they don't need to ask for it again for 5 hours, and explained that the client is running a little low on their rate limiting policy with only 50 more requests allowed in the next 30 seconds.

API responses contain lots of useful metadata in the headers, but data is going to be in the response body.

### Response Body

You should strive to keep response consistent and well-structured, with minimal nesting and correct use of data types.

```json
{
  "id": 123,
  "name": "High Wood",
  "location": {
    "lat": 50.464569783,
    "lon": -4.486597585
  },
  "created_at": "2024-10-24T12:00:00Z",
  "links": {
    "reviews": "/places/123/reviews"
  }
}
```

It's pretty common to add an `id` of some sort, often data will have dates, and
relationships and available actions can be linked allowing API consumers to easily
find related information without going on a scavenger hunt.

### Status Codes

So far we've only looked at success, but how do we know if something
has worked or not? 

You could look at the response body and try to figure it out, and for years
people were doing silly things like setting `{ "success": true/false }` in their
response body to give people a hint, but as always there's a far better way
defined in the HTTP spec which covers loads more use-cases and works out of the
box with loads of tools: HTTP Status Codes.

A status code is a number and a matching phrase, like `200 OK` and `404
Not Found`. There are countless status codes defined in the HTTP RFCs and
elsewhere, and some big companies have invented their own which became common
conventions, so there's plenty to choose from.

Arguments between developers will continue for the rest of time over the
exact appropriate status code to use in any given situation, but these are the
most important status codes to look out for in an API:

#### 2XX is all about success

Whatever the API was asked to do was successful, up to the point that the
response was sent. A `200 OK` is a generic "all good", a `201 Created` means
something was created, and a `202 Accepted` is similar but does not say anything
about the actual result, it only indicates that a request was accepted and is
being processed asynchronously. It could still go wrong, but at the time of
responding it was all looking good at least up until it was put in the queue.

The common success status codes and when to use them:

* **200** - Generic everything is OK.
* **201** - Created something OK.
* **202** - Accepted but is being processed async (for a video means.
encoding, for an image means resizing, etc.)
* **204** - No Content but still a success. Ideal for a successful `DELETE` request, for example.

Example success response

```http
HTTP/1.1 200 OK
Content-Type: application/json

{
  "user": {
    "id": 123,
    "name": "John Doe"
  }
}
```

#### 3XX is all about redirection

These are all about sending the calling application somewhere else for the
actual resource. The best known of these are the `303 See Other` and the `301
Moved Permanently`, which are used a lot on the web to redirect a browser to
another URL. Usually a redirect will be combined with a `Location` header to
point to the new location of the content.

#### 4XX is all about client errors

Indicate to your clients that they did something wrong. They might have
forgotten to send authentication details, provided invalid data, requested a
resource that no longer exists, or done something else wrong which needs fixing.

Key client error codes:

* **400** - Bad Request (should really be for invalid syntax, but some folks.
use for validation).
* **401** - Unauthorized (no current user and there should be).
* **403** - The current user is forbidden from accessing this data.
* **404** - That URL is not a valid route, or the item resource does not exist.
* **405** - Method Not Allowed (your framework will probably do this for you.)
* **406** - Not Acceptable (the client asked for a content type that the API does not support.)
* **409** - Conflict (Maybe somebody else just changed some of this data, or status cannot change from e.g: "published" to "draft").
* **410** - Gone - Data has been deleted, deactivated, suspended, etc.
* **415** - The request had a `Content-Type` which the server does not know how to handle.
* **429** - Rate Limited, which means take a breather, sleep a bit, try again.

#### 5XX is all about service errors

With these status codes, the API, or some network component like a load
balancer, web server, application server, etc. is indicating that something went
wrong on their side. For example, a database connection failed, or another
service was down. Typically, a client application can retry the request. The
server can even specify when the client should retry, using a `Retry-After` HTTP
header.

Key server error codes:

* **500** - Something unexpected happened, and it is the API's fault.
* **501** - This bit isn't finished yet, maybe it's still in beta and you don't have access.
* **502** - API is down, but it is not the API's fault.
* **503** - API is not here right now, please try again later.

As you can see, there are a whole bunch of HTTP status codes. You don't need to try and use
them all, but it is good to know what they are and what they mean so you can use
the right one for the job.

You have two choices, either read the [full list of status codes from the
IANA](https://www.iana.org/assignments/http-status-codes/http-status-codes.xhtml),
or swing by [http.cats](http://http.cat/) and see what the cats have to say
about it.

### Using Status Codes

```ts
import axios, { AxiosError } from 'axios';

async function makeHttpRequest() {
  try {
    const response = await axios.get('https://example.com/api/resource');
    console.log('Response:', response.data);
  } catch (error) {
    if (! axios.isAxiosError(error)) {
      console.error('An unexpected error occurred:', error);
      return;
    }
    const axiosError = error as AxiosError;
    if (axiosError.response?.status === 401) {
      console.error('You need to log in to access this resource.');
    } else if (axiosError.response?.status === 403) {
      console.error('You are forbidden from accessing this resource.');
    } else if (axiosError.response?.status === 404) {
      console.error('The resource you requested does not exist.');
    } else {
      console.error('An error occurred:', axiosError.message);
    }
  }
}

makeHttpRequest();
```

Now you can warn API consumers of fairly specific problems. Doing it way is
cumbersome, but there's plenty of generic libraries with various extensions and
"middlewares" that will help auto-retry any auto-retriable responses,
automatically cache cachable responses, and so on.

<Callout title="NOTE" variant="info">
  <p>Avoid confusing your API consumers by <a href="/docs/runtime/retries">enabling retry logic</a> in your Speakeasy SDK.</p>
</Callout>

## Best Practices

### 1. Keep Status Codes Appropriate & Consistent

It's important to keep status codes consistent across your API, ideally across your entire organization. 

This is not just for nice feels, it helps with code reuse, allowing consumers to
share code between endpoints, and between multiple APIs. 

This means they can integrate with you quicker, and with less code, and less maintenance overhead.

### 2. Keep Request & Response Bodies Consistent

Sometimes API developers end up with divergent data models between the request
and the response, and this should be avoided whenever possible. 

Whatever shape you pick for a request, you should match that shape on the response.

```json
// POST /places 

{
  "name": "High Wood",
  "lat": 50.464569783,
  "lon": -4.486597585
}
```

```json
// GET /places/123

{
  "id": 123,
  "name": "High Wood",
  "lat": 50.464569783,
  "lon": -4.486597585,
  "created_at": "2024-10-24T12:00:00Z"
}
```

You can see that some differences, like `id` or `created_at` dates on the
response but not the request. That's OK, because they can be handled as
"read-only" or "write-only" fields in the API documentation and generated code,
meaning they are using the same models just ignoring a few fields depending on
the context.

The problem often comes from various clients having a word with the API
developers about "helping them out", because some library being used by the iOS
app would prefer to send coordinates as a string and they don't want to convert
them to a decimal for some reason. Then the API team wanted to have the
responses wrapped into objects to make it look tidy, but the React team said it
would be too hard to get their data manager to do that, so the request skipped
it.

```json
// POST /places

{
  "name": "High Wood",
  "lat": "50.464569783",
  "lon": "-4.486597585"
}
```

```json
// GET /places/123

{
  "id": 123,
  "name": "High Wood",
  "location": {
    "lat": 50.464569783,
    "lon": -4.486597585
  },
  "created_at": "2024-10-24T12:00:00Z"
}
```

Aghh!

This sort of thing causes confusion for everyone in the process, and whilst any
one change being requested might feel reasonable, when a few of them stack up
the API becomes horrible to work with. 

Push back against request/response model deviations. It's not worth it.

### 3. Return detailed errors

Just returning a status code and a message is not enough, at the bare minimum
add an error message in the JSON body that adds more context. 

```
HTTP/2 409 Conflict
Content-Type: application/json

{
  "error": "A place with that name already exists."
}
```

This is better than nothing but not ideal. Other information needs to be added
to help with debugging, and to help the API client differentiate between errors.

There is a better way: [RFC 9457](https://tools.ietf.org/html/rfc9457) which
defines a standard way to return errors in JSON (or XML).

```http
HTTP/2 409 Conflict
Content-Type: application/problem+json

{
  "type": "https://api.example.com/probs/duplicate-place",
  "title": "A place with that name already exists.",
  "detail": "A place with the name 'High Wood' already exists close to here, have you or somebody else already added it?",
  "instance": "/places/123/errors/<unique-id>",
  "status": 409
}
```

More on this in the [API Errors](/api-design/responses/errors) guide.

## Best Practices

### 1. Keep Status Codes Appropriate & Consistent

It's important to keep status codes consistent across your API, ideally across your entire organization. 

This is not just for nice feels, it helps with code reuse, allowing consumers to
share code between endpoints, and between multiple APIs. 

This means they can integrate with you quicker, and with less code, and less maintenance overhead.

### 2. Keep Request & Response Bodies Consistent

Sometimes API developers end up with divergent data models between the request
and the response, and this should be avoided whenever possible. 

Whatever shape you pick for a request, you should match that shape on the response.

```json
// POST /places 

{
  "name": "High Wood",
  "lat": 50.464569783,
  "lon": -4.486597585
}
```

```json
// GET /places/123

{
  "id": 123,
  "name": "High Wood",
  "lat": 50.464569783,
  "lon": -4.486597585,
  "created_at": "2024-10-24T12:00:00Z"
}
```

You can see that some differences, like `id` or `created_at` dates on the
response but not the request. That's OK, because they can be handled as
"read-only" or "write-only" fields in the API documentation and generated code,
meaning they are using the same models just ignoring a few fields depending on
the context.

The problem often comes from various clients having a word with the API
developers about "helping them out", because some library being used by the iOS
app would prefer to send coordinates as a string and they don't want to convert
them to a decimal for some reason. Then the API team wanted to have the
responses wrapped into objects to make it look tidy, but the React team said it
would be too hard to get their data manager to do that, so the request skipped
it.

```json
// POST /places

{
  "name": "High Wood",
  "lat": "50.464569783",
  "lon": "-4.486597585"
}
```

```json
// GET /places/123

{
  "id": 123,
  "name": "High Wood",
  "location": {
    "lat": 50.464569783,
    "lon": -4.486597585
  },
  "created_at": "2024-10-24T12:00:00Z"
}
```

Aghh!

This sort of thing causes confusion for everyone in the process, and whilst any
one change being requested might feel reasonable, when a few of them stack up
the API becomes horrible to work with. 

Push back against request/response model deviations. It's not worth it.

### 3. Return detailed errors

Just returning a status code and a message is not enough, at the bare minimum
add an error message in the JSON body that adds more context. 

```
HTTP/2 409 Conflict
Content-Type: application/json

{
  "error": "A place with that name already exists."
}
```

This is better than nothing but not ideal. Other information needs to be added
to help with debugging, and to help the API client differentiate between errors.

There is a better way: [RFC 9457](https://tools.ietf.org/html/rfc9457) which
defines a standard way to return errors in JSON (or XML).

```http
HTTP/2 409 Conflict
Content-Type: application/problem+json

{
  "type": "https://api.example.com/probs/duplicate-place",
  "title": "A place with that name already exists.",
  "detail": "A place with the name 'High Wood' already exists close to here, have you or somebody else already added it?",
  "instance": "/places/123/errors/<unique-id>",
  "status": 409
}
```

More on this in the [API Errors](/api-design/api-errors) guide.


 This is the content for the doc api-design/security.md 

 ---
description: Designing for API security from the ground up.
---

# Designing for API security

Creating an API is like opening a door to the outside world. Who is allowed
through, what they can carry, and where they're allowed to go is incredibly
important. In this guide we'll see how design choices made early on impact the
security of an API once it's built. 

Many API security problems come down to coding errors or misconfigured
infrastructure, but this guide focuses more on the foundational API design
decisions that effect the security of an API from day one.

## Why care about API security

APIs often protect sensitive data or critical functionality. Whether it's a
payment gateway, a medical records system, or a social media app, an API needs
to be designed with security in mind to protect both the organization and its
users.

API security breaches in 2022 caused losses worth [$12–$23
billion](https://www.darkreading.com/application-security/api-security-losses-billions-complicated)
in the US and [$41–$75 billion
globally](https://techwireasia.com/2022/06/api-vulnerabilities-costing-businesses-up-to-us75-billion-annually/).

To pick just a few examples, since the introduction of General Data Protection
Regulation (GDPR), Amazon Europe were fined €746m in 2021, Meta was fined €1.2bn
in 2023, and - to show it's not just tech giants - Marriott International (a
hotel chain) got stuck with a £20m fine in 2022.

More countries and regions strengthening privacy laws along the lines of GDPR:
California Consumer Privacy Act (CCPA), Canada's Personal Information Protection
and Electronic Documents Act (PIPEDA), and Brazilian General Data Protection Law
(LGPD).

Even if data breaches and leaks don't result in hefty fines, the reputational
damage that comes with leaking customers private information can be a big issue,
so it's important to do everything possible to keep APIs secure.

Let's walk through some key security concepts in API design to see how
decisions can make or break an API's defenses before it's even built.

## Principle #1: Design with the least privilege

**Every API consumer should only have access to what they need and nothing more.**

Imagine designing an API for an e-commerce platform. A customer should be
able to view their order history, but not other customers' orders. 

Similarly, a "staff" user might need access to refund functionality but shouldn't
necessarily see sensitive payment details. 

**What Could Go Wrong**: Failure to verify this could lead
to Insecure Direct Object References (IDOR), a common flaw where attackers can
manipulate identifiers to access data they shouldn't.

**Design Decision**: The first issue to make sure endpoints are protected with
access controls, restricting the the specific user, or to a user with the right
role.

```http
GET /orders/{orderId}
Authorization: Bearer {access_token}
```

The application should verify that the `orderId` belongs to the authenticated
user, unless the user has a role like `admin`. 

Refund logic and payment details can be split onto their own endpoints:

```http
POST /orders/{orderId}/refund
Authorization: Bearer {staff_access_token}
```

```http
GET /orders/{orderId}/payments
Authorization: Bearer {admin_access_token}
```

This allows staff handle refunds, but does not leak sensitive credit card
information to as many people within the company, whilst still making it
possible to escalate customer problems to a higher access user. 

Better yet, **the payments collection is not even on the API**, it's something only
viewable in an admin backend system thats protected with a firewall and VPN.

## Principle #2: Always validate input

**Inputs should be treated as untrusted, even if the API is "internal" or
"private".**

Any incoming API traffic could be compromised in some way, even if it's
considered to be a trusted source. 

An API could suddenly become public: either intentionally when infrastructure
teams move things around, or accidentally when somebody de-compiles an iOS
application or sniffs traffic to find an API that people thought was hidden. 

Even if an API is firewalled off from public traffic, another API or service
could have been hacked giving them access to the protected API.

It's best to treat everyone with suspicion, and validate all inputs as strictly
as possible. 

**What Could Go Wrong**: Malicious data could be introduced, or private
information leaked, leading to any number of issues. People could delete invoice
payment records and updating payment details to trigger a second payment to the
wrong person. They could change passwords for users so they can log in as them
to access information and processes not even available in the API.

**Design Decision**: Set strict rules for which properties are editable, which
can be returned, and set strict validation rules for these properties. 

This can be described in OpenAPI early on utilizing `readOnly`, `writeOnly`,
`required`, setting `additionalProperties: false`. [Learn more about
additionalProperties](https://www.speakeasy.com/guides/openapi/additionalproperties).
This means when the API is developed the OpenAPI can be used for integration
testing to poke and prod to see if extra properties can sneak though. 

Comical examples of this was somebody hacking GitHub and Rails to update the
`created_at` date to have the year 3012. This attack is known as Bender from the
Future (a reference to TV show Futurama) and made the concept of "Mass
Assignment" popular. Whitelist which specific properties should be allowed to be
written/read in planning documents and OpenAPI, and either use that OpenAPI
document for validation and serialization, or test against it once they've built
the API.

## Principle #3: Keep secrets out of the URL

Sensitive information like API keys or tokens should never appear in URLs.

Let's say an API allows filtering resources:

```http
GET /products?search=blue&apiKey=my-secret-key
```

**What Could Go Wrong**: Logs, browser history, and proxies often store URLs. If
an API key or sensitive data is passed in the URL, it's at risk of exposure.

**Design Decision**: Always pass sensitive data through headers or the body of
the request, not the URL. The body will be encrypted when HTTPS is used, but the
URL is not.

```http
GET /products?search=blue
Authorization: Bearer my-secret-key
```

Using `Authorization` has the added benefit over generic custom headers like
`X-API-Key` because it will alert HTTP caching tools to not reuse this response
for other users by default. 

This is not simply about authorization though, there are lots of other
"sensitive" things which should not go into the URL. Email addresses, social
security numbers, anything that should not be leaked to the public in general.
Pop it in the body instead.

A `GET` method generally should not have a HTTP request body (behavior is
undocumented, support is inconsistent, generally unadvisable), but the [`QUERY`
draft RFC](https://httptoolkit.com/blog/http-search-method/) could be solution
we're all looking for.

## Principle #4: Limit one-time URLs

Logins and file uploads often involve allowing a user to pass in a URL, which
will then be downloaded or redirected to. 

```http
POST /products/{productId}/images
Authorization: Bearer {access_token}
Content-Type: application/json

{
  "import_url": "http://hopefully-innocent-website.com/something.jpg"
}
```

**What Could Go Wrong**: THis can be a big source of problems for an API, even
if the use case is something small and simple like importing an avatar for a
user. THe URL could be:

- A malicious file the API is being asked to download.
- A very large file the API will run out of resources trying to download.
- Intercepted by a malicious actor on an infected network to change the DNS of
  that URL to another server which is malicious.

**Design Decision**: The API design for image uploads could be changed to take a
HTTP request with the image directly.

```http
POST /products/{productId}/images
Authorization: Bearer {access_token}
Content-Type: image/png

<image data>
```

People could still try to upload malicious files directly, but its easier to
scan the incoming request body for problems and reject it. This can be done on
the API gateway or via other threat detection like Cloudflare.

With an API gateway in place, if this request is problematically large the
gateway will reject the request without consuming any resources at all on the
API server.

Malicious users on an infected network could still be messing with DNS settings,
but they would have to mess with the API in question - which should have proper
HTTPS setup and be much harder for them to do. Compared to their ability to mess
with `http://hopefully-innocent-website.com/` which may not be set up so well.

## Principle #5: Don't help competitors steal data

Using auto-incrementing IDs as identifiers in an API makes it incredibly easy
for malicious actors to glean insights into potentially sensitive data a
business might not want to expose, or allow outright theft of an entire dataset.

A startup tracking street art around the world (think Banksy, Bragga, and
smaller artists) built an amazing unique database of user-generated photographs
and locations of all sorts of graffiti, sculptures, installations, etc. 

This data was not available anywhere else on the Internet, but their website
relied on two API endpoints:

```http
GET /artworks/234
GET /users/6138
```

**What Could Go Wrong**: Looking at the URL `GET /users/6138`, its not too hard
to assume I can look at `GET /users/1`. If that shows me data, I can reasonably
assume they have at least 6138 users, but to find the total I can easily make a
script that `id+1` and counts every HTTP status 200 to show me how many users
are in the system. It can also counts things like 404 or 410, to give a accurate
number of how many active users versus inactive users, leaking a "churn rate"
which could be embarrassing in the press of scare off investors.

Using the same approach a client can hit `GET /artworks/1` and loop through with `id
+ 1` to grab a hold of all that data, which helped that company populate their
own database, making a new competitor quite easily, and with a slightly better
app as they didn't have to spend time or money building the dataset in the first
place. This put the original startup out of business.

**Design Decision**: There are non-incremental or "hard to guess" system of
identifiers instead. Standards like
[UUID](https://www.rfc-editor.org/rfc/rfc9562.html) or
[Snowflake](https://en.wikipedia.org/wiki/Snowflake_ID) instead.

Instead of having `/artworks/1` and `/artworks/2`, design the API to use UUID:

```http
GET /artworks/c1b07800-b001-4ba9-8372-e0260cf25242
GET /artworks/4e44cf4a-fbe0-4630-983f-ccd55b7e4870
```

There is no way for anyone to glean from this how many resources the API has, or
guess the next one, without brute forcing the API with infinite arbitrary
requests...

## Principle #6: Rate limiting and throttling

Prevent abuse by controlling how frequently clients can interact with an API.

Consider a public API endpoint for retrieving weather data:

```http
GET /weather?city=London
```

**What Could Go Wrong**: Without rate limiting, a single client could make
thousands of requests per second, overloading API servers and possibly causing
a denial of service (DoS).

**Design Decision**: Implement rate limiting at the design level. Define
thresholds for various user roles:

- Free users: 100 requests per hour
- Paid users: 1,000 requests per hour

Communicate these limits clearly in API documentation and return appropriate
status codes like `429 Too Many Requests` when limits are exceeded. 

Learn more about [rate limiting](/api-design/rate-limiting).

## Principle #7: Security through obscurity is not enough

An e-commerce platform for online stores (shops) provides a listing page with
the revenue charts for their hosted shops. Inspecting the browser requests, an
attacker can identify the API endpoints used as a data source for those charts
and their pattern: `/shops/{shopName}/revenue_data.json`. Using another API
endpoint, the attacker can get the list of all hosted shop names. With a simple
script to manipulate the names in the list, replacing {shopName} in the URL, the
attacker gains access to the sales data of thousands of e-commerce stores.

With `/shops/{shopName}/revenue_data.json` clients could access all the
sales. Even if its a special UUID for the shop, that might be good until
somebody shares that UUID or another developers exposes the uuids elsewhere not
realizing they're being used as security... Use proper auth for things that need
to be hidden or it will be exposed.

## Open Web Application Security Project (OWASP)

OWASP is an online community that produces freely available content to help
organizations avoid making costly security mistakes with their software.

The [OWASP API Security Project](https://owasp.org/API-Security/) helps focus
specific on risks and problems that can effect insecure APIs, and illustrating
how these risks may be mitigated. To make sure an API is secure as possible,
it's worth reading through the [OWASP API Security Top 10: 2023
Edition](https://owasp.org/API-Security/editions/2023/en/0x00-header/) and
keeping up to date with new editions when they're released.

## Tooling

Much of this advice and more can be applied to an OpenAPI automatically to help
whole teams make good decisions early on in the API design process. 

- [Vacuum](https://quobix.com/vacuum/) via the built in [OWASP Ruleset](https://quobix.com/vacuum/rules/owasp/).
- [Spectral](https://github.com/stoplightio/spectral) with the [Spectral OWASP Ruleset](https://github.com/stoplightio/spectral-owasp-ruleset).

## Summary

API security isn't a bolt-on; it's a mindset. By making deliberate design
choices around authentication, authorization, data handling, and rate limiting,
many of the pitfalls outlined here and in the OWASP API Security Top 10 can be avoided.

Remember, every design decision is a trade-off. Security measures often add
complexity or impact usability. The goal is to strike the right balance,
keeping the needs of both API consumers and the business in mind. 

There's no need to go to massive massive and intrusive lengths to secure
information that is fine out in the public, but it is important to establish
good practices for limiting interactions for more sensitive data. 

Maybe this means creating more than one API.


 This is the content for the doc api-design/status-codes.md 

 # Using HTTP status codes

Arguments between developers will continue for the rest of time over the
exact appropriate code to use in any given situation, but these are the
most important status codes to look out for in an API, and their accepted meanings:

HTTP status codes can convey a lot of assumptions, but they cannot possibly
cover all situations, so it's important to add something for the human
developers to see what's wrong.

## 2XX is all about success

Whatever the client tried to do was successful, up to the point that the
response was sent.

* **200** - Generic everything is OK.
* **201** - Created something OK.
* **202** - Accepted but is being processed async (for a video means.
encoding, for an image means resizing, etc.).
* **204** - No Content but still a success. Used for a DELETE request, for example.

Example success response

```http
HTTP/1.1 200 OK
Content-Type: application/json

{
  "user": {
    "id": 123,
    "name": "John Doe"
  }
}
```

## 3XX is all about redirection

These are all about sending the calling application somewhere else for the
actual resource. The best known of these are the `303 See Other` and the `301
Moved Permanently`, which are used a lot on the web to redirect a browser to
another URL. Usually a redirect will be combined with a `Location` header to
point to the new location of the content.

## 4XX is all about client errors

Indicate to clients that they did something wrong. They might have
forgotten to send authentication details, provided invalid data, requested a
resource that no longer exists, or done something else wrong which needs fixing.

There are a lot of status codes for client failures, but here are the most
common ones to be found see in API responses:

- *400 Bad Request* - The request was invalid or cannot be served. The exact error should be explained in the error payload.
- *401 Unauthorized* - The request requires an authentication token.
- *403 Forbidden* - The server understood the request, but is refusing it or the access is not allowed.
- *404 Not Found* - There is no resource behind the URI.
- *405 Method Not Allowed* - The request method is known by the server but has been disabled and cannot be used.
- *406 Not Acceptable* - The requested media type is not supported.
- *408 Request Timeout* - The server timed out waiting for the request.
- *409 Conflict* - The request could not be completed because of a conflict.
- *410 Gone* - The resource is no longer available and will not be available again.
- *412 Precondition Failed* - The server does not meet one of the preconditions that the requester put on the request.
- *413 Content Too Large* - The request body is larger than limits defined by server. The server might close the connection or return an `Retry-After` header field.
- *414 URI Too Long* - The URI requested by the client is longer than the server is willing to interpret.
- *415 Unsupported Media Type* - The request entity has a media type which the server or resource does not support.
- *416 Range Not Satisfiable* - The client has asked for a portion of the file, but the server cannot supply that portion.
- *417 Expectation Failed* - The server cannot meet the requirements of the `Expect` request-header field.
- *418 I'm a Teapot* - The Network Working Group were particularly bored one day and did an April fools joke.
- *429 Too Many Requests* - The user has sent too many requests in a given amount of time.

Example error response:

```http
HTTP/1.1 400 Bad Request
Content-Type: application/json

{
  "errors": [
    {
      "code": "VALIDATION_ERROR",
      "message": "Email address is not properly formatted",
      "field": "email"
    }
  ]
}
```

## 5XX is all about service errors

With these status codes, the API, or some network component like a load
balancer, web server, application server, etc. is indicating that something went
wrong on their side. For example, a database connection failed, or another
service was down. Typically, a client application can retry the request. The
server can even specify when the client should retry, using a `Retry-After` HTTP
header.

Key server error codes:

- *500 Internal Server Error* - The server has encountered a situation it doesn't know how to handle.
- *501 Not Implemented* - The request method is not supported by the server and cannot be handled.
- *502 Bad Gateway* - The server, while acting as a gateway or proxy, received an invalid response from the upstream server.
- *503 Service Unavailable* - The server is not ready to handle the request.
- *504 Gateway Timeout* - The server, while acting as a gateway or proxy, did not receive a timely response from the upstream server.

Example error response:

```http
HTTP/1.1 500 Internal Server Error
Content-Type: application/json

{
  "errors": [
    {
      "code": "SERVER_ERROR",
      "message": "Something went wrong on our end"
    }
  ]
}
```

There are a whole bunch of HTTP status codes and it's not important to try and
use them all, but it is good to know what they are so the right one can be used
for the job.

To learn more about HTTP status codes, either read the [full list of status codes from the
IANA](https://www.iana.org/assignments/http-status-codes/http-status-codes.xhtml),
or swing by [http.cats](http://http.cat/) and see what the cats have to say
about it.

## Best practices 

### Ambiguity in error code?

The `404` status code is drastically overused in APIs. People use it for "never
existed", "no longer exists", "you can't view it" and "deactivated", which is
way too vague. That can be split up into `403`, `404` and `410` for different
meanings.

If you get a `403`, this could be because the requesting user is not in the
correct group to see the requested content. Should the client suggest you
upgrade your account somehow? Are you not friends with the user whose content
you are trying to view? Should the client suggest you add them as a friend?

A `410` on a resource could be due to the resource being deleted, or it could be
due to the user deleting their entire account.

Sometimes being more specific about these different use-cases can help, but
sometimes it can leak sensitive information. For example, GitHub prefer to
return a `404` for a private repository that you do not have access to, instead of
a `403`, because a `403` would confirm the existence of the repository. You maybe
don't want people knowing that github.com/acme/your-secret-repo exists, so it's
better to not give out any hints.


 This is the content for the doc api-design/urls.md 

 ---
description: Learn how the Uniform Resource Locator works within a HTTP/REST API.
---

# Structuring URLs

A **URL** (Uniform Resource Locator) is like an postal address on the internet. Just as a postal address tells the postal service where to deliver mail, a URL tells your browser where to go to find a specific website, page, or resource in a way that both humans and computers can understand.

```
https://www.example.org/products/shoes?size=10&color=red
```

At first glance, this may seem complicated, but it's essentially a set of instructions for reaching a specific location on the web, with each word, doc, slash, and other punctuation meaning something in particular.

## The Parts of a URL

Let's break down this URL to understand how each part works, and have a look at URLs in general, before we get stuck into URLs specifically in the context of an API.

### 1. Protocol (Scheme)

```
https://
```

The **protocol** tells your browser how to communicate with the server that hosts the website. The most common protocols are:

- **HTTP** (Hypertext Transfer Protocol): This is the standard protocol used for loading web pages.
- **HTTPS** (HTTP Secure): This is the secure version of HTTP, meaning the data exchanged between your browser and the server is encrypted to increase security of data transfer.

These days most people are using HTTPS for everything. Google prefers HTTPS in search rankings. Using HTTP for an API is advised against strongly. It requires setting up a certificate which used to be expensive, but thanks to projects like [Let's Encrypt](https://letsencrypt.org/) it became free, quick, and relatively easy, to the point where most web hosting offers it by default, `https://` has become the defacto standard.

### 2. Domain Name

```
www.example.org
```

The domain name points your browser to the server where the website is hosted so that users can access the content without having to find and remember long complicated IP addresses.

The **domain name** identifies the website you want to visit. It consists of:
- **Subdomain** (optional): The `www` here is a common subdomain used for websites.
- **Second-level domain (SLD)**: `example`, which is the core part of the domain name.
- **Top-level domain (TLD)**: `.com`, which represents the organization type (`.gov`, `.edu`, `.mil`) the country (`.uk`, `.fr`), or the purpose (`.info`, `.blog`, `.earth`).

### 3. Path

```
/products/shoes
```

The **path** indicates the specific page or resource you're trying to access within the website. It works like an internal "room" in a building—after reaching the site (the building), the path tells you where to go inside. In this case, `/products/shoes` leads to a page showing shoes for sale.

### 4. Query String

```
?size=10&color=red
```

The **query string** is used to send extra information to the website, often to filter or customize what you want to see. In this example, `size=10&color=red` asks the website to show red shoes in size 10.

Query strings start with a `?` and are followed by **key-value pairs** that are separated by `&`. This helps the server return exactly what you're asking for.

## How URLs Work in the Browser

When you enter a URL into your browser or click a link, here's what happens behind the scenes:

1. **Request**: Your browser sends an **HTTP request** to the server at the domain (e.g., `www.example.org`). It uses the protocol (HTTP or HTTPS) to communicate.
2. **Process**: The server checks the **path** (e.g., `/products/shoes`) to find the correct resource (in this case, a web page about shoes).
3. **Response**: If the server finds the resource, it sends back an **HTTP response** containing the content (e.g., a web page, an image, or a file). If the resource doesn't exist, you might get an error, like "404 Not Found."

The URL is the full address your browser needs to complete these steps, making it a crucial part of how you access information online.

## URLs in REST APIs: A Gateway to Resources

In the world of **REST APIs**, URLs are not just for webpages but also for identifying and interacting with specific resources across the API, across an organization, or across the entire Internet. APIs are used by systems and applications to communicate with each other, and URLs act as a unique identifier for that resources, as well as acting as an address with to find more information.

Here's an example of how URLs work in REST APIs:

```
https://api.example.org/collections/shoes/products?sort=date&size=10&color=red
```

This may look similar to a regular web URL, and it absolutely is. All the same concepts existing in an API as in a general website, because the Internet is built using the same principles as a REST API. In an API, each part of the URL has a specific purpose for data exchange.

### 1. Protocol

```
https://
```

Just like with a regular website, the **protocol** specifies whether the communication should be secure (HTTPS) or not (HTTP). A website might redirect `http://` requests to `https://`, but for an API it's better to simply reject requests as they could be leaking sensitive information like authentication tokens, passwords, or other private information.

### 2. Domain

```
api.example.org
```

The **domain** points to the API server that you're communicating with. Many APIs use a subdomain like `api` to distinguish the API service from the main website.

You might have one API subdomain `api.example.org` for a single API, or multiple subdomains for different services `stats.api.example.org` or `servicename.example.org`. Alternatively you might decide to host the API on the main domain and use a path for the API `example.org/api/`. 

It doesn't matter much these days. Splitting web traffic between two different domains - e.g.: `www.example.org` and `api.example.org` had some pretty sizable performance concerns back in the HTTP/1.x days, as browsers would have to do DNS resolution and connection establishment repeatedly adding latency to the web request, but in a HTTP/2 (and HTTP/3) world this is no longer relevant, but some people have strong preferences based on historical reasons or infrastructure requirements.

### 3. Path

```
/collections/shoes/products
```

In a REST API, the **path** refers to a specific **resource** on the server. This e-commerce API is representing Collections and Products, and products can be assigned to collections, giving us the following structure:

- `/collections` refers to a list of collections.
- `/collections/shoes` refers to the **shoes** collection in particular. This is not defined in code, but is known as a "path parameter" allowing you to look up `shoes` in the database.
- `/collections/shoes/products` refers to the products in the shoes collection.

This allows API consumers to interact with the API in a structured way, and to access the data they need reliably in a predictable and generally optimizable way.

There are a few names for different parts of the path when used for a REST API.

1. `/users` this is known as a **Collection**.
1. `/users/<id>` this is a **Resource**.
1. `/users/<id>/posts` this is a **Sub-Collection**.
1. `/users/<id>/posts/<post-id>` this is a **Sub-Resource**.

Having sub-collections and sub-resources is known as "nesting", and you want to limit nesting as much as possible. It can feel neat and sensible at first, but it's easy to get carried away and people do things like `/users/<id>/orders/<order-id>`. This is not only unnecessary and complicated, but can lead to a few awkward problems.

**Resources are not strictly dependent on their parent:** The hierarchy becomes restrictive when sub-resources can exist independently or relate to multiple entities. For instance, an `order` might belong to a `user`, but you may also need to access `orders` independently or through other entities (like `products` or `shops`).

**Excessive Depth:** Deeply nested resources can lead to complex, hard-to-manage URL structures. For example:

```
/users/123/orders/456/products/789/reviews/1011
```

This URL indicates a very rigid hierarchy where reviews are always tied to a product that is tied to an order for a user. If your use case changes, or if you want to access reviews independently (e.g., by searching for reviews across all products or orders), this rigid hierarchy becomes unwieldy.

**Loss of Flexibility:** As your application evolves, you might need to interact with resources in ways that don’t fit the original hierarchy. Overly strict nesting forces clients to always traverse through the parent resources, even when it's unnecessary or illogical for certain operations. For example, fetching an order might not always need to be tied to a user, especially if your system grows to allow for admin views where orders are retrieved without needing the user context.

**Duplication of Resources:** If a resource belongs to multiple parents, nesting creates redundant endpoints. For example a sub-resource like this:

```
/users/123/orders/456
/shops/789/orders/456
```

The order belongs to both a user and a shop, forcing you to maintain multiple endpoints for the same resource. This increases code complexity, and makes network caching confusing and inconsistent. There's no reason to litter the URL with irrelevant parent data, that sub-resource example could just be: 

```
/orders/456
```

Instead of using sub-collections, we can use a "top-level collection" with query parameters.

```
/orders?user-id=123
/orders?shop-id=789
```

### 4. Query Parameters

```
?sort=date&size=10&color=red
```

In REST APIs, **query parameters** are used to refine the data you're requesting, allowing for [filtering & sorting](/api-design/filtering-responses.mdx), and [pagination](/api-design/pagination).

- `size=10` asks the server to return 10 products.
- `color=red` asks the server to return only red products.
- `sort=date` asks the server to return the products sorted by date.

The first query parameter is demarcated with a `?`, and subsequent query parameters are separated by `&`.

Filtering can be done for related content too, with id's or other criteria being passed in: 

```
/orders?user-id=123&status=pending
```

Query string parameters are handy, but the more an API uses, the harder it is to [cache](/api-design/caching). There is no right or wrong number of query parameters to use, just try to weigh up the value of the functionality they will offer, against the performance cost they may incur. 

### Summary

A URL is like an address that tells your browser or application where to find a resource on the internet or an API. In a web context, URLs help us navigate to specific pages, while in REST APIs, they act as powerful tools for accessing and manipulating data.

By understanding the different parts of a URL—protocol, domain, path, and query parameters—you can better navigate the web and use APIs to retrieve or update information in a precise, structured way. 


 This is the content for the doc api-design/versioning.mdx 

 ---
description: Learn how to manage API versioning and evolution to ensure smooth transitions and backward compatibility for clients.
---

import { Callout } from "~/components";

# Versioning & Evolution

Once an API has been designed, built, deployed, and integrated with by various
consumers, changing the API can be very difficult.

**Additive Changes**: Adding new endpoints, adding new properties to a response,
or introducing optional parameters are non-breaking changes, and can typically
be done without significant issues.
  
**Breaking Changes**: Removing or renaming endpoints, removing required fields,
or changing response structures are considered breaking changes. These have
the potential to disrupt existing client applications, resulting in errors and
loss of functionality. Clients, especially paying customers, may face the
expense and time-consuming task of adapting their code to accommodate these
changes.

For effective management of changes, developers must navigate versioning and
evolution of APIs carefully, ensuring that client integrations are not
negatively impacted. Let's explore these challenges in more detail.

## When are API changes an issue

Some APIs are built purely to service a single application. This might be a
backend for a web application, handled by a single full-stack developer or team
that manages both the frontend and the API. In this case, changes aren't as
problematic because both the frontend and backend can be updated simultaneously,
ensuring that there's no risk of breaking integration.

In most cases APIs are consumed by multiple clients, ranging from other teams
within the same organization to external customers. This introduces
complexities:

- **Internal Clients**: Even when the consumers are within the same
  organization, changes may require coordination, especially if the API is used
  by different teams working on separate services. The timing of updates and
  changes can cause delays or disruptions.

- **External Clients**: If the API is being used by third-party clients,
  particularly paying customers, changes can become even more difficult.
  External clients may resist updates due to the effort and risk involved in
  modifying their integrations. A major change could result in lost business,
  dissatisfaction, or churn.

When API consumers are not in sync with the development team, managing
versioning becomes essential to ensure smooth transitions.

## Why APIs need to change

Has anyone ever released any software then thought: "That's perfect, no change
needed"?

Probably not. APIs evolve over time like any other software, whether it's due to
changing business requirements, feedback from users, or the need to adopt new
technologies. APIs are rarely “perfect” and immutable.

Just like software versioning, APIs also require a versioning system to
accommodate changes. Developers use version numbers to signify changes in the
API contract, allowing consumers to choose which version of the API they wish to
use. This ensures backward compatibility for existing clients while introducing
improvements and fixes in newer versions.

With most software users can have any version running, with multiple versions of
the software running on various users computers at once. Common conventions,
including [Semantic Versioning](https://semver.org/), use three numbers: major,
minor, and patch, so some users might be running 1.0.0 whilst others run 1.0.2
and eventually some may be on 2.1.3.

Breaking changes might look like:

- A change to the endpoint structure.
- Adding a new required field.
- Removing a field from a response.
- Changing the behavior of an endpoint.
- Changing validation rules.
- Modifying the response format (e.g.: implementing a standard data format like JSON:API).

If any of this is done, a new API version may be required to avoid breaking existing clients.

## Versioning an API

API versioning involves assigning a version number or identifier to the API,
essentially creating multiple different APIs which are segmented in some clear
way so that the consumer can specify which version of the API they wish to
interact with.

There are countless ways people have tried to solve this problem over time, but
the two main approaches are:

### URL versioning

URL versioning is one of the most common approaches. It involves including a
version number in the URL, segmenting the API. Typically, only a major version
is used, as seen in this example:

```http
GET https://example.com/api/v1/users/123
```

```json
{
	"id": 123,
	"first_name": "Dwayne",
	"last_name": "Johnson"
}
```

In this example, `v1` refers to the version of the API, and its whatever the
resource was designed at first. 

As the API grows a new version is introduced to accommodate changes separate
from the first version.

```http
GET https://example.com/api/v2/users/3a717485-b81b-411c-8322-426a7a5ef5e6
```

```json
{
	"id": 123,
	"full_name": "Dwayne Johnson",
	"preferred_name": "The Rock"
}
```

Here, the v2 endpoint introduces a few notable changes: they phased out
auto-incrementing IDs as per the [security advice](/api-design/security),
ditched the [fallacy of people having two
names](https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/),
and helped people put in a preferred name to show publicly instead of forcing
everyone to publicize their legal name. Great, but this is a big change. 

If this change was deployed on the `/v1` version it would have broken most
clients usage, they would see 404 errors using the old IDs, and the fields have
changed so validation failures would occur.

As this is being run simultaneously under `/v2`, both can be used at once. This
allows clients to migrate at their own pace, and for the API to be updated
without breaking existing clients.

### Media-type versioning

Instead of embedding the version number in the URL, media type versioning places
the versioning information in the HTTP `Accept` header. This allows for more
flexible management of the API contract without altering the URL structure.

```http
GET https://example.com/api/users/123
Accept: application/vnd.acme.v2+json
```

```json
{
	"id": 123,
	"full_name": "Dwayne Johnson",
	"preferred_name": "The Rock"
}
```

In this case, the client specifies the version they want by including the
`Accept` header with the version identifier (e.g.:
`application/vnd.acme.v2+json`). The advantage is that the API URL remains
clean, and the versioning is managed through the HTTP headers.

This approach is less common than URL versioning, and has a few downsides. It's a
bit more complex to implement, and it's not as easy to see which version of the
API using.

## API evolution as an alternative

While versioning is a popular solution to API changes, **API evolution** is an
alternative focuses on maintaining backward compatibility and minimizing
breaking changes. Instead of introducing entirely new versions, API developers
evolve the existing API to accommodate new requirements, but do so in a way that
doesn't disrupt clients.

API evolution is the concept of striving to maintain the "I" in API, the
request/response body, query parameters, general functionality, etc., only
breaking them when absolutely necessary. It's the idea that API
developers bend over backwards to maintain a contract, no matter how annoying
that might be. It's often more financially and logistically viable for the API
developers to bear this load than dumping the workload onto a wide array of
consumers.

### API evolution in practice

To work on a realistic example, here's a simple change that could come up:

> The property `name` exists, and that needs to be split into `first_name` and `last_name` to support Stripe's name requirements.

A minor example, but removing name and making all consumers need to change all code to use the two new fields would be a breaking change. There are ways to retain backwards compatibility.

Most web application frameworks commonly used to build APIs have a feature like "serializers", where database models are turned into JSON objects to be returned, with all sensitive fields removed and any relevant tweaks or structure added. 

The database might have changed to use `first_name` and `last_name`, but the API does not need to remove the name property. It can be replaced with a "dynamic property" which joins the first and last names together and returns it in the JSON.

```ruby
class UserSerializer 
  include FastJsonapi::ObjectSerializer

  attributes :name, :first_name, :last_name
    "#{object.first_name} #{object.last_name}" 
  end 
end
```

```http
GET https://api.example.com/users/123
```

```json
{
  "id": 123,
  "name": "Dwayne Johnson",
  "first_name": "Dwayne",
  "last_name": "Johnson"
}
```

When a `POST` or `PATCH` is sent to the API, the API does not need to think about a version number to notice that `name` is being sent. IF name is sent it can be split, and if first_name and last_name are sent it can handle as expected.

```ruby
class User < ApplicationRecord
  def name=(name)
    self.first_name, self.last_name = name.split(' ', 2)
  end
end
```

A lot of changes can be handled with new properties, and supporting old properties indefinitely, but at a certain point that becomes cumbersome enough to need a bigger change.

When an endpoint is starting to feel a bit clunky and overloaded, or fundamental relationships change, an API rewrite can be avoided by evolving the API with new resources, collections, and relationships.

### Changing the domain model

In the case of [Protect Earth's](https://www.protect.earth/), a reforestation and rewilding charity, the Tree Tracker API needed some fundamental change. It used to focus on tracking trees that were planted, recording a photo and coordinates, and other metadata to allow for sponsoring and funding tree planting.

There's a `/trees` resource, and `/orders` has a `plantedTrees` property, but the charity expanded beyond trees to sowing wildflower meadows, rewetting peat bogs, and clearing invasive species. Instead around adding `/peat`, and `/meadows` resources, the API became more generic with a `/units` collection.

Removing `/trees` or `plantedTrees` would break customers, and that would stem the flow of funding. API evolution focuses on adding new functionality without breaking existing clients, so instead of removing the `/trees` endpoint, the API now supports both `/units` and `/trees`, with the `/trees` resource simply filtering the `/units` based on the `type` field:

```http
GET https://api.protect.earth/trees
```

```json
{
  "id": 123,
  "species": "Oak",
  "location": {
    "latitude": 42.0399,
    "longitude": -71.0589
  }
}
```

```http
GET https://api.protect.earth/units
```

```json
{
  "id": 123,
  "type": "tree",
  "species": "Oak",
  "location": {
    "latitude": 42.0399,
    "longitude": -71.0589
  }
}
```

This allows existing developers to continue using the `/trees` endpoint while new developers can use the `/units` endpoint. The API evolves to support new functionality without breaking existing clients.

What about the `/orders` having `plantedTrees` on them? Removing this property would be a breaking change, so a backwards compatible solution is needed, and with API evolution there are countless options.

It's possible to add both an old and a new property, allowing clients to migrate at their own pace. This can be done by adding a new `allocatedUnits` property to the `/orders` resource, while keeping the old `plantedTrees` property:

```http
GET https://api.protect.earth/orders/abc123
```

```json
{
  "id": "abc123",
  "organization": "Protect Earth",
  "orderDate": "2025-01-21",
  "status": "fulfilled",
  "plantedTrees": [
    {
      "id": 456,
      "species": "Oak",
      "location": {
        "latitude": 42.0399,
        "longitude": -71.0589
      }
    }
  ],
  "allocatedUnits": [
    {
      "id": 456,
      "type": "tree",
      "species": "Oak",
      "location": {
        "latitude": 42.0399,
        "longitude": -71.0589
      }
    }
  ]
}
```

However, for orders with 20,000 trees this means there will be 40,000 items across those two sub-arrays with both of them being identical. This is a bit of a waste, but really this is helping to highlight an existing design flaw. Why are these sub arrays not [paginated](/api-design/pagination), and why are units being embedded inside orders?

They are different resources, and its far easier to treat them as such. API evolution gives us a chance to fix this.

There is already an `/units` endpoint, let's use that.


```http
GET https://api.protect.earth/orders/abc123
```

```json
{
  "id": "abc123",
  "organization": "Protect Earth",
  "orderDate": "2025-01-21",
  "status": "fulfilled",
  "unitType": "peat",
  "links": {
    "units": "https://api.protect.earth/units?order=abc123"
  }
}
```

That way, the order resource is just about the order, and the units are about the units. This is a more RESTful design, and it's a better way to handle the relationship between orders and units.

Where did "plantedTrees" go? It's moved behind a switch. It will only show up on orders for trees, and all other unit types can be found on the `units` link which benefits from full pagination.

### Deprecating endpoints

All this flexibility comes with a tradeoff, it's more work to maintain two endpoints, because there may be performance tweaks and bug reports that need to be applied to both. It's also more work to document and test both endpoints, so it can be a good idea to keep an eye on which endpoints are being used and which aren't, and remove the old ones when they're no longer needed.

Old endpoints can be deprecated using the `Sunset` header. 

```http
HTTP/2 200 OK
Sunset: Tue, 1 Jul 2025 23:59:59 GMT
```

Adding a `Sunset` header to `/trees` will communicate to API consumers that the endpoint will be removed, and if it's done with sufficient warning and with a clear migration path, it can lead to a smooth transition for clients.

Further details can be provided in the form of a URL in a `Link` header and the `rel="sunset"` attribute.

```
HTTP/2 200 OK
Sunset: Tue, 1 Jul 2025 23:59:59 GMT
Link: <https://example.org/blog/migrating-to-units>; rel="sunset"
```

This could be a link to a blog post or an upgrade guide in documentation.

### Deprecating properties

Deprecating properties is a little more difficult, and generally best avoided whenever possible. It's not possible to use `Sunset` to communicate a property going away as it only applies to endpoints, but OpenAPI can help.

OpenAPI v3.1 added the `deprecate` keyword, to allow API descriptions to communicate deprecations as an API evolves.

```yaml
components:
  schemas:
    Order:
      type: object
      properties:
        plantedTrees:
          type: array
          items:
            $ref: '#/components/schemas/Tree'
          deprecate: true
          description: >
            An array of trees that have been planted, only on tree orders.
            *Deprecated:* use the units link relationship instead.
```

This will show up in the documentation, and can be used by SDKs to warn developers that they're using a deprecated property. 

Removing the `plantedTrees` property from the API entirely could be done, but it's a breaking change, and it's best to avoid breaking changes whenever possible. 

A better option is to stop putting the `plantedTrees` property into new orders starting on the deprecated date, and leave it on older orders.

Another change being added to the API is the concept of orders expiring, because companies should have got their data out of the API within six months, otherwise the information is archived to reduce wasting emissions as database expands. If `plantedTrees` is not added to new orders, and eventually orders archive, then eventually it will be gone completely and can be removed from code.

### API design-first reduces change later

Some APIs have kept their v1 API going for over a decade, which suggests they probably didn't need API versioning in the first place.

Some APIs are on v14, because the API developers didn't reach out to any stakeholders to ask what they needed out of an API and just wrote loads of code, rushing to rewrite it every time a new consumer came along with slightly different needs instead of finding a solution that worked for everyone.

Doing more planning, research, upfront API design, and prototyping can cut out the need for the first few versions, as many of those come from not getting enough user/market research done early on. This is common in startups that are moving fast and breaking things, but it can happen in any size business.

## Summary

When it comes to deciding between versioning and evolution, consider how many consumers will need to upgrade, and how long that work is likely to take. If it's two days of work, and there are 10 customers, then that's 160 person-hours. With 1,000 customers, that's 16,000 person-hours. 

At a certain point it becomes unconscionable to ask paying customers to all do that much work, and it's better to see if it could be handled with a new resource, new properties, or other backwards compatible changes which can slowly phase out their older forms over time, even if its a bit more work.


 This is the content for the doc blog/5-potential-use-cases-for-Arazzo/index.mdx 

 ---
title: "5 potential use cases for Arazzo"
description: "Discover how Arazzo simplifies API workflows, enhances AI accuracy, and streamlines development, security, and testing."
image: "/media/5-potential-use-cases-for-Arazzo.png"
date: 2025-01-22
authors:
  - name: Bill Doerrfeld
  - image_url: "/media/author-headshots/bill.jpeg"
tags:
  - OpenAPI
featured_image: "/media/5-potential-use-cases-for-Arazzo.png"
---

import { Callout } from "~/components";

Digital interactions often involve sequences of API calls to achieve goals like user authentication, booking a flight, or ordering a product online. These multi-step workflows rely on passing parameters between various services, with each step dependent on the outcome of preceding API calls. Although API-based workflows are commonplace, they typically aren't formally documented, hindering repeatability and developer experience.

Enter [Arazzo](https://github.com/OAI/Arazzo-Specification), a new specification from the [OpenAPI Initiative](https://www.openapis.org/) that can be used to describe an interconnected series of API calls and their dependencies. Announced in [mid-2024](https://youtu.be/EQaGHjMIcD8?si=CxVLfxyLAn7cESM2), Arazzo is on [version 1.0.1](https://github.com/OAI/Arazzo-Specification/pull/318) at the time of writing. 

Italian for "tapestry," Arazzo is aptly named since it can be used to weave together sequences of API calls to illustrate a specific business pattern. Although new on the scene, the API community is excited about the potential of using Arazzo to standardize deterministic workflows for various use cases.

There are countless examples of interlinked API sequences out there, and defining them could greatly boost API-driven development. From better articulating common customer flows to empowering quality engineers and optimizing AI agents, there is a fountain of [possibilities for using Arazzo](https://nordicapis.com/3-example-use-cases-for-arazzo-descriptions/). Below, we'll explore a handful of possible use cases and how they could benefit developer consumers and their end business objectives.

## 1. Making AI more deterministic

AI has become a household technology. Yet, large language models (LLMs) are still prone to inaccuracies and hallucinations. Plus, autonomously integrating with APIs and performing interconnected interactions still poses a challenge. This is in part due to a lack of repeatable machine-readable API-related semantics for LLMs.

Arazzo could be used to apply more deterministic API processes to AI agents. By ingesting OpenAPI specifications and Arazzo definitions, an AI could understand what operations are available and what workflows they should invoke to perform common actions. This could greatly empower AI agents with greater context, optimize their behaviors, and help reduce errors and randomness.

For example, consider an LLM-powered AI assistant within an online food ordering system. Suppose a user asks it to 're-order my last Thai dinner.' An AI could invoke an Arazzo description related to reordering, detailing all the required steps, such as order look-ups, availability and balance checks, and payment processing, to set up and initiate a reorder.

## 2. Simplifying multi-endpoint libraries

Have you ever read an OpenAPI definition? The average API has 42 endpoints, and these YAML or JSON files can become pretty unwieldy, with thousands of lines and metadata that are often irrelevant to an individual use case. To make matters more complicated, many workflows call APIs from disparate sources, including internal and external services.

Arazzo could be used to greatly abstract complexity for developers by helping to document and generate workflows around common business functions. Rather than fully describing every endpoint and method in an API, Arazzo could help generate [higher-level SDKs](https://speakeasy.hashnode.dev/apis-vs-sdks-why-you-should-always-have-both) that are multi-endpoint and use-case-specific. 

For instance, consider a developer tooling company that offers recruiting software as a platform. Suppose a common functional use case is to recruit a candidate that matches certain criteria. Well, an Arazzo workflow could document how to search for a user, check they are free for work, initiate outreach in the system, and update the status to `contacted`. It could even automate external calls for background checks or pull in public social media information.

Arazzo could deterministically describe the API calls and parameters required to achieve this process. These libraries could even combine interactions across various APIs, greatly streamlining the developer experience.

## 3. Demystifying authorization flows

Modern applications don't just authenticate the user — they make sure the user has the correct permissions. Behind the scenes, APIs typically require authorization flows using OpenID Connect and OAuth, involving back-and-forth exchanges between the requesting client, an API gateway, and an external identity server. 

Arazzo could be used to formulate a sequence of calls for an [OAuth service](https://github.com/OAI/Arazzo-Specification/blob/main/examples/1.0.0/oauth.arazzo.yaml), making a process like [refreshing an access token](https://github.com/OAI/Arazzo-Specification/blob/main/examples/1.0.0/oauth.arazzo.yaml) more transparent and repeatable. 

For example, the OAI provides an example of using Arazzo to describe a [Financial Grade API (FAPI) profile](https://github.com/OAI/Arazzo-Specification/blob/main/examples/1.0.0/FAPI-PAR.arazzo.yaml), which is a common flow for PSD2 open banking scenarios. Defining this could streamline how developers implement financial security flows, removing some guesswork from the picture. That said, the authentication aspect of OAuth flows are often unspecified and will depend on the exact configurations of the identity server. 


## 4. Automating end-to-end API testing

The standards for digital experiences are high, meaning quality assurance or site reliability engineers have their work cut out for them. API testing takes this to a whole new level since so much can go wrong with a programmatic interface that is continually updated and versioned. It takes a broad range of routine tests to ensure APIs are stable. From functional testing to performance testing, reliability testing, validation testing, security testing, chaos engineering, linting, and more. 

QA engineers often create Postman Collections that save API calls, but wouldn't it be nice to automate API testing? Arazzo could greatly aid [end-to-end testing](https://www.speakeasy.com/post/e2e-testing-arazzo) to ensure sequences of API calls are fully functional and meet service-level agreements, bringing efficiency benefits to the testing process.

Consider engineers working within a healthcare company — these folks could use Arazzo workflows to automate regulatory compliance checks. For instance, a conformance testing workflow could test whether a system violates regulations around data sharing across regional boundaries when passed certain geographic-specific parameters.

## 5. Standardizing patterns in unified APIs

Unified APIs take the integration hassle out of aggregating tens, if not hundreds, of software systems and endpoints for similar domains. For instance, take Avage API for construction software, Argyle for payroll services, Duffel for airline booking, or Plaid for integrating with bank data. Many more unified APIs exist for categories like CRM, cloud storage, accounting, and more.

Unified APIs could greatly benefit from Arazzo since they already define common user experiences across software domains. There are many common, repeatable pathways within a particular domain. For instance, a unified CRM API could create an agnostic workflow for adding a new qualified lead to a CRM system. Actionable flows for standard processes like this could improve the unified API integration developer experience.

## Optimizing working with Arazzo

It's good to note that Arazzo's actual utility will hinge on whether the API tooling ecosystem embraces it. Part of this will be making working with Arazzo more streamlined. Similar to API definition linting tools, the same thing for Arazzo is emerging, enabling you to validate that the Arazzo specification is correct. Speakeasy has open-sourced one such parser for this very purpose, [`speakeasy lint arazzo`](https://www.speakeasy.com/docs/speakeasy-reference/cli/lint/arazzo). Such tools will help API providers and API management platforms integrate Arazzo more easily into their pipelines and offerings.

## Let's see what the builders build

By defining common workflows, Arazzo could greatly reduce the mean time for integrations and help standardize complex descriptions typically housed in PDFs or Word documents outside of official API documentation. For developers, it can generate useful onboarding information to create more interactive, "living" workflow documentation.

Beyond the examples above, there are countless other [potential use cases](https://nordicapis.com/3-example-use-cases-for-arazzo-descriptions/) for Arazzo specifications. The Arazzo Specification repository also includes use cases such as securing a loan at a [buy now pay later](https://github.com/OAI/Arazzo-Specification/blob/main/examples/1.0.0/bnpl-arazzo.yaml) (BNPL) platform, or [applying coupons to a purchase](https://github.com/OAI/Arazzo-Specification/blob/main/examples/1.0.0/pet-coupons.arazzo.yaml). 

Arazzo is the first of its kind — a standard with growing industry momentum to denote composite API workflows around specific business goals. From an end consumer perspective, the standard could usher in more predictable [AI agents](https://thenewstack.io/its-time-to-start-preparing-apis-for-the-ai-agent-era/) and better cross-platform customer experiences. For developers, Arazzo could streamline stitching together common request patterns, demystify security flows, and make testing easier. 

A lot is hypothetical now, but the future is looking bright for this new standard. Now, it's just up to the builders to build.


 This is the content for the doc blog/announcing-easysdk-generator.mdx 

 ---
title: "Announcing: EasySDK Generator"
description: "A SDK generator that improves upon the OpenAPI service. Speakeasy is able to handle typing correctly & is opinionated about what makes for a good SDK."
image: "/media/announcing-easysdk-generator.png"
date: 2022-08-31
authors:
  - name: Sagar Batchu
  - image_url: 'https://uploads-ssl.webflow.com/62ccd7b208cab0723d026273/62cdf303b28bf9598d7a6b63_sagar_headshot-p-500.jpeg'
tags:
  - Product Updates
featured_image: "/media/announcing-easysdk-generator.png"
---

## What We’ve Built

We’re excited to announce that we are publicly launching an [SDK generator](#) that improves upon the [OpenAPI service](https://github.com/OpenAPITools/openapi-generator).  In our view the biggest problem with the OpenAPI generator was that it produced client libraries that were untyped and unopinionated. That’s why last week we focused on building a generator that is able to handle typing correctly and is opinionated about what makes for a good SDK:

- **Low-dependency** - To try and keep the SDK isomorphic (i.e. available both for Browsers and Node.JS servers), we wrap axios, but that’s it.This is intended to be idiomatic typescript; very similar to code a human would write; with the caveat that the typing is only as strict as the OpenAPI specification.
- **Static Typing** - At this point static typing is everywhere. So wherever possible, we generate typed structures, construct path variables automatically, pass through query parameters, and expose strictly typed input / output body types.
- **Language idiomatic & opinionated** - There’s value in being neutral, but we felt like there is more value in being opinionated. We’ve made choices for how things like Pagination, Retries (Backoff/Jitter etc), Auth integrations, should be handled in the SDK.

## Why We Think SDKs Are Important

A good developer experience means flattening the learning curve by meeting developers where they already; that’s why every company with an [API platform](/post/why-an-api-platform-is-important/) should strive to offer SDKs for integrating with their APIs.  Language-idiomatic SDKs improve user productivity by removing the need for writing the boilerplate required to send API requests and parse response objects.  Companies that offer SDKs get faster adoption, spend less time troubleshooting, and provide an overall better developer experience.

We look forward to hearing from the community what they think of the service. We’d love to know what else people would want to see included in an SDK, and what languages we should support next.


 This is the content for the doc blog/api-auth-guide/index.mdx 

 ---
title: "Guide to API Auth & A Novel Approach"
description: "A deep dive on the approaches to API Auth, and a novel approach we tried."
keywords: [api, api key management, api key, api auth, auth, shared secrets, signed tokens, oauth2, developer experience, devex, dx]
image: "/media/guide-api-auth.png"
date: 2023-01-20
authors:
  - name: Thomas Rooney
  - image_url: "/media/author-headshots/thomas.jpeg"
tags:
  - API Advice
  
featured_image: "/media/guide-api-auth.png"
---

## API Authentication Overview

If you're selling an API Product, one of the first decisions you're going to have to make is how to authenticate your users. This decision, once made, is hard to go back upon; any significant change will require user action to keep their integration working.

This blog post is intended to be a guide to the different API Authentication methods in common use, and the tradeoffs to consider between them, generally categorized into the following 3 metrics.

* **Time to API Call**
* **Ease of API Producer Implementation**
* **Ease of API Consumer Integration**

This post is inspired by work we recently undertook at Speakeasy (API DevEx tooling company) to build an API Key authorization flow that integrates into any API Gateway, and allows users to self-service and rotate their keys. For this project we evaluated all the standard approaches to authentication before deciding on a novel approach: signed tokens as API Keys, but with 1 signing key per API key. What follows are a deep dive on the typical methods, as well as our approach.

## Types of Authentication

Authenticated APIs require some way of identifying the client making the request, so that the API provider can authorize the requests to be processed, and identify the subset of data that can be accessed.

From an API Consumer perspective, the flow is usually:

1. You get a **Secret Key** from the service (e.g. in an authenticated Web UI).
2. You store that **Secret Key** somewhere securely, such that your application can access it.
3. You use that **Secret Key** within your application logic to authenticate with the API.

This is incredibly simple, and hence has great Developer Experience (DX).

However, from an API Producer Perspective, it's not so simple. There are choices you need to make about how the **Secret Key** is implemented which greatly impacts the Security Model of your application. Once you have users in production, Machine to Machine (M2M) authentication is hard to change, assuming you don’t want to break existing integrated users. Therefore, choose wisely:

1. **Opaque Token / Shared Secrets**
2. **Public / Private Key Pairs**
3. **OAuth 2.0 Secrets**
4. **Signed Tokens (one Signing Key)**
5. **Signed Tokens (many Signing Keys)**

Let’s look at the advantages and disadvantages of each approach…

### Opaque Tokens / Shared Secrets

An _Opaque Token_ is a _Shared Secret_ that is used to authenticate a client. It is _Opaque_ in that there is no message to read: all you can do with it is look up its existence in a centralised store (e.g. a database), and then, if it exists, you know who the user is.

This is functionally the same as a password, except that it is ideally generated by a process which ensures the entropy of the token is high enough that it is entirely unguessable.

Assuming this token is passed into the API in the `Authorization` header, from an API Consumer perspective, accessing the API is as simple as:

```sh
curl https://api.example.com/v1/endpoint
  --header "Authorization ${API_KEY}"
```

From an API Producer perspective, there's a few more moving parts, but it's usually pretty trivial to implement:

```mermaid
sequenceDiagram
  participant API Consumer[Bob]
  participant API
  participant Authorization Server
  rect rgba(0, 0, 0, 0.05)
  note over API Consumer[Bob],Authorization Server: Configure Key
  API Consumer[Bob]-->>API: AUTHORIZED SESSION [Bob] with [API]
  API-->>API Consumer[Bob]: 
  API Consumer[Bob]->>API: Request New Secret Key
      Note right of API: Generate some random bytes.<br>Render bytes as [Secret Key]
  API->>Authorization Server: Store [Bob, [Secret Key]]
  Authorization Server-->>API: Success
  API-->>API Consumer[Bob]: [Secret Key]
  end
  rect rgba(0, 0, 0, 0.05)
  note over API Consumer[Bob],Authorization Server: Authorized API Request
  API Consumer[Bob]->>API: [Req] with [Secret Key]
  API->>Authorization Server: Who is this? [Secret Key]
  Authorization Server-->>API: Bob
      Note right of API: Process Request under context Bob
  API-->>API Consumer[Bob]: [Resp]
  end
```

* **Time to API Call**: As Fast as it gets.
* **Ease of API Consumer Integration**: Very Easy
* **Ease of API Producer Implementation**: Very Easy
* **Other Considerations**:
  * Often Difficult to integrate into an API Gateway
  * Any validation requires a lookup where-ever these are stored. If you solely store them in the DB, this means that all lookups require a DB Read, which can cause additional latency to every request.

### OAuth 2.0 Secrets

```mermaid
sequenceDiagram
  participant API Consumer[Bob]
  participant API
  participant Authorization Server
  rect rgba(0, 0, 0, 0.05)
  note over API Consumer[Bob],Authorization Server: Configure Key
  API Consumer[Bob]-->>API: AUTHORIZED SESSION [Bob] with [API]
  API-->>API Consumer[Bob]: 
  API Consumer[Bob]->>API: Request New oAuth Application
      Note left of API: Generate some random bytes.<br>Render bytes as [Client ID], [Client Secret]<br>(AKA Secret Key). 
  API->>Authorization Server: [Bob, [Secret Key]]
  Authorization Server-->>API: Success
  API-->>API Consumer[Bob]: [Secret Key]
  end
  rect rgba(0, 0, 0, 0.05)
  note over API Consumer[Bob],Authorization Server: Authorized API Request
  API Consumer[Bob]-->>Authorization Server: SECURE SESSION [Unknown] with [Authorization Server]
  Authorization Server-->>API Consumer[Bob]: 
  API Consumer[Bob]->>Authorization Server: GET /oauth/token with [Secret Key]
    Note left of Authorization Server: Lookup [Secret Key] as Bob<br>Generate short-lived JWT with {"sub":"Bob"} claims
  Authorization Server-->>API Consumer[Bob]: JWT["Bob"]
  API Consumer[Bob]-->>API: SECURE SESSION [Unknown] with [API]
  API-->>API Consumer[Bob]: 
  API Consumer[Bob]->>API: [Req] with header "Authorization Bearer \${JWT["Bob"]}"
      Note left of API: Process Request under context Bob
  API-->>API Consumer[Bob]: [Resp]
  end
```

OAuth is commonly associated with user authentication through a social login. This doesn't make sense for API applications, as the system authenticates and authorizes an application rather than a user.

However, through the Client Credentials Flow ([OAuth 2.0 RFC 6749, section 4.4](https://tools.ietf.org/html/rfc6749#section-4.4])), a user application can exchange a **Client ID**, and **Client Secret** for a short lived Access Token.

In this scenario, the **Client ID** and **Client Secret** pair are the **Shared Secret** which would be passed to the integrating developer to configure in their application. In the OpenID Connect (OIDC) protocol, this happens by making a request to the **/oauth/token** endpoint.

```sh
TOKEN=$(curl --header "Content-Type: application/x-www-form-urlencoded" \
  --request POST \
  --data "grant_type=client_credentials" \
  --data "client_id=CLIENT_ID" \
  --data "client_secret=CLIENT_SECRET" \
  https://auth.example.com/oauth/token | jq -r '.access_token')

curl --header "Authorization Bearer $TOKEN" \
  https://api.example.com/v1/endpoint
```

* **Time to API Call**: Slow.
* **Ease of API Consumer Integration**: Difficult
* **Ease of API Producer Implementation**: Difficult
* **Other Considerations**:
  * Can enable additional use-cases, such as granting third party systems the capability to make API calls on behalf of your users.

### Public/Private Key Pairs

A Public Private Key Pair allows for a user to hold a secret and the server to validate that the user holds a secret, without the server ever holding the secret. For the purpose of authenticating users, this mechanism has the lowest attack surface : i.e. it is the most secure.

```mermaid
sequenceDiagram
  participant API Consumer[Bob]
  participant API
  participant Authorization Server
  rect rgba(0, 0, 0, 0.05)
  note over API Consumer[Bob],Authorization Server: Configure Key
  API Consumer[Bob]-->>API: AUTHORIZED SESSION [Bob] with [API]
  API-->>API Consumer[Bob]:  
  Note right of API Consumer[Bob]: Generate Public/Private Key Pair
  API Consumer[Bob]->>API: Send Public Key
  API->>Authorization Server: Store [Bob, [Public Key]]
  Authorization Server-->>API: Success
  API-->>API Consumer[Bob]: Success 
  end
  rect rgba(0, 0, 0, 0.05)
  note over API Consumer[Bob],Authorization Server: Authorized API Request  
  API Consumer[Bob]-->>API: SECURE SESSION [Unknown] with [API]
  API-->>API Consumer[Bob]: 
  API Consumer[Bob]->>Authorization Server: Hello, I am Bob
  Note left of Authorization Server: Generate Random Number [NONCE]
  Authorization Server-->>API Consumer[Bob]: Hello Bob, please sign [NONCE]
  Note right of API Consumer[Bob]: Sign [NONCE] with Private Key as [PROOF]
  API Consumer[Bob]->>Authorization Server: [PROOF] 
  Note left of Authorization Server: Validate [NONCE] signature using [Public Key[Bob]]
  Authorization Server-->>API Consumer[Bob]: Success. [Session Token]
  API Consumer[Bob]-->>API: AUTHORIZED SESSION [Bob] with [API] via [Session Token]
  API-->>API Consumer[Bob]: 
  end
```

The cost and complexity of building and maintaining a Public/Private Key Authentication mechanism, without exposing the Private Key, opening up replay attacks, or making a mistake in implementation somewhere can be high.

If you're selling an API to multiple consumers, it's unlikely that it will be as trivial as the following `curl` to invoke the API that you want; as the integrating system will need to understand the protocol you choose. There are also complexities regarding the certificate lifecycle, and the need to either manage certificate rotation, or pin (hardcode) each certificate into the system.

```sh
# mTLS via curl: Should you be looking to grant unscoped access to a specific API Route to trusted consumers,
#   this can usually be configured by some API Gateway products in infrastructure configuration. 
curl --cacert ca.crt \
     --key client.key \
     --cert client.crt \
     https://api.example.com/v1/endpoint
```

* **Time to API Call**: Slow.
* **Ease of API Consumer Integration**: Difficult
* **Ease of API Producer Implementation**: Difficult
* **Other Considerations**:
  * Severity of the public key being leaked is functionally zero -- no intermediary system holds enough data to make/replay requests except the original sender.

### Signed Tokens as API Keys

A Signed Token is secret; in the same way that a Shared Secret is secret. 

However, due to standardization of technologies, it is starting to become commonplace to use long-lived Signed Tokens, in the form of JWTs (JSON Web Tokens), as API Keys. This enables the following pattern: 

```sh
curl --header "Authorization Bearer ${API_KEY}"\
     http://api.example.com/v1/endpoint 
```

```mermaid 
sequenceDiagram
  participant API Consumer[Bob]
  participant API
  participant Authorization Server
  rect rgba(0, 0, 0, 0.05)
  note over API Consumer[Bob],Authorization Server: Configure Key
  API Consumer[Bob]-->>API: AUTHORIZED SESSION [Bob] with [API]
  API-->>API Consumer[Bob]: 
  API Consumer[Bob]->>Authorization Server: Request New Secret Key
      Note left of Authorization Server: Make JWT[Secret Key] with Signing Key [K] via: <br>SIGN({"sub": Bob, "exp": 3 months}, K).
  Authorization Server-->>API Consumer[Bob]: [Secret Key]
  end
  rect rgba(0, 0, 0, 0.05)
  note over API Consumer[Bob],Authorization Server: Authorized API Request
  API Consumer[Bob]-->>API: SECURE SESSION [Unknown] with [API]
  API-->>API Consumer[Bob]: 
  API Consumer[Bob]->>API: [Req] with header "Authorization Bearer \${JWT["Bob"]}
  API->>Authorization Server: Check JWT["Bob"]
    Note left of Authorization Server: Validate JWT[Secret Key] with Signing Key [K]
  Authorization Server-->>API: Success
      Note left of API: Process Request under context Bob
  API-->>API Consumer[Bob]: [Resp]
  end
```

* **Time to API Call**: Fast.
* **Ease of API Consumer Integration**: Simple
* **Ease of API Producer Implementation**: Simple
* **Other Considerations**:
  * Difficult to revoke tokens: either a whitelist/blacklist is used (in which case, little advantage exists over shared secrets), or the signing key must be rotated.
  * Easy to add complexity through custom claims into the token, which can lead to complexity migrating tokens into a new form.
  * Easy to block requests from reaching application servers through an API Gateway and Asymmetric Keys (split into Private JWTs and Public JWKs).

### Our Approach: Signed Tokens as API Keys, but 1-Signing-Key-Per-API-key

There are 3 problems with the Signed Tokens as API Keys pattern:

1. Revoking a key is hard: it is very difficult for users to revoke their own keys on an internal compromise.
2. Any compromise of the Signing Key is a compromise of all keys.
3. API Gateways can't usually hook into a whitelist/blacklist of tokens.

To tackle these, we can do three things:

1. Use Asymmetrically Signed JWTs, storing and exposing a set of Public Keys via a JWKS (JSON Web Key Set) URI.
2. Sign each Token with a different Signing Key; burn the Signing Key immediately afterwards.
3. Ensure that API Gateways only retain a short-lived cache of JWKS (the Public Key to each Signing Key).

```mermaid
sequenceDiagram
  participant API Consumer[Bob]
  participant Authorization Server
  participant API
  rect rgba(0, 0, 0, 0.05)
  note over API Consumer[Bob],API: Configure Key
  API Consumer[Bob]-->>Authorization Server: AUTHORIZED SESSION [Bob] with [Authorization Server]
  Authorization Server-->>API Consumer[Bob]: 
  API Consumer[Bob]->>Authorization Server: Request New Secret Key [K]
      Note left of Authorization Server: Create a new Signing Key Pair [K]<br> (e.g. EdDSA Certificate with High Entropy)<br>Sign [Bob] with [K["PrivateKey"]] as [Secret Key]<br>Store [Bob, K["PublicKey"]]<br>Burn K["Private Key"]       
  Authorization Server-->>API Consumer[Bob]: [Secret Key]
  end
  rect rgba(0, 0, 0, 0.05)
  note over API Consumer[Bob],API: Authorized API Request
  API Consumer[Bob]-->>Authorization Server: SECURE SESSION [Unknown] with [Authorization Server]
  Authorization Server-->>API Consumer[Bob]: 
  API Consumer[Bob]->>Authorization Server: [Req] with header "Authorization Bearer \${JWT["Bob"]}"
    Note left of Authorization Server: Decode["Secret Key"] into JWT<br>Lookup Bob["Public Key"] via JWT.KID<br>Validate ["Secret Key"] matches Public Key["KID"]
  Authorization Server->>API: [Req], [Bob]
      Note left of API: Process Request under context Bob
  API-->>Authorization Server: [Resp]
  Authorization Server-->>API Consumer[Bob]: [Resp]
  end
```

This gives us a flow that's very similar to a shared secret, but with the advantage that:

* Revoking a key is easy: the Public Key just needs to be removed from the JWKS and caches invalidated.
* There is no Signing Key to compromise ; all Authorization Server state can be public.

With the tradeoff:

* Any API Gateway that validates a JWT must be able to regularly fetch JWKS (and cache them) from the Authorization Server.
* After creating/revoking a key, there will be a short time delay (we generally configure this to be 15 seconds) whilst the key propagates into all API Gateway JWK caches.
* More compute is required when constructing the Initial API Key; due to the generation of a public/private key pair.
* You need to give the API Gateway application cluster slightly more memory to keep all Public Keys (1 per API key) in memory.

import portal_url_1 from './assets/auth-clip.mp4'

  <video controls={false} loop={true} autoPlay={true} muted={true} width="100%" alt="Auto-clip">
    <source src={portal_url_1} type="video/mp4" />
  </video>

#### In Practice: Envoy / Google Endpoints / EspV2

Envoy is a proxy server that is used to route traffic to a backend service. We ran extensive tests with ESPV2 (Envoy Service Proxy V2) and Google Endpoints, and found/validated the following performance characteristics as the number of keys increased. 

```yaml
# Espv2 is configured with an OpenAPI specification to use an external JWKs URI to validate incoming API Keys
securityDefinitions:
  speakeasy_api_key:
    authorizationUrl: ""
    flow: "implicit"
    type: "oauth2"
    x-google-issuer: "https://app.speakeasy.com/v1/auth/oauth/{your-speakeasy-workspace-id}"
    x-google-jwks_uri: "https://app.speakeasy.com/v1/auth/oauth/{your-speakeasy-workspace-id}/.well-known/jwks.json"
    x-google-audiences: "acme-company"
```

We ran benchmarks of up to 1138688 API keys, structured as public/private 2048-bit RSA keypairs with a single RS256 Signed JWT-per-JWK, using a 8GiB Cloud Run managed Espv2 instance. At this key size and at ~100 requests/second, we observed a peak of ~37% utilization of memory, with ~7% utilized at no API keys. This implies an 8GiB EspV2 instance should scale to ~3M API Keys at this request rate.

This also implies that this mechanism will scale with Envoy RAM with a minor deviation in maximum latency, to the degree of ~3.5ms additional maximum request latency every 8192 API keys. The average additional latency introduced by large numbers of API keys is affected to a much lesser degree, ~0.2ms every 8192 API Keys.

![envoy-scaling.png](./assets/envoy-scaling.png)

Given most API Applications have less than 3M API Keys active at any given time, this approach, in our belief, combines the best of both worlds: Public/Private Key Crypto, with the ease of an API Key.


 This is the content for the doc blog/api-design/index.mdx 

 ---
title: "Designing your API: Find the RESTful sweet spot"
description: "Learn to make practical API design decisions that will help you create a better developer experience."
keywords:
  [
    api,
    api design,
    sdk,
    developer experience,
    devex,
    dx,
    openapi,
    rest,
    rest api,
    restful,
    restful api,
    swagger,
  ]
image: "/media/guide-api-design.png"
date: 2025-01-01
authors:
  - name: Sagar Batchu
  - image_url: "/media/author-headshots/sagar.jpeg"
tags:
  - API Advice

featured_image: "/media/guide-api-design.png"
---

import { Callout } from "~/components";

<Callout title="API Design Guide" variant="success">
  <p>If you're looking for a more comprehensive guide to API design, you can read our <a href="/api-design">REST API Design Guide</a>.</p>
</Callout>

What we call RESTful APIs today often refer to JSON over HTTP, which is an offspring of the RESTful APIs Roy Fielding defined in his [dissertation](https://ics.uci.edu/~fielding/pubs/dissertation/fielding_dissertation_2up.pdf) in 2000. Back then, JSON was still just an idea, and the web was still in its infancy. The constraints Fielding defined in his dissertation were a reaction to the state of the web at the time, but they remain relevant to the web today. Rather than co-opting the term RESTful, JSON-over-HTTP APIs could have benefited from a new term that acknowledges their differences from Fielding's REST. 

Alas, the term RESTful stuck, and here we are. This article will explore what it means to be RESTful in 2025, why it matters, and how to find the sweet spot between adhering to RESTful principles and following established practices.

First, let's clarify the difference between RESTful APIs and REST-like APIs.

# Designing your API: Find the RESTful sweet spot

Fielding's original REST model defines six architectural constraints, liberally summarized as follows:

1. **Client-server architecture**: This separation allows the client and server to evolve independently as long as the interface doesn't change.
2. **Statelessness**: Each request from the client contains all the information the server needs to fulfill that request, easing server workload and improving scalability.
3. **Cacheability**: Responses are explicitly labeled as cacheable or non-cacheable, which helps reduce client-server interactions and improves performance.
4. **Uniform interface**: This constraint is broken into four subconstraints.
   - **Resource identification**: Resources are identified in requests, typically using URIs.
   - **Resource manipulation through representations**: Resources are manipulated through representations, such as JSON or XML.
   - **Self-descriptive messages**: Messages include all the information needed to understand them.
   - **Hypermedia as the engine of application state (HATEOAS)**: Responses include links to related resources, allowing clients to navigate the API dynamically.
5. **Layered system**: A client should be unable to tell whether it is connected directly to the end server or to an intermediary along the way.
6. **Code on demand (optional)**: Servers can extend client functionality by transferring executable code, like Java applets or client-side scripts.

Adherence to these constraints distinguishes a truly RESTful API from one that is REST-like.

## How most REST-like APIs adhere to REST constraints

If you're building a modern API, you're likely adhering to some of the REST model's constraints, even if you're not following them all to the letter. The key is to understand the principles behind REST and apply them in a way that makes sense for your use case.

### ✅ Client-server architecture: Followed by most REST-like APIs

In the context of APIs, this means that the client and server communicate over a network, with the server providing resources and the client consuming them. This separation is central to RESTful design and may be why the term "RESTful" was adopted for APIs that follow this pattern.

```mermaid
sequenceDiagram
    participant Client
    participant Server
    Client->>Server: Request
    Server->>Client: Response
```

### ✅ Statelessness: Followed by most REST-like APIs

Statelessness means that each request from the client to the server must contain all the information needed to fulfill that request. This constraint simplifies server logic and improves scalability by allowing servers to handle requests independently.

Most APIs follow this constraint by requiring clients to include all necessary information in each request.

```mermaid
sequenceDiagram
    participant Client
    participant Server
    Client->>Server: GET /orders/123
    Server->>Client: 200 OK { "order": { "id": 123, "status": "shipped" } }
```

### ✅ Cacheability: Followed by most REST-like APIs

Cacheability allows responses to be explicitly labeled as cacheable or non-cacheable, reducing the need for repeated requests to the server. By specifying cacheability, APIs can improve performance and reduce server load.

Most APIs follow this constraint by including cache-control headers in their responses.

```mermaid
sequenceDiagram
    participant Client
    participant Cache
    participant Server
    Client->>Cache: GET /orders/123
    Cache->>Server: GET /orders/123
    Server->>Cache: 200 OK { "order": { "id": 123, "status": "shipped" } }
    Cache->>Client: 200 OK { "order": { "id": 123, "status": "shipped" } }
    Client->>Cache: GET /orders/123
    Cache->>Client: 200 OK { "order": { "id": 123, "status": "shipped" } }
```

The first request retrieves the order from the server and caches it. The subsequent request is served from the cache, reducing the load on the server.

### ⚠️ Uniform interface: Partially followed by most REST-like APIs

The uniform interface constraint is seen by many as the heart of REST. It defines a standard way to interact with resources, making APIs more discoverable and easier to use. This constraint is often broken down into four sub-constraints:

✅ **Resource identification**: Resources are identified in requests, typically using URIs. Followed by most APIs.

✅ **Resource manipulation through representations**: Resources are manipulated through representations, such as JSON or XML. Followed by most APIs.

⚠️ **Self-descriptive messages**: Messages include all the information needed to understand them. Through the use of media types, APIs can achieve this sub-constraint. Partially followed by most APIs.

❌ **Hypermedia as the engine of application state (HATEOAS)**: Responses include links to related resources, allowing clients to navigate the API dynamically. Rarely followed by APIs.

The last two sub-constraints of uniform interfaces are often the most challenging to implement and are frequently omitted in practice.

HATEOAS, in particular, is a powerful concept that applies extremely well to web APIs that serve human users. For example, HTML returned by a web server contains links that users can click to navigate the web.

Take this HTML response as an example:

```bash
curl --header "Accept: text/html" https://api.example.com/orders/123
```

```html
<!doctype html>
<html>
  <head>
    <title>Order 123</title>
  </head>
  <body>
    <h1>Order 123</h1>
    <p>Status: Shipped</p>
    <a href="/orders/123">View Order</a>
    <a href="/customers/456">View Customer</a>
  </body>
</html>
```

In this case, the links are clickable, allowing users to navigate the API by following them. This is the essence of HATEOAS.

Contrast this with a JSON response:

```bash
curl --header "Accept: application/json" https://api.example.com/orders/123
```

```json
{
  "id": 123,
  "status": "shipped",
  "links": [
    { "rel": "self", "href": "/orders/123" },
    { "rel": "customer", "href": "/customers/456" }
  ]
}
```

In this example, the response includes links to the order itself and the customer who placed the order. By following these links, a client can navigate the API without prior knowledge of its structure. In practice, an SDK or client library would need to be aware of these links to provide a similar experience. From a developer experience perspective, this can be challenging to implement and maintain.

Instead of implementing HATEOAS, many APIs rely on documentation to inform developers how to interact with the API. While this approach is more common, it lacks the dynamic nature of HATEOAS.

### ✅ Layered system: Followed by most REST-like APIs

The layered system constraint allows for intermediaries between the client and server, such as proxies or gateways. This separation enhances scalability and security by isolating components and simplifying communication.

Most APIs follow this constraint by permitting intermediaries between the client and server.

```mermaid
sequenceDiagram
    participant Client
    participant Proxy
    participant Server
    Client->>Proxy: Request
    Proxy->>Server: Request
    Server->>Proxy: Response
    Proxy->>Client: Response
```

Since API requests are stateless and contain all the information needed to fulfill them, intermediaries can forward requests without needing to maintain session state. This is especially useful for load balancing and security purposes.

### ⚠️ Resource-oriented architecture: Followed by some REST-like APIs

Each of the constraints above contributes to a resource-oriented architecture, where resources are identified by URIs and manipulated through representations. This architecture makes APIs more predictable and easier to use by following standard resource interaction patterns.

Most APIs follow this constraint by organizing their resources into collections and items, with standard methods for interacting with them.

This pattern is often reflected in an API's URL structure, where resources are presented as nouns, and instead of using verbs in the URL, API actions are represented by HTTP methods. For example:

- `GET /orders`: Retrieve a list of orders.
- `POST /orders`: Create a new order.
- `GET /orders/123`: Retrieve order 123.
- `PUT /orders/123`: Update order 123.
- `DELETE /orders/123`: Delete order 123.

Many API design guidelines recommend using resource-oriented URLs to make APIs more intuitive and easier to use. We'll explore this in more detail later in the article.

### ❌ Code on demand: Rarely used in the context of APIs

We'll skip this constraint for now, as it's optional and rarely implemented in practice outside hypermedia-driven APIs.

## Why adherence matters

Is this a cargo cult, or do these constraints actually matter? They do, and here's why:

### The principle of least astonishment

There is a big difference between delighting users and surprising them. The principle of least astonishment states that the behavior of a system should be predictable and consistent.

When your API behaves in a predictable way, like sticking to the standard ways of using HTTP methods and formats, you're making it easy for developers to use. They shouldn’t have to spend hours figuring out quirky behaviors or unexpected responses - that just leads to headaches and wasted time.

### Scalability

Scalability is essential when designing APIs that need to handle varying loads and growth over time. Adhering to REST principles inherently supports scalability in several ways:

- **Statelessness**: Without the need to maintain session state, servers can handle requests independently, making it easier to scale horizontally.
- **Cacheability**: REST APIs explicitly label responses as cacheable. This reduces the server load, as cached responses can be reused from previous requests.
- **Layered system**: REST architecture allows the deployment of intermediary servers, such as load balancers and cache servers, which isolate client requests from direct backend processing.

### Maintainability

The constraints of the REST model naturally lead to a design that is easier to update and manage:

- **Resource-oriented design**: By focusing on resources rather than actions, APIs become more modular and logically structured.
- **Independent client and server evolution**: The client-server separation supported by REST allows both sides to evolve independently.

## Quantifying REST adherence

[The Richardson Maturity Model](https://martinfowler.com/articles/richardsonMaturityModel.html) provides a framework for evaluating an API's compliance with RESTful principles. This model outlines four levels of RESTfulness, each building on the previous one, allowing you to assess and improve your API's design objectively:

### Level 0: The swamp of POX (Plain Old XML)

At this base level, APIs typically rely on a single URI and use HTTP merely as a transport protocol. There is little to no differentiation in the use of HTTP methods, and operations tend to be defined solely by the payload. This resembles the remote procedure call over HTTP (RPC over HTTP) protocol, where the rich set of HTTP features, like methods and status codes, are underutilized.

An example of a Level 0 API might be:

```bash
curl -X POST https://api.example.com/api.aspx?method=createOrder -d "..."
```

### Level 1: Resources

The first step towards RESTfulness is exposing resources via distinct URIs. At Level 1, APIs start organizing data into resources, often with a collection-item structure, making the API more resource-oriented. Each resource, such as `/orders` or `/customers`, typically represents a collection of items, with individual resources accessible by identifiers like `/orders/{orderId}`.

An example of a Level 1 API might be:

```bash
# Retrieve a list of orders
curl -X POST https://api.example.com/orders

# Retrieve order 123
curl -X POST https://api.example.com/orders/123
```

Note how the API is starting to use URIs to represent resources, but the use of POST for retrieval is not ideal.

### Level 2: HTTP verbs

At this level, RESTful APIs progress by using HTTP methods (like GET, POST, PUT, and DELETE) to perform operations on resources. Level 2 APIs respect the semantics of these verbs, leveraging the full power of HTTP to perform operations that are predictable and standardized. For example, GET retrieves data, POST creates new resources, PUT updates existing resources, and DELETE removes them.

An example of a Level 2 API might be:

```bash
# Retrieve a list of orders
curl -X GET https://api.example.com/orders

# Retrieve order 123
curl -X GET https://api.example.com/orders/123

# Create a new order
curl -X POST https://api.example.com/orders -d "..."

# Update order 123
curl -X PUT https://api.example.com/orders/123 -d "..."

# Delete order 123
curl -X DELETE https://api.example.com/orders/123
```

### Level 3: Hypermedia controls (HATEOAS)

HATEOAS distinguishes Level 3 APIs from the rest. At Level 3, responses include hypermedia links that offer clients dynamic navigation paths within the API. This allows for discoverability directly embedded in API responses, fostering a dynamic interaction model in which clients can follow links to escalate through states or access related resources.

## Evaluating your API's RESTfulness

To evaluate the RESTfulness of your API, consider the following questions:

1. Level 1: Are resources uniquely identified by URIs?

   For example, `/orders/123` uniquely identifies an order resource.

2. Level 2: Are all URLs resource-oriented, and do they use standard HTTP methods?

   For example, URLs should use GET to retrieve resources, POST to create new resources, PUT to update existing resources, and DELETE to remove resources.

   None of your endpoint URLs should contain verbs or actions. For example, neither `/cancelOrder` nor `/orders/123/cancel` is resource-oriented,.

3. Level 3: Do responses include hypermedia links for dynamic navigation?

   For example, a response might include links to related resources, allowing clients to navigate the API without prior knowledge of its structure.

## The RESTful sweet spot

We believe the RESTful sweet spot lies somewhere between Level 1 and Level 2 of the Richardson Maturity Model. This is where most APIs can find a balance between adhering to RESTful principles and practicality. 

In case it wasn't crystal clear from the previous sections, we think HATEOAS isn't practical or relevant for most APIs. It was a powerful concept when the web was young, and APIs were meant to be consumed by humans.

Here's what we recommend:

1. **Embrace resource-oriented design**: Think in terms of resources rather than actions. Identify the key resources in your API domain and structure your endpoints around them.

   This ensures your API is predictable and intuitive, making it easier for developers to understand and use.

   ✅ Good: Use resource-oriented URLs like `/orders` and `/customers`.

   ```yaml openapi.yaml
   paths:
     /orders: # Resource-oriented
       get:
         summary: Retrieve a list of orders
       post:
         summary: Create a new order
     /orders/{orderId}: # Resource-oriented
       get:
         summary: Retrieve order details
       patch:
         summary: Update an order
       put:
         summary: Replace an order
       delete:
         summary: Delete an order
   ```

   ❌ Bad: Don't use action-based URLs like `/cancelOrder`.

   ```yaml openapi.yaml
   paths:
     /cancelOrder: # Not resource-oriented
       post:
         summary: Cancel an order
   ```

   ✅ Compromise: Use sub-resources like `/orders/{orderId}/cancellations`.

   If you must include actions in your URLs, consider using sub-resources to represent them as resources in their own right.

   ```yaml openapi.yaml
   paths:
     /orders/{orderId}/cancellations: # Resource-oriented
       post:
         summary: Create a cancellation for an order
   ```

   ✅ Less ideal compromise: Use top-level actions like `/orders/{orderId}/cancel`.

   ```yaml openapi.yaml
   paths:
     /orders/{orderId}/cancel: # Resource-oriented with action
       post:
         summary: Cancel an order
   ```

2. **Use standard HTTP methods wisely**: Choose the appropriate method for each operation - GET for retrieval, POST for creation, PATCH for partial updates, PUT for complete updates or replacement, and DELETE for removal. Ensure your API follows the semantics of these methods.

   This allows developers to predict how operations will behave based on the HTTP method. It also implies idempotency, safety, and cacheability where applicable.

   An operation is considered **safe** if it doesn't modify resources. It is **idempotent** if the result of performing it multiple times is the same as performing it once. It is **cacheable** if the response can be stored and reused.

   ```yaml openapi.yaml
   paths:
     /orders:
       get: # Safe, idempotent, cacheable
         summary: Retrieve a list of orders
       post: # Unsafe, potentially idempotent, not cacheable
         summary: Create a new order
     /orders/{orderId}:
       get: # safe, idempotent, cacheable
         summary: Retrieve order details
       patch: # unsafe, potentially idempotent, not cacheable
         summary: Update an order
       put: # unsafe, idempotent, not cacheable
         summary: Replace an order
       delete: # unsafe, idempotent, not cacheable
         summary: Delete an order
   ```

3. **Document thoroughly with OpenAPI**: Instead of relying on HATEOAS for dynamic navigation, use OpenAPI to provide comprehensive documentation for your API. OpenAPI allows you to define your API's structure, endpoints, methods, parameters, and responses in a machine-readable format. This ensures clarity and type safety for developers using your API.

### How targeting SDK generation enables better API design

While you're designing your API, consider how it will be consumed by developers. If you're providing an SDK or client library, you can optimize your API design to make SDK generation easier and more effective, and you may find the optimized design also leads to a more RESTful API.

We often see APIs with action-based URLs like `/orders/{orderId}/cancel` or `/orders/{orderId}/refund`. While partially resource-oriented, these URLs include actions as part of the URL.

Firstly, these URLs are not as maintainable as resource-oriented URLs. If you decide to allow multiple cancellations for an order - for example, when an order is restored after being canceled and then canceled again - you may wish to represent cancellations as resources in their own right. This would lead to URLs like `/orders/{orderId}/cancellations/{cancellationId}`. Alternatively, you may wish to allow partial refunds, leading to URLs like `/orders/{orderId}/refunds/{refundId}`.

Secondly, these URLs are not as predictable as resource-oriented URLs. Developers may not know which actions are available for a given resource, leading to a reliance on documentation or trial and error.

From a design perspective, this could look like the following:

```yaml openapi.yaml
paths:
  /orders/{orderId}/cancellations:
    post:
      summary: Set the order status to cancelled
  /orders/{orderId}/refunds:
    post:
      summary: Create a refund for the order
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                amount:
                  type: number
                  minimum: 0
                  maximum: 100
```

Then, in a future version of your API, you could introduce cancellations and refunds as resources in their own right:

```yaml openapi.yaml
paths:
  /orders/{orderId}/cancellations:
    get:
      summary: Retrieve a list of cancellations for the order
    post:
      summary: Create a new cancellation for the order
  /orders/{orderId}/cancellations/{cancellationId}:
    get:
      summary: Retrieve a specific cancellation for the order
  /orders/{orderId}/refunds:
    get:
      summary: Retrieve a list of refunds for the order
    post:
      summary: Create a new refund for the order
  /orders/{orderId}/refunds/{refundId}:
    get:
      summary: Retrieve a specific refund for the order
```

This approach allows you to evolve your API over time without breaking existing clients.

## Going beyond RESTful principles

While adhering to RESTful principles is essential, it's also important to consider the practicalities of API design. Here are some detailed topics we'll explore in future articles:

1. [**Pagination**](/api-design/pagination): How to handle large collections of resources. Should you use offset-based pagination, cursor-based pagination, or something else?
2. [**Filtering and searching**](/api-design/filtering-responses): How to allow clients to filter and search resources efficiently.
3. [**Error handling**](/api-design/errors): How to communicate errors effectively and consistently.
4. **Versioning**: How to version your API while maintaining backward compatibility.
5. **Security**: How to secure your API using authentication and authorization mechanisms.
6. **Rate limiting**: How to protect your API from abuse by limiting the number of requests clients can make.
7. **Webhooks**: How to implement webhooks for real-time notifications.

Be sure to check back for more insights on API design.


 This is the content for the doc blog/api-experts-akshat-agrawal.mdx 

 ---
title: "API Experts - APIs That Build APIs"
description: "Akshat Agrawal, API product manager, discusses how to PM an API and how to reduce the 'time to wow' of your API."
keywords: [api, openapi, swagger, akshat agrawal, skyflow, security, compliance, privacy tech, developer experience, devex, dx, sdk generation, sdk]
image: "/media/api-experts-akshat-agrawal.png"
date: 2022-09-15
authors:
  - name: Nolan Sullivan
  - image_url: 'https://uploads-ssl.webflow.com/62ccd7b208cab0723d026273/62cdf9e45dcbb4d20be59f5f_head.jpeg'
tags:
  - API Advice
featured_image: "/media/api-experts-akshat-agrawal.png"
---

### TL;DR

- Spend time thinking about your API taxonomy. Talk to users to make sure the language you use matches with how they understand your product.
- Sometimes you need to build endpoints that deviate from the long-term vision of your API to deliver value to clients. That’s okay, just always have a plan to bring things back into alignment down the road.
- Even if you’re building an API product, some tasks are more easily solved with a UI, be strategic about what you ask user’s to do via API and know where a UI is important.
- For a good DevEx, relentlessly focus on minimizing your time to ‘wow’.
- If you’re PM’ing an API product, you need to build with your API to uncover the points of friction.

## Introduction

_Akshat Agrawal, is an MBA candidate at Harvard Business School. Prior to pursuing an MBA, Akshat worked as a senior product manager at_ [_Skyflow_](https://www.skyflow.com/)_. Skyflow is a data vault accessible via an API interface. Akshat was an early product hire at the company and was tasked with building and refining the MVP of Skyflow’s API interface._ _Prior to joining Skyflow, Akshat worked as a PM for Google Stadia, Google’s next generation cloud gaming platform._

**_Can you let people know what Skyflow does?_**

Of Course. So, Skyflow is a Data Vault delivered as an API. There's a lot packed in that sentence, so let me break it down a little bit. First of all, what is a Data Vault? This is an emerging category of data infrastructure. Instead of sticking your sensitive and non-sensitive  data into the same database, which is traditionally how companies have been operating, the emerging paradigm is to create a separate construct called a Data Vault, specifically designed for you to store and compute sensitive data. Sensitive data includes things like PII and KYC (know your customer data), basically anything you wouldn't want breached.

Skyflow provides data vaults that have all the bells and whistles you’d want: encryption, key rotation, data governance, data redaction, field masking, all of that is baked in. It is a totally managed service that we deploy within our cloud for most of our customers. Then the way that developers interact with the data stored in the vault is through an API. So at Skyflow the API is a big part of the product, in some sense it is the product. The API is used for the entire lifecycle of interaction with your data vault, from creating the data vault, to specifying the schema to actually reading, writing, deleting data from the vault, as well as computing on that data, sharing that data, configuring governance rules. It's all meant to be done via API.

**_What does API development look like at Skyflow? Who is responsible for releasing APIs publicly?_**

Yeah, that's a good question. Well as you might expect, as a startup the API development process isn't perfect. When I was at Skyflow, we were definitely still figuring it out. But in general, there are a couple of key steps. First is designing the taxonomy (structure) of the API. This is a bit of a nuance to Skyflow’s business, but because we are a data platform, we actually don't have a fixed schema. It's up to the customer to define what their schema looks like. You know, how they want to arrange their tables and columns. And that makes it very different from your typical REST-based API. We had to design an API that was generic enough to be able to serve whatever data schema our customers designed. And so this really exacerbates the taxonomy problem. We had to make sure that we were really thoughtful with the terms we used to describe our resources. We had to make sure that the structure of the API made intuitive sense and that whenever we add a new endpoint it’s a natural extension of the existing taxonomy.  

So step 1 was getting the right structure for the endpoints and parameters. Then step 2 would be the actual development, including testing and telemetry for the API. Then step 3 would be rollout. Depending on the nature of the API, we might offer it as a beta; and test with certain customers. We’d assess whether there were any back-compat issues that needed to be accounted for. Then last step, we would prepare the documentation. That is super important.  We were rigurious with making sure that the documentation is up to date. There’s nothing worse than stale documentation. And that, roughly, is the full scope of the API development process.

**_How does Skyflow ensure that publicly released APIs are consistent?_**

It starts with establishing a literal rulebook for API development, and getting the whole team to buy into using it. We created guidelines which contained rules about how APIs are named, how parameters are named, standard semantics, expected functionality, all that kind of stuff. Ironing out those rules enables developers building new endpoints to get it mostly right on the first go. That’s really important for development velocity.

Building the rulebook isn’t a one-time activity, our API best practices document was ever-evolving. As time has gone on, we’ve gotten to something pretty prescriptive. So now, if you want to add a new endpoint, you should be able to get it pretty close to perfect just by adhering to the things in the document.  And that rulebook is actively maintained by our API excellence committee.

**_If I were to ask a developer building public APIs at Skyflow what their biggest challenge was, what do you think they would say?_**

Well like I said. I think how you describe your API resources is important.  And one specific challenge for Skyflow was maintaining intelligibility across clients and user personas. As an example, some users might call it consistency, while other user’s called it quorum, and still other people might call it something else. It can be really challenging for them to understand the product when it’s presented in unfamiliar terms. That challenge is not technical. It's more organizational and logistical, and it’s especially hard when you're in startup mode. You've got tight deadlines and customers asking for endpoints that need to be shipped quickly. So really balancing the cleanliness and excellence of the API against time constraints and organizational constraints is really hard. And it’s all compounded by the fact that your APIs can't really change easily once you launch them.

**_What about the challenges facing the developers who consume Skyflow’s APIs?_**

We are a security and privacy product and there’s some problems that can create for the developer experience. As an example, a lot of APIs can afford giving users a static key, but for Skyflow that's not the case. We have to take extra steps to make sure interactions with the API are safe.

We use a jwt token, which you have to dynamically generate and that jwt token contains all the scopes and permissions that define what data you can interact with.The whole process of generating that token, and getting authenticated to the API is non trivial.  You have to get a credentials file with a private key, then run a script on it to generate a token, then you include that token in your API requests. Asking users to perform all those steps creates friction. We saw we had a lot of drop off along the usage path, especially for developers who were just trying the API for the first time.

To address that issue, we built a trial environment into our dev portal.  We auto-generate a personal access token that users can use to experiment with the API. But in production, we want to stick with that really robust, secure mechanism.

## API architecture Decisions

**_Skyflow’s public APIs are RESTful, Can you talk about why you decided to offer Restful APIs over say, GraphQL?_**

I think for our use case, specifically, GraphQL is actually a good option, we are trying to appeal to developers at other tech companies. But ultimately it’s still a bit ahead of the market. I think GraphQL is really cool, but you know, compared to REST, it’s not as easy to understand or as commonly known by developers. So REST still felt like a natural starting point. GraphQL could always be added in the future.

**_If someone was designing their API architecture today, what advice would you give them?_**

Yeah, totally. I think API security is something everyone should spend more time on.  APIs are the attack vector for a lot of the data breaches which are happening today. It's really hard to secure the entire surface of your API because unlike traditional services, which are running in a confined environment, APIs try to be universally accessible. I wish I had some more concrete advice, but if you’re a startup, spend time discussing it as a team.

## PM’ing APIs

**_What’s it like being a product manager working on an API product?_**

I do think, relative to other PM roles, you need to be more technical. That being said, it shouldn't be a barrier for most people. If you’re at the point where you can make an API request, then you can probably develop and grow into the role. So I definitely would tell people to not let the fear of not being an engineer discourage you.

Like any good PM, you should be a consumer of your own product. And for an API product, that means you need to use your API. That's the real way that you figure out where the friction points are. And when I say use, I mean really build with your product.  Of course, you can get out Postman, get an API token and make an API call. But when you actually go to build something, you start to discover all these little thorns. I built with our API from day one.I had little side projects. In the case of Skyflow, that helped me identify that the definition of the schema was a pain. It was quite difficult to do via API. So we actually built a web UI that could do the schema definition. So, yeah, the TLDR is you have to build stuff with your API.

And of course not specific to an API per say, but at the end of the day, making your early customers and partners successful is the most important thing, so let that guide you. Oftentimes, we would have to make hard trade-offs, keeping the shape of the API completely standard vs. giving a client specific functionality they needed. If you’re PM’ing an API, your challenge will lie in eventually bringing everything back into a state of alignment. You're managing an ever expanding body of endpoints and functionality and so you really need to be thinking about the long-term. How will we hurtle the cattle and bring the endpoints back into a clean shape as a cohesive product?

**_What were the KPIs that you tracked against for your product area?_**

I focused a lot on developer experience. The most important metric that I tracked was our user's average time to ‘wow’. For a lot of API products, ‘wow’ will be the first successful API request. The time from when you click ‘get started’ to when you make that first successful API request, we put a lot of effort into trimming that time down. And it can’t just be the sum of every step of the process: getting your credentials to the account, setting up your data vault, authenticating a token, creating a well formed API request, sending it debugging. You also need to account for the time where your user is stuck or inactive. They don't see anything in the documentation, so they give up and come back to it in a day. Because the documentation is part of your product. Reaching out to support and waiting for support to get back to me, that counts against time to wow.

Through a combination of working on authentication, onboarding, documentation, API design, we were actually able to get that down to minutes. For developers, short time to ‘wow’ inspires confidence. And it has a sort of dopamine effect. We feel good when we get that 200 after sending the API request. I don't know about you, but anytime I start to use a new API. I have this sense of foreboding. I’m just waiting to run into an error or stale docs or something like that. And when it just works the first time, it's an amazing feeling.

## _DevEx for APIs_

**_What do you think constitutes a good developer experience?_**

There's a lot of things that go into creating a good developer experience, but I would say that at the end of the day it boils down to productivity. A good developer experience is when developers can be productive. This is not just for APIs, this is for all dev tools. It's all about productivity. A lot of studies show with the proliferation of SaaS tools, Cloud Tools, microservices, new architectures, developer productivity has kind of hit rock bottom. There’s just so much to account for. The amount of hours that developers spend doing random stuff besides just writing and testing good code is way off the mark. The proliferation of APIs should have big benefits in the long term, but currently developers are losing a lot of time understanding partner APIs, looking for documentation, debugging, dealing with authentication.

So when it comes to APIs, a good developer experience is one that makes developers maximally productive when developing atop your API. Unlike most products, with an API you want your users spending as little time as possible with your API.  It's not their job to spend a lot of time on your API. They're just using your API to get something done. So you really want them kind of in and out the door, so to speak.

## Closing

**_A closing question we like to ask everyone: any new technologies or tools that you’re particularly excited by? Doesn’t have to be API related._**

That's a good question. And I think a big part of being at grad school is my desire to explore that fully. I've been really deeply in the world of security, privacy, data and fraud for the last few years. I'm excited to see what else is out there. One thing that's really interesting to me right now is climate tech. I think there's just so much scope for software to make an impact in mitigating climate change and so I'm really excited to explore that space more.


 This is the content for the doc blog/api-experts-clarence-chio.mdx 

 ---
title: "API Experts - APIs to Fight Fraud"
description: "Clarence Chio, the CTO of Unit21, discusses how the company expanded their product from a pure web app to include a very popular API interface."
keywords: [api, openapi, swagger, clarence chio, unit21, fintech, banking, fraud, developer experience, devex, dx, sdk generation, sdk]
image: "/media/api-experts-clarence-chio.png"
date: 2022-09-20
authors:
  - name: Nolan Sullivan
  - image_url: 'https://uploads-ssl.webflow.com/62ccd7b208cab0723d026273/62cdf9e45dcbb4d20be59f5f_head.jpeg'
tags:
  - API Advice
  
featured_image: "/media/api-experts-clarence-chio.png"
---

### TL;DR

- Tech writing teams are often papering over the gaps created by immature API development processes.
- As businesses grow, the complexity of managing even simple API changes grows exponentially.
- Designing an API is no different from building any product. The question of what framework you use should be dictated by the requirements of your users.
- Don’t build an API just to have one. Build it when you have customer use cases where an API is best suited to accomplishing the user’s goal.
- Good devex is when your services map closely to the intention of the majority of your users.
- The mark of good devEx is when you don't have to rely on documentation to achieve most of what you're trying to do.

## Introduction

[_Clarence Chio_](https://www.linkedin.com/in/cchio/)_, is the CTO and one of the co-founders of Unit21._ [_Unit21_](https://www.unit21.ai/) _is a no-code platform that helps companies fight transaction fraud. Since the company started in 2018 it has monitored over $250 billion worth of transactions, and been responsible for preventing over $1B worth of fraud and money laundering._ _In addition to his work on Unit21, Clarence is a Machine Learning lecturer at UC Berkeley and the author of_ [_“Machine Learning & Security”_](https://www.oreilly.com/library/view/machine-learning-and/9781491979891/)

## API Development @ Unit21

**_What does API development look like at Unit21? Who is responsible for releasing APIs publicly?_**

API development is a distributed responsibility across the different product teams. Product teams build customer-facing features which take the form of dashboards in the web interface, API endpoints, or both. So generally, every product team maintains their own sets of API endpoints defined by the scope of the product vertical that they're working on.

But the people that make the decisions defining best practices and the consistency of how our APIs are designed is typically the tech leads working on our data ingestion team.  As for who maintains the consistency of how APIs are documented and presented to our customers, that is our technical writing team.

**_How come the data ingestion team owns API architecture is that just a quirk of Unit21 engineering? Is there any resulting friction for API development in other parts of the engineering organization?_**

Yeah, the reason for this is pretty circumstantial. Most of the API usage that customers engage with are the APIs owned by our data ingestion team. We’re a startup and we don’t want too much process overhead, therefore we just let the team that owns the APIs with the most engagement, maintain the consistency and define what the API experience should be.

As for friction with other parts of the org, it's largely been okay. I think the places of friction aren’t specifically related to having designated a specific product team to be the owner. A lot of the friction we’ve had was down to lack of communication or immature processes.

A common example is when the conventions for specific access patterns, or query parameter definitions aren’t known across the org. And if the review process doesn't catch that, then friction for the user happens. And then we have to go back and clean things up. And we have to handle changing customer behavior, which is never fun. However, it's fairly rare and after a few incidents of this happening, we’ve started to nail down the process..

**_What is Unit21’s process to ensure that publicly released APIs are consistent?_**

There is a strenuous review process for any customer facing change, whether it's on the APIs or the dashboards in the web app. Our customers rely on us to help them detect and manage fraud so our product has to be consistent and reliable.

The API review process has a couple of different layers. First we go through a technical or architectural review depending on how big the changes; this happens before any work is done to change the API definitions.. The second review is a standard PR review process after the feature or change has been developed. Then there’s a release and client communication process that's handled by the assigned product manager and our technical writing team. They work with the customer success team to make sure that every customer is ready if there’s a breaking change.

**_If I were to ask a developer building public APIs at Unit21 what their biggest challenge was, what do you think they would say?_**

It's probably the breadth of coverage that is growing over time. Because of the variance in the types of things that our customers are doing through our API and all the other systems and ecosystems that our customers operate in, every time we add a new API the interactions between that API and all the other systems can increase exponentially.

So now the number of things that will be affected if, for example, we added a new field in the data ingestion API is dizzying. A new field would affect anything that involves pulling this object type. Customers expect to be able to pull it from a web hook from an export, from a management interface, from any kind of QA interface, they want to filter by it, and be able to search by it. All those things should be doable. And so the changing of APIs that customers interact with frequently cause exponentially increasing complexity for our developers.

**_What are some of the goals that you have for Unit21’s API teams?_**

There's a couple of things that we really want to get better at. And by this, I don’t mean a gradual improvement on our current trajectory, but a transformational change. Because the way we have been doing certain things hasn't been working as well as I’d like.  The first is the consistency of documentation.

When we first started building our API's out, we looked at all the different types of spec frameworks, whether it was Swagger or OpenAPI, and we realized that the level of complexity that would be required if we wanted to automate API doc generation support would be too much ongoing effort to be worthwhile. It was a very clear answer. But as we continue to increase the scope of what we need to document and keep consistent, we realized that now the answer is not so clear.

Right now this issue is being covered over by our technical writing team working very, very closely with developers. And the only reason this work is because our tech writer is super overqualified. She's really a rock star that could be a developer if she wanted to. And we need her to be that good because we don't have a central standardized API definition interface; everything is still defined as flask routes in a Python application. She troubleshoots and checks for consistency at the time of documentation because that’s where you can never hide any kind of inconsistency.

But this isn’t the long term solution, we want to free up our tech writers for differentiated work and fix this problem at the root.  We’re still looking into how to do this. We’re not completely sold on Swagger or OpenAPI, but if there are other types of interfaces for us to standardize our API definition process through, then potentially, we could find a good balance between achieving consistency in how our APIs are defined & documented and the efforts required from our engineering team.  But yeah, the biggest goal is for more documentation consistency.

## API architecture Decisions

**_When did you begin offering APIs to your clients, and how did you know it was the right time?_**

When we first started, we did not offer public APIs. We first started, when we realized that a lot of our customers were evolving use cases that needed a different mode of response from our systems. Originally, we would only be able to tell them that this transaction is potentially fraudulent at the time of their CSV, or JSON file being uploaded into our web UI, and by then the transaction would have been processed already. In many use cases this was okay, but then increasingly, many of our customers wanted to use us for real time use cases.  

We developed our first set of APIs so that we could give the customer near real time feedback on their transactions, so that they could then act on the bad event and block the user, or block the transaction, etc. That was the biggest use case that pushed us towards the public API road. Of course, there's also a bunch of other problems with processing batch files. Files are brittle, files can change, validation is always a friction point. And the cost of IO for large files is a big performance consideration.

Now we’re at the point where more than 90% of our customers use our public APIs. But a lot of our customers are a hybrid; meaning they are not exclusively using APIs. Customers that use APIs also upload files to supplement with more information. And we also have customers that are using our APIs to upload batch files; that's a pretty common thing for people to do if they don't need real time feedback.

**_Have you achieved, and do you maintain parity between your web app and your API suite?_**

We don't. We actually maintain a separate set of APIs for our private interfaces and our public interface. Many of them call the same underlying back end logic. But for cleanliness, for security, and for just a logical separation, we maintain them separately, so there is no parody between them in a technical sense.

**_That’s a non-standard architecture-choice, would you recommend this approach to others?_**

I think it really depends on the kind of application that you're building. If I were building something where people were equally likely to interact through the web as through the API, then I think I would definitely recommend not choosing this divergence.  

Ultimately, what I would recommend heavily depends on security requirements. At Unit21 we very deliberately separate the interfaces, the routes, the paths between what is data ingestion versus what is data exfiltration. That gives us a much better logical separation of what kinds of API routes we want to put into a private versus public subnet, and what exists and just fundamentally does not exist as a route in a more theoretically exposed environment. So ultimately, it's quite circumstantial.

For us, I would make the same decision today to keep things separate. Unless there existed a radically different type of approach to API security. I mentioned earlier there were a couple of things that we would like to do differently. One of them is something along this route. We are starting to adopt API gateways and using tools like Kong to to give us better control over API access infrastructure, and rate limiting. So it’s something that we're exploring doing quite differently.

**_Unit21’s public APIs are RESTful, Can you talk about why you decided to offer Restful APIs over say, GraphQL?_**

I think that designing an API is very similar to building any other product that's customer facing, right? You just need to talk to users. Ask yourself what would minimize the friction between using you versus a competitor for example. You should always aim to build something that is useful and usable to customers. For Unit21 specifically, the decision to choose REST was shaped by our early customers, many of whom were the initial design partners for the API. We picked REST because it allowed us to fulfill all of what the customers needed, and gave them a friendly interface to interact with.

I've been in companies before where GraphQL was the design scheme for the public facing APIs. And in those cases the main consumer persona of the APIs were already somewhat acclimatized to querying through GraphQL. Asking people that haven't used GraphQL to start consuming a GraphQL API is a pretty big shift. It's a steep learning curve, so you need to be really thoughtful if you’re considering that.

**_Also, you’ve recently released webhooks in beta, what was the impetus for offering webhooks, and has the customer response been positive?_**

That's right. Actually, we’ve had webhooks around for a while, but they hadn't always been consistent with our API endpoints. We recently revamped the whole thing to make everything consistent, and that’s why it’s termed as being ‘beta’. Consistency was important to achieve because, at the end of the day, it's the same customer teams that are dealing with webhooks as with the push/pull APIs. We wanted to make sure the developer experience was consistent.  And since we made the shift to consistency, the reaction has been great, customers were very happy with the change.

## Developer Experience

**_What do you think constitutes a good developer experience?_**

Oof this is a broad question, but a good one. All of us have struggled with API Docs before; struggled with trying to relate the concept of what you're trying to do with what the API allows you to do. Of course, there are some really good examples of how documentation can be a great assist between what is intuitive and unintuitive. But I think that a really good developer experience is when the set of APIs maps closely to the intention of the majority of the users using the API, so that you don't have to rely on documentation to achieve most of what you're trying to do.

There are some APIs like this that I've worked with before that have tremendously intuitive interfaces.  In those cases, you really only need to look at documentation for exceptions or to check that what you're doing is seen. And I think those APIs are clearly the result of developers with a good understanding of not only the problem you're trying to solve, but also the type of developers who will be using the API.

## Closing

**_A closing question we like to ask everyone: any new technologies or tools that you’re particularly excited by? Doesn’t have to be API related._**

Yeah, I think there's a bunch of really interesting developments within the datastream space.  This is very near and dear to what we're doing at Unit21. A lot of the value of our company is undergirded by the quantity and quality of the data we can ingest from customers.

We're currently going through a data architecture, implementing the next generation of what data storage and access looks like in our systems, and there's a set of interesting concepts around what is a datastream versus what is a database. And I think we first started seeing this become a common concept with K sequel, in confluent, Kafka tables etc. But now with concepts like rocks set with, snowflake and databricks all releasing products that allow you to think of data flowing into your system as both a stream and a data set. I think this duality allows for much more flexibility. It’s a very powerful concept, because you no longer have to think of data as flowing into one place as either, but it could be both supporting multiple different types of use cases without sacrificing too much of performance or storage.


 This is the content for the doc blog/api-experts-jack-reed.mdx 

 ---
title: "API Experts - APIs that Move Money"
description: "Jack Reed, software engineer at Increase, discusses how the company has built out APIs that helps people build banks."
keywords: [api, openapi, swagger, jack reed, increase, fintech, banking, developer experience, devex, dx, sdk generation, sdk]
image: "/media/api-experts-jack-reed.png"
date: 2023-01-04
authors:
  - name: Nolan Sullivan
  - image_url: 'https://uploads-ssl.webflow.com/62ccd7b208cab0723d026273/62cdf9e45dcbb4d20be59f5f_head.jpeg'
tags:
  - API Advice
  
featured_image: "/media/api-experts-jack-reed.png"
---
## TL;DR

- It’s great to have an industry standard like OpenAPI for API development, but the flexibility in the spec has made true consensus elusive.
- If you’re building an SDK, the goal is to make it as easy as possible for users to make their first request.
- When you’re building in a regulated space, it’s important to offer a sandbox to help customers build quickly and safely.
- At Stripe, maintaining the [API platform](/post/why-an-api-platform-is-important/) required a large team. In the last couple years this API DevEx space has started to take off and democratize API product quality. It’s a welcome development for people who care about this type of stuff.

## Introduction

_Jack Flintermann Reed is an engineer at [Increase](https://increase.com/), a company that provides an API which enables developers to build a bank. Jack spends his time thinking about how to make the [Increase API](https://increase.com/documentation/api#api-reference) enjoyable for customers to use, and scalable for the Increase team to develop. Before he joined Increase, Jack was a staff engineer at Stripe._

## APIs at Increase

**Increase offers a REST API to developers, how did you decide that was the right interface?**

At one point we tried offering both a REST API and a GraphQL API but we found operating the GraphQL API to be challenging and ended up winding it down. There was a cognitive overhead for the user that was hard to overcome: like, welcome to our GraphQL API, here are all the libraries you're gonna have to install before you get started. And you know, GraphQL libraries across different programming languages have really varying quality. The Ruby GraphQL client is okay for example, but other languages not so much. And then, client aside, you need to learn all these new concepts before you can even do what you want. For users it’s like, “I just want to make an ACH transfer here.”
And that’s the really nice thing about REST. Every single language has an HTTP client built into it. There is no stack required to get started. It is really simple. You can play with a lot of REST APIs in the browser’s URL bar if you want to. And that makes getting started with documentation and integration a lot simpler. So the barrier to entry is lowest on REST. And, that's not to say it doesn't come with plenty of issues of its own, which I'm sure we will get into. But we think it’s what is right for us right now.
Ultimately, one of the reasons we had to give up on GraphQL is we realized that because of the level of DevEx we wanted to give users, we only had the bandwidth to do one public API really well, and so we were forced to pick. I suppose that is one of the main reasons Speakeasy exists, right?

**Yes it is. And to explicitly state something that I think you're implying, the developer tools that are needed to support a REST API and a Graph QL API, are fairly distinct?**

I think so. And again it’s partially down to the maturity of the technology. For example, when we think about the SDKs that we want to offer to our users, one of the things that we feel pretty strongly about is not having any dependencies. And that's just not possible with a GraphQL API. Like, you're not going to write your own GraphQL client, you're going to bundle something in. But that exposes you to problems where there could be a version conflict with the client you've pinned yourself to.

And with REST, it's at least possible to avoid that problem. We've recently written a Ruby SDK for our API, and we're pretty happy that it’s got no external dependencies.

**Are you versioning your API, do you plan to?**

We don't version the API yet. We are small enough that we just haven't had to do it. We inevitably will have to, but I'm trying to delay it for as long as possible. It's a lot of work, and there's just not a canonical way to do versioning. So I'm cheating by just making it a problem for my 2023 self.
We do occasionally deprecate things, but we're close enough to our users right now that we can just reach out to them. And for us right now, we're in a phase where the API is mostly additive, we're still building out the core resources. The deprecations we’ve done have been at the API method level. It’s easy enough for us to handle. The real nightmare is when you want to take out this field from this API response. And you have no idea if anyone is relying on it or not.

That's the one that sucks. And fortunately, we haven't had to do that. And we are trying really hard to not have to do that. A good way to save yourself trouble is by being really conservative about what you put in your API to begin with.

That's our approach right now, we want users to pull features out of the API. Yes we have the data, we obviously could expose that field, but until a few people are asking us for it, we are not going to put it into the API.

## Thoughts On OpenAPI

**Do you like OpenAPI? Does your team maintain an OpenAPI spec?**

I'm glad it exists, we publish an open API spec. And it is going to be the foundation of all of the stuff that you and I are talking about right now. We’re going to generate our documentation and clients from our OpenAPI spec. That said, I think it's extremely difficult to work with. So, I'm glad that there is a standard, but I wish the standard were better.

I think OpenAPI has taken this nice big-tent approach where anyone can describe their API with OpenAPI. But there are some crazy APIs out there, right? And so there are a million features inside OpenAPI.

I’ve been working in the API space for a while and it took me some pretty serious effort to try and understand OpenAPI and how to get started. There are a lot of features you could use, but there’s no easy way to separate out the set of features that you should use.

One example that I always harp on is null. If I have a nullable parameter, there's at least four ways to represent that in OpenAPI 3.1. But not every library implements the full OpenAPI specification, so the tools and libraries that I want to have consume my spec might only support a certain way to represent null. So while it's all well and good that you can represent null 4 different ways, if you actually use two of them, none of your tooling will work. And that type of opinionated information is extremely hard to pin down.

**Do you generate your OpenAPI spec from code?**

Yeah, we have this cool library we use internally, I’m pretty proud of it. If I had infinite time, I think there's probably a great little framework that could be pulled out of this. 
Increase is written in Ruby, and we use [Sorbet](https://sorbet.org/) from Stripe (a type checker for Ruby). We’ve built a Domain-Specific Language, you might've seen other companies do something similar, where you can write API methods using a declarative syntax. You say, this API takes a parameter called, “name”, and it's a string, and this one takes “age”, it's an integer. And this library then goes and produces a couple of outputs. On the one hand, it gives you an OpenAPI spec. Then on the other, it handles all the shared application concerns around parsing requests: error messages, type suggestions, etc. Everything you don't want to force individual developers at Increase to be thinking about. It will spit out into the application, a parsed, strongly-typed, parameters object.
And so, the developer experience is pretty nice for folks at Increase. If you add a parameter, it just shows up in the docs and the SDKs without you having to do anything, which is the dream. It’s an all in one generator for artifacts.

## How Does Increase Think About DevEx

**You mentioned earlier that one of the guiding principles in your SDK generator development was limiting dependencies. What other principles do you have for SDK development?**

We want to make it very easy to write your first request. And to achieve that, the majority of the library should be type definitions. It should list out the API methods and each of those should have a nice type definition that makes it easy to autocomplete when you’re working. Then there's the other piece of the SDK, sort of the core, which actually handles making the requests. That should be very tunable, and ultimately, swappable.

I worked at Stripe before coming to Increase. And at Stripe, when we were integrating with an external service, we’d rarely use the official SDK, because of all the things, like internal service proxies, that we would need to configure in order to safely get requests out of the stripe infrastructure. It would have been really nice to say, we have our own way of making HTTP requests, and I'm happy to write adapter code to conform to your interface; I'll just kind of ram it into your type definition. That would have been the sweet spot for us, and that experience has influenced our approach at Increase. We have written it to be the kind of SDK we would have wanted to integrate with.

If people are interested in this stuff, there's [a really fantastic blog](https://brandur.org/articles) by a developer I used to work with at Stripe, Brandur. He’s a prolific writer and often dives deep on the minutiae of good API design. If people haven't read it, they should.

**What about DevEx for APIs? What’s most important?**

There's a lot of different things that go into it. The first piece is documentation, trying to make your system legible to others. Good documentation focuses on the concepts you need to know, without overwhelming you with information.

I think at Increase, we do an okay job of this right now, but it’s one of the things I most want to improve about our own product. We’ve got the API reference, which is great - it's the index at the back of the encyclopedia, but it's not enough. It’s really important to know the typical path for building an integration with your API and then communicate: “here are the five things you're going to want to go and touch first.” I think that initial communication is more important than almost anything else to me when it comes to the experience of using an API.
And again, I'm a major type system enthusiast. And that’s because when you get it right with your developer tooling, your documentation can just flow into the developer’s text editor. And to me, that's the dream experience. But it takes a lot of work to get there.

That’s all concerning understanding the API, and doing the initial integration. There’s also the ongoing operation of the integration code. And that’s about answering questions like: How do I see what I've done? Did I make a mistake? How do I debug my mistake?  That's where tools that enable you to see the requests you've made are really useful.

It’s hard to say what specifically constitutes a great developer experience, but a lot goes into it, and, fortunately for developers, the bar for what great means gets higher every year. Tools are getting better.

**Are there other companies that you look to as a north star when building DevEx?**

So, obviously Stripe gets a lot of praise for being the first company here. They were so much better than anything else at the time they started. And I think if you look back at the things that they did, a lot of them were really small touches. Things like copy/paste for code examples, right? Small, but really nice. Another one was, if you misspell an API parameter, giving you an error message that suggests the correct spelling. It's not rocket science. But those little things add up into something really nice.  I'm obviously really biased because I worked there, but I still think Stripe is the best. I learned a lot from my time there, so it’s still the company I model after most. 
Besides Stripe, I wouldn’t say there’s one company that is necessarily a North Star. I have a loose list of peer companies. When I'm doing research, it’s usually because I’m grappling with the fact there’s a lot of under-specified things in building REST APIs and I want to see how others have handled it before I make a decision. I’m not interested in features or clever things that they’ve done that I want to copy, it’s more about checking whether there is consensus on a topic. If there is a consensus, I want to follow the precedent so that it is not surprising to our users. However, I'm usually disappointed. There often isn’t consensus on the way to do basic features in REST APIs. Which is funny and sad.

Incidentally, that's one of the nice things about GraphQL. They tell you how to do everything. It’s very proscriptive.

**How is the value of Increase’s investment in DevEx measured? Are there metrics you want to see improve?**

It's not really obvious, it's much more of a qualitative metric. We open a Slack channel for every new user, and we stay pretty close to them during the integration. We're not quite looking over their shoulder, but if we could, we would. And so, we're pretty sensitive to how that kind of first experience goes. We try to get feedback from pretty much everybody who goes through it. And so, it’s just things that we see mainly. Like if everyone is running into a similar stumbling block, we prioritize removing it. It's not like there's a KPIs dashboard on a wall. It’s things we see in conversations with users every day. It scales pretty well if the whole team pitches in on working with users. Slack Connect is a pretty great tool.

## Increase’s Journey

**You guys are a startup, but you’ve been investing in your API’s DevEx from the get go. Was that a conscious decision?**

Yeah, again, It's pretty informed by our experiences at Stripe. It’s also a consequence of the space we work in. We offer a set of banking APIs, and traditionally banking API integration is pretty laborious. With a traditional bank, it'll be an extended contracting phase, then you get to a signed contract, and finally you’ll get this PDF for docs. And then the integration involves sending files back and forth between FTP servers. It’s not fun.
And so as a startup in the space, our goal is to make the integration timeline as short as possible, and as dev-friendly as possible. Even in the cases where we need a signed contract before a user can go live in production, we enable their development team to build while that’s being sorted.

**Ya, please expand on that a bit more. As a banking API there’s inevitably compliance. How do you guys balance integration speed with compliance?**

It depends on the use case - we can get most companies live in production very quickly. And then there are some that require more due diligence. That's where tooling like an API sandbox is helpful. With a sandbox, you can build your whole integration. The dashboard experience looks and feels real, and so you can just build out what you need while the legal stuff is handled.

We’ve learned that it takes a lot of work to make a good sandbox. We have to do a lot of weird things to simulate the real world. For example, in the real world there's several windows throughout the day when the federal government processes ACH transfers and they go through. So if a developer makes an ACH transfer in the sandbox, what do we do? Should we simulate that happening immediately, or wait like the real world. There’s not always a right answer. We actually built a simulation API, where you can simulate real world events in all kinds of tunable ways. And so that has been fascinating. It made us re-architect a fair amount of code to get it working.

## Closing

**In the longer term, are there any big changes you think are coming to the way people interact with APIs?**

Haha I think this question is above my paygrade. It's not quite the lens through which I think about things… But, I guess one thing that is interesting is how many companies are starting to pop up in this API DevEx space. It seems like there were zero people working on this stuff two, three years ago. Now there's a few.

Before I joined Increase, I was thinking I might start a company of my own in the space. One of the ideas I was kicking around was a platform for webhook delivery. I've seen a bunch of new startups doing that over the interim 2 years.

I think that’s cool. The size of the team that maintained these things at Stripe was big. It required a lot of manual work. And so it's cool to see that level of API product quality being democratized a little bit. Like I said, I think the quality bar will continue to rise, and it has risen. But, today, it's still a pain. It still takes a lot of work, you really have to care about it. I'm hoping it becomes a little bit easier to buy off the shelf as time goes on.


 This is the content for the doc blog/api-experts-mathias-vagni/index.mdx 

 ---
title: "API Experts - APIs to Support your Customers"
description: "Mathias Vagni, Plain's CTO & Co-founder discusses API-first development and the novel DevEx they've built to support their API."
keywords: [api, graphql, mathias vagni, plain, customer support, react, react embeds, developer experience, devex, dx]
image: "/media/api-experts-mathias-vagni.png"
date: 2023-01-31
authors:
  - name: Nolan Sullivan
  - image_url: 'https://uploads-ssl.webflow.com/62ccd7b208cab0723d026273/62cdf9e45dcbb4d20be59f5f_head.jpeg'
tags:
  - API Advice
  
featured_image: "/media/api-experts-mathias-vagni.png"
---

## TL; DR

- Building your API to be public (without private endpoints) pays huge dividends in the form of extensibility for your customers.
- GraphQL requires some extra legwork to cover the basics, but comes in handy for supporting more advanced UIs and apps.
- The Plain team is not only working on their APIs’ DevEx, but is trying to make it easy for customers to build their own API endpoints.
- For smaller companies, the qualitative feedback on your developer experience is more useful than tracking metrics.

## Introduction to Plain

**To warm up, could you give a brief overview of [Plain](https://plain.com/) and what y’all are building?**

Plain is the customer support tool for developer tools.

My co-founder Simon and I both found that existing customer service platforms were really hard to integrate with and fundamentally not built for engineering led products. Their APIs were afterthoughts tacked on and dumbed down to be made public. Their data models and workflows assumed that they were the primary store of customer data vs your own databases. These assumptions just didn’t make sense, especially in the developer tools vertical.

We wanted to build a customer service platform that is incredibly easy for developers to build with. The inspiration came from our experiences at Deliveroo (food delivery) and Echo, an online pharmacy. All of Plain is built API-first, meaning the API we make available to users is identical to the API we use internally. This means that there are no restrictions with what you can do programmatically. Anything you can do in Plain, you can do via the API. By dog-fooding our API this way, we end up constantly working on making it easier to build with and integrate Plain into your own systems.

## Architecture of the Plain API

**How does being API-first impact product development? Move fast and break things doesn’t work so well for APIs right?**

Yeah, we have virtually no private surface area, which is a challenge since it means that everything we build is exposed. Things that are traditionally quite easy are harder, for example deprecating an endpoint, but the rewards are massive.

You have to move a bit slower at the beginning, especially when you're conceiving a new feature. It forces you to think quite carefully as to where you put logic and where you ‘encode’ your opinions if that makes any sense. For example, [our data model](https://docs.plain.com/data-model) is something that is integral and is set in the API. A less clear cut case is certain opinions around how you should work as someone providing support to users. We do our best to make sure that opinions are more in our UI and the Plain app, and the API remains more generic.

As a result of this, we definitely have more in depth conversation around API design than you would normally see at an early stage startup. The payoff though is huge. When we onboard customers, and our platform is missing certain features, they are able to extend and just build the features themselves.

As an example, we had one customer using Discord to manage their community. We didn't offer a Discord integration. So they built it. It’s super cool, now they’re able to talk to users in Discord and sync it to Plain. That's where you reap the benefits of an API first approach.

**One of the first decisions an API company needs to make is REST or GraphQL, Plain is GraphQL-based. What persuaded you that GraphQL was best?**

We realised early on that most companies want to use customer support APIs to automate only small parts of their workflow. For example when they have an issue in their back-end they might want to proactively reach out to that customer and so they might open a ticket. In these early stages of companies, the requirements for customer support tooling are quite simple so we were very tempted to use REST. For simple API calls REST is typically way easier.

However at the same time we learnt that many people were looking for a productivity focused, power user friendly, support app. This is especially true for engineers as a demographic. Given this, GraphQL seemed like a much better fit. For building complex applications the schema stitching required by REST can be very painful and it was something we were keen to avoid.

Ultimately weighing both of these conflicting requirements we went for GraphQL. We felt like if we start with a GraphQL API, we could always add a REST layer on top later. The reverse would be more difficult to do in a performant way.

**I’ve spoken to some other founders who started with GraphQL, before switching to REST. One of their criticisms was that the ecosystem of GraphQL tooling left a lot to be desired. Yes, What has your experience been?**

There are some things that are great. And then there are some things that you expect to be good, but are terrible. For basic apps, where you just want to make some queries and run some mutations, you're going to have a great time. You make some GraphQL files, generate all of your types. It can even generate clients for you. There's lots of libraries that take care of things like caching and normalisation, definitely in the React ecosystem, but also in others. So that's all fine.

I think, where you start running into GraphQL weirdness and where the ecosystem leaves a lot to be desired is in terms of some more basic elements like error handling and complex input types. With REST APIs a 401 is an unauthorized request, you don’t need to explain that to anyone. And because you are not restricted to a type system you can do things that are just plain awkward in GraphQL (e.g. union input types).

**How do you handle errors in GraphQL then?**

Unlike REST, GraphQL is multileveled so certain problems become harder. Suddenly, you might be in a situation where the user has top level permissions to get something, but then doesn’t have permission for one of the nested elements. The way that you handle that error is DIY. There’s no (good) convention to follow.

We made some good and bad decisions in our early days. What helped was that very early on, we decided to write our own linting rules. GraphQL has an inbuilt tool that essentially parses the GraphQL schema when a request comes in, and with this, you can write rules to lint your schema. We write our schema first, so the linters enforce convention before any API code is actually written. And it’s not just syntactical errors, our linters enforce things like, every mutation must return an error, an error must be of this type, etc. It’s served us really well, because it means that we have very few debates on PRs around repeated API design topics.

**What’s been the client reaction to the GraphQL API?**

It's interesting. I think the more complex the use case, the happier users are with GraphQL. There's obviously still a bit of a gap between GraphQL and REST when it comes to awareness, and we do encounter companies who want to use Plain where the engineers have never worked with GraphQL, beyond reading blog posts. It's definitely a barrier, but not insurmountable; it just requires a little bit more hand holding. We help customers through this by giving them code examples and instructions on how to make GraphQL requests, how our errors work, that kind of thing.

Overall, we’ve found that as long as you’ve put work into developer experience and consistent API design, developers will pick things up quickly. And we are militant about the consistency and experience of our API. As an example, we provide incredibly thorough error messages, which is something that developers love. Once they start building they quickly realise: “Okay, this is solid.”

## Plain’s API DevEx

**That's a good segue. What additional tooling do you give to users to support API integration? What tooling improvements are you looking to make?**

Before we dive in, it's worth knowing that there are two ways you build with Plain. There is the GraphQL API, which we've been talking about, but there’s also something we call [Customer Cards](https://docs.plain.com/adding-context/customer-cards). They are really interesting, because they’re the inverse of the traditional way that other support tools work. Instead of our customers calling us and syncing data (when the support tool is the primary source of truth), our users provide a URL, which we call to fetch *your* customer data which is then loaded up within the Plain UI.

![customer-card image](./assets/customer-card.png)

This means that when your support team is helping someone they instantly have access to context from your own systems about the customer they are trying to help. What account tier they are, what their recent activity has been, how much they're paying you every month, etc.

We want that data to continue to live in our customers systems, so for the product to work, we need to help our customers construct an API endpoint which we can call. We’ve put in quite a bit of work into the DX of Customer Cards but I think our developer experience is a work in progress. It’s a fairly novel workflow, so it’s harder to do well than when trying to document an API.

import portal_url from './assets/customer-card-playground.mp4'

  <video controls={false} loop={true} autoPlay={true} muted={true} width="100%" alt="customer card playground">
    <source src={portal_url} type="video/mp4" />
  </video>

**How have you been trying to solve it so far?**

I think we've made some good steps. We’ve built a playground that can assist users while they’re building which is quite nice, but there's definitely more to do. This data transfer is async. It's happening all the time. And so error handling and the developer experience here is actually a lot more challenging than a traditional API. We have to provide a lot more visibility on errors and monitoring. We need to know if you responded with something wrong, and why it was wrong. We then need to notify you that it was wrong and persist it so that you can fix it. The same goes for timeouts and anything else that’s unexpected. It’s complicated and we’ve not totally solved this experience.

**Do you offer SDKs for the Plain API?**

We haven't yet, but we plan on it. We’ve been relying on every ecosystem having its own stack, for generating GraphQL tooling. But we plan on offering SDKs to make integration easier, and to make errors easier to understand and parse. We really want to get to a place where, for a simple use case, you don’t have to deal with GraphQL at all. If you look at how Slack does it, it’s very good. No one is actually making raw Slack API calls, right? They're all using the client and the playground provided to visually work out how to construct messages and do things with bots and so on.

**Any other DevEx challenges that you’re tackling?**

I think on the API side, we’ve covered it, we really just want to make it easier and easier to integrate and use Plain. It's our raison d’être, I don’t think we’ll ever ‘finish’ working on DevEx

Outside of our API, we also have [a chat solution](https://docs.plain.com/support-channels/chat/overview), and we’ve spent a lot of time thinking about the DevEx there. If you’re using a React app and you want to embed chat into your product, it’s a UI-level integration, and that has its own challenges. If you look at how most support tools or chat providers allow you to embed, it's through a script tag. You add a tag, and a floating button appears on the bottom right. And that's your chat. In our world, we wanted to allow chat to be a bit more embeddable, to essentially look like it's part of your product and deeply look like its native. To do that, we've built a React package.

It’s been a tough nut to crack. Everyone has such specific expectations of how your embed should behave. And you're confronted with the multitude of different stacks and approaches people take. People do things that are super delightful, but unexpected. And that's where the complexity comes in when you are trying to deliver that seamless Developer Experience.

**Are there metrics you track to measure impact of the work on your API’s DevEx?**

Not yet, we're still so focused on every customer that metrics don’t really make sense yet. We track all the feedback we get meticulously. Even slight grievances with our API or our integrations we discuss thoroughly. That's a scale thing, largely.

What's also interesting is, by focusing on developer tools as a customer base, the feedback we get is really, really good. You get feedback similar to when an in-house engineer reports a bug. So. much. detail. So yeah, for us, feedback has generally been very, very specific and high quality.

## What’s the Plan for 2023

**What’re you most excited for in the coming year?** Plain is now in a really cool place where we have moved on from the basics and are now getting really deep into some of the hard problems within customer support. For example, right now we're looking at managing triaging and SLA and complex queue management. It's going to bring with it, a whole host of other API integrations, to enable users to be in control of their support queues and prioritise some customer issues over others, and so on. I really can't wait to share our solution here.

We’re also going to be growing the team ([we’re hiring!](https://plain-public.notion.site/Help-us-power-support-for-the-world-s-best-companies-7ea2f1a4cc084b96a95141a30e136b5b)) and onboard many more customers - it’s going to be an exciting year for Plain!


 This is the content for the doc blog/api-landscape.mdx 

 ---
title: "The Speakeasy API Landscape"
description: "Our thoughts around the day-to-day challenges facing developers on the ground as they build and manage their APIs."
image: "/media/api-landscape.png"
date: 2022-06-30
authors:
  - name: Sagar Batchu
  - image_url: 'https://uploads-ssl.webflow.com/62ccd7b208cab0723d026273/62cdf303b28bf9598d7a6b63_sagar_headshot-p-500.jpeg'
tags:
  - API Advice
  
featured_image: "/media/api-landscape.png"
---

Two weeks ago Postman published their [**API Platform Landscape**](https://blog.postman.com/2022-api-platform-landscape-trends-and-challenges/). They’ve done a great job highlighting some of the organizational challenges faced by companies building APIs.  We at Speakeasy wanted to chip in with our own two cents. Specifically, we wanted to highlight the day-to-day challenges facing developers on the ground when they’re building and managing APIs.

Our thoughts have been shaped after spending a lot of time over the past 6 months talking to developers about their experiences building & managing APIs. To put some numbers behind it, my cofounder Simon and I have spent a combined 70 hours interviewing over 100 developers about their experiences. All of that goes into our work at Speakeasy, and in the spirit of openness, we wanted to share our learnings with everyone else. So, without further ado, here’s what we have learned…

## Trends

### Developers want good UX

We touched on this in an earlier post, but developers are increasingly demanding that the tools they use have a great UX.  Tools should largely abstract away complexity without becoming a black box.  This doesn’t mean just throwing a GUI in front of the terminal. Developers want tools that are embedded in their existing workflows. This may mean that a tool intended for developers needs to have multiple surface areas. Features may exist as a CLI, VS Code extension, Git Action, Web app, React Embeds, API endpoint, etc. DevEx should prioritize helping the user accomplish their task with minimum friction and deviation from their existing workflows, and be flexible on where/how that feature is exposed to users.

### A “Design-first” approach has limits

The “Design-first” approach to APIs is often presented as a virtue, to be contrasted with the dirty sin of a code-first approach (implementing the API without formal design).  Most companies will claim they follow a purely design-first approach, however talking to developers on the ground, we found a much more nuanced reality.  In fact, it’s almost always a mix of the two.  Many teams confessed that they felt writing the whole Open API spec was a tedious exercise for little gain. In their weaker moments they would often forgo the full open API standard in the beginning in order to get started faster, then they would go back and generate it later for the purpose of documentation.

### Different API frameworks for different jobs

GraphQL, gRPC, Websockets, are all hot on the block, meanwhile RESTful still constitutes the vast majority of APIs (80+%). Although the twitter debates continue to rage and there will always be champions for different paradigms, we see things eventually settling into a comfortable coexistence.  Websockets are great for real time applications, gRPC is great for microservice architectures, GraphQL is a great choice when you control both the client and the data source, and REST is great when you are/will be working with external consumers.  We expect that in 5 years time it will be very normal for companies to be using all of the above as part of their technical architecture.

### API Platform teams are choosing to build on top of the best

Thanks to a tight labor market and the Cambrian explosion in 3rd party dev tools, platform teams have switched tactics when it comes to how they support application developers. In the past, internal teams struggled with the sisyphean mandate of trying to handroll an entire scaffolding to support their companies development. We’re seeing a new trend emerge, where lean platform orgs have the mandate to pick the best vendor products available, and stitch them together to create something tailored to their own business’s needs.  This helps to keep the company’s resources focused on the creation of differentiated value for their customers.

## Challenges developers face

### API testing is still limited

When it comes to exhaustive testing of an API, there can be a dizzying number of parameter permutations to consider.  So, while it’s great that developers have tools that let them easily mock an ad hoc API request, creation of a test suite for production APIs can be a major pain point.  Similarly, test suites for internal microservices (understandably) don’t get the same attention as endpoints used by external clients. However, oftentimes the external endpoints rely on a collection of microservices to work. It can be hard to effectively test the external endpoint if there’s no corresponding test suite for the microservices it relies on. If behavior changes is it because something in the external endpoint changed, or one of the underlying microservices?

### API usage is hard to grok

Which customers use this API? Can this old version be deprecated? Is anyone still using this parameter?  All common questions developers ask, all unnecessarily difficult to answer. The lucky developers have Datadog dashboards and full sets of logs in warehouses.  However, many developers lack this sort of reporting infrastructure.  [**It’s a big up front investment to set up, and an expensive ongoing cost**](https://www.linkedin.com/feed/update/urn:li:activity:6945789783235338240/).  Small orgs can’t justify the upfront time investment, and even larger orgs often go without for internal endpoints.  The result is that developers lack an API-centric view where they could easily get a real understanding of how their API is being used in the wild.  This makes it difficult for new developers to quickly grok how the API endpoints are used by consumers, how they work internally, and what to consider when working to evolve the API.

### APIs are stuck on v1

When companies roll out an API, they will confidently put v1 in the API path. This is to highlight that in the future, they will be evolving the API and rolling out a new version, v1.1 or maybe even a v2 someday.  Often though, the API never moves past v1.  Now, this could be because the developers who built the API were oracles with perfect foresight and the API has never needed to be changed. In that case, hats off.  More commonly though, the APIs failure to ever evolve is a byproduct of a broken platform behind the scenes.  In conversations we heard devs say over and over, “We know it’s going to be painful down the road, we’re trying not to think about it.” Without a robust platform providing scaffolding, versioning APIs is a near impossible task. Teams opt to live with the pain of never updating their APIs. In cases where the team gets really desperate, they may change the behavior of the v1 API and brace themselves for the storm of angry client emails.

### Every problem with APIs is exacerbated for internal APIs

It’s been 20 years since [**Jeff Bezos’s (in)famous platformization memo**](https://chrislaing.net/blog/the-memo/). At this point, most software companies have formally adopted a microservice architecture mandate; there’s no doubt that companies are chock-a-block with services. The problem is that microservices tend to get micro-resource allocation when it comes to their tooling.  All the problems we discussed: grokking usage, exhaustive testing, service evolution, are exacerbated for internal APIs. This leads to stale documentation, unintended breaking changes, and unexpected behavior.  Left to fester, it can create a culture where developers become reluctant to trust that a service works as described. They will spend time interfacing with internal teams to make sure that the service works as they expect.  Loss of trust in reliability has real costs in terms of developer’s productivity.

### Developers lack support from the wider org

APIs are an inherently technical product. They are built by developers, and they are consumed by developers. It’s therefore understandable why organizations have siloed all aspects of managing APIs to their development teams.  But if you consider that APIs are a product line for many businesses, that is foolish. APIs require customer support, and product marketing and all the same resources as a conventional software product.  When API development teams are left to handle all these aspects of product development on their own, they either: 1) do it poorly because it’s not their expertise, or 2) don’t have time to develop new features, because their time is sucked up by other management concerns.


 This is the content for the doc blog/api-ops-usage-monitoring/index.mdx 

 ---
title: "API Ops - The Difficulties In Monitoring API Usage"
description: "API monitoring shouldn't be difficult. Speakeasy gives you the proper tooling to reduce API management pain. Learn how we can improve your API monitoring."
image: "/media/api-ops-usage-monitoring.png"
date: 2022-05-23
authors:
  - name: Nolan Sullivan
  - image_url: 'https://uploads-ssl.webflow.com/62ccd7b208cab0723d026273/62cdf9e45dcbb4d20be59f5f_head.jpeg'
tags:
  - API Advice
featured_image: "/media/api-ops-usage-monitoring.png"
---

When building & managing APIs, understanding usage and monitoring for changes is critical to success.  The following post summarizes my experience working on LiveRamp’s web APIs (2016-2018). I will go through the tooling & processes we used to diagnose and triage issues with our APIs (~6 billion requests/day), as well as the gaps in tooling I wished were filled. I hope that other people working on API operations find this useful, and share what has worked well for them.

Before diving in, I’m fully aware that in software, citing experiences from 4 years ago is often an exercise in exploring the ancient past; pace of change and improvements typically render anything older than 2 years null and void.  Given the way APIs and microservices have moved from the periphery to foundational components of the modern tech stack over the last 10 years, the expectation would be that the way APIs are built and managed would’ve correspondingly undergone significant change.  And yet, the primary tooling for monitoring, detecting and troubleshooting APIs doesn’t seem to have evolved much during my hiatus from the space.  People are by and large still using the same set of tools as they were back in 2016.

## Overview of Tooling

Our team was a laboratory for experimentation within the business, and consequently we had our hands on some of the best tools available. Consequently we got exposure to some of the most commonly used tools.  When I began working with the team, our logging/monitoring setup was:

- **Cloud Provider**: AWS
- **Logging infrastructure**: Kinesis Firehose
- **Data Warehouse**: Redshift/Athena/S3
- **Analysis & Insights**: Kibana & Datadog

By the time I stopped working with the web APIs team, the setup was:

- **Cloud Provider**: GCP
- **Logging infrastructure**: Gazette ([**the precursor to Estuary’s Flow**](https://docs.estuary.dev/))
- **Data Warehouse**: BigQuery
- **Analysis & insights**: Grafana & Datadog

Regardless of the stack used, the tools did what they were intended to do. More interesting to me is what the tooling couldn’t do and the operational processes we developed to cope with that lack of tooling.

## Plugging the gaps with Process

Logging infrastructure was great for alerting us that something was amiss, but diagnosing and triaging the issue was an APIOps activity that was largely manual.  To help ourselves we developed processes to categorize issues, and then respond accordingly:

![Issues categories with impact, frequency, and description.](./assets/api-ops-usage-monitoring-image-01.png)

Here’s an overview of how we would diagnose issues…

![A diagram of how issues are resolved.](./assets/api-ops-usage-monitoring-image-02.png)

Rather than beat a dead horse by reiterating the exact text in the flow chart, I want to dive into dealing with breaking change issues. I will give some specific advice for proactively mitigating these types of issues, what the challenges are and what the gap in tooling is.

### Breaking change to an upstream dependency

- **Note:** In the microservices world we’re all living in, it’s very likely that an external service relies on a dozen or more internal microservices to function.  The typical software business might have hundreds or thousands of microservices.  For most, it’s not realistic to invest in the same level of tooling as they do for external API endpoints (logging, dashboarding). It’s therefore entirely possible, dare I say likely, that a microservice team is unaware that external APIs are dependent on their service.
- **How to diagnose**: All that in mind, these can be tricky to diagnose. The first sign will be an anomaly in your traffic patterns. Figure out when the anomaly first appeared, get as specific a time if you can (hopefully you’ll be able to find a step change). First check to make sure that the anomaly doesn’t line up with a push by your own team.  Next you need to figure out if an upstream dependency is the cause.
- **How to triage**: You could take time to dive into the code to test dependencies, but for the sake of speed, I recommend making a list of the teams responsible for the microservices your team depends on. Reach out to each team to check if any of them made a change around the time the anomaly appeared. If there’s a match, ask them to rollback their change so you can see whether the rollback fixes the problem.  After client impact has been mitigated, take the time to dive in and figure out where/how the dependency broke.
- **Advice**: For improved speed, I recommend maintaining a list of the dependencies, and the owning teaming for each. This will allow you to move faster when it matters most.  As I mentioned above, a lot of team’s lack the tooling that would be required to mitigate issues like this.  

### Breaking change to the API itself

- **Note:** Breaking changes in themselves are not necessarily a problem if you are properly versioning your API ([**Brandur Leach wrote a great piece on versioning**](https://stripe.com/blog/api-versioning)).  In this case, I’m referring to accidental breaking changes, which are definitely a problem.
- **How to diagnose**:  Fastest way to spot them is if you notice a spike in error codes and it corresponds with a release your team made to production.  Unfortunately, not all breaking changes trigger spikes in errors.  The problem may only manifest when two API calls are used in sequence, see if there’s any kind of unexpected change in the overall traffic patterns (e.g. drop in total calls to an endpoint).
- **How to triage**: If an anomaly in the data corresponds with a push to production, then rollback the change ASAP.  Even if you haven’t diagnosed the exact error, the priority is client stability.  Later once you have made a diagnosis, address the issue, and test very carefully before pushing to prod again. You will owe your customers an explanation, write out what caused the error, and the steps the team is taking to make sure the issue never reoccurs.
- **Advice**: Again this is a hard one.  If you’re not doing versioning, then do versioning before your client’s leave you.  Integration testing is also definitely your friend. Similar to upstream breaking changes, feels like there’s a tooling gap here that could be better addressed (discussed below).

## The Gaps as I see it

As I said earlier, it strikes me that the development of these processes was in order to cope with a lack of tooling.  There are a few things that it would be great to have, which aren’t available today.  My hope is that in time Speakeasy is able to help devs make steps in addressing some of these gaps.

1. **API-centric monitoring:** The entire process for troubleshooting I layed out is dependent on making a not insignificant investment in logging and dashboarding infrastructure.  Without these, understanding usage is difficult and the surfacing of issues is likely to come in the form of an angry email from your client. It’s always struck me that we were extremely fortunate to have been able to make that investment in a robust logging infrastructure, complete with relevant dashboards and automated alerting.  At Speakeasy we believe there should be something more API-native and developer-first. Devs should be able to drop-in a couple lines of code and get something that works out of the box. This would provide smaller teams the same level of insight into usage as large companies. Who’s using which API version, is the integration healthy, are there anomalies?
2. **API Developer Guardrails:** Most unintentional breaking changes follow similar patterns: adding a required field, changing the response structure, changing a method or resource name, etc. Introducing some guardrails that could sense check changes for common mistakes would go a long way towards helping devs avoid costly client errors.  It’s something that a lot of big companies have, that is often too expensive for a smaller org to develop internally.
3. **API Dependency Mapping:** I mentioned how it can be useful to maintain a list of the dependencies an API depends on.  That’s really a stop gap measure. It would be better if this could be automatically tracked, and what would be even better is if there was tooling which made it easy for microservices to understand the usage of their services, so they could track use by external services.

I’m really curious about how much overlap there is between my experience and those of other people. Would love to hear what off-the-shelf tools other people have used to manage their API operations, and what processes people have developed internally to cope with the lack of tooling.


 This is the content for the doc blog/apis-for-global-shipping.mdx 

 ---
title: "API Experts - Shipping APIs for Global Shipping"
description: "Find out how Flexport is building an API platform to make global trade easy for everyone."
keywords: [api, openapi, swagger, eric chung, flexport, logistics, shipping, developer experience, devex, dx, sdk generation, sdk]
image: "/media/api-experts-eric-chung.png"
date: 2022-10-27
authors:
  - name: Nolan Sullivan
  - image_url: 'https://uploads-ssl.webflow.com/62ccd7b208cab0723d026273/62cdf9e45dcbb4d20be59f5f_head.jpeg'
tags:
  - API Advice
featured_image: "/media/api-experts-eric-chung.png"
---

### TL;DR

- Platformization efforts can start off as a single team mandate, but you need to figure out how to scale them into an org wide mandate to be successful.
- For larger companies with an acquisition strategy, an API platform can be the difference between a successful or a failed integration with an acquired business.
- Public API teams often evolve into having a DevEx mandate. Making users successful in using your API isn't narrowly focused on documentation.
- For a good DevEx, focus on eliminating the need for any interaction with customer support.

## Introduction

**_Could you explain what you & your team are responsible for?_**

I'm a Senior Engineering Manager of Flexport’s Public API team, which also goes by the moniker of ‘Developer Platform’. Those two things are related because the API platform serves as the foundational piece of our external developer experience. We're a 2.5-year-old team, and we’re looking to greatly expand our scope to become the horizontal tooling provider for all the internal Flexport engineering teams who publish and maintain APIs, and enable them to give external non-Flexport developers a superior industry-leading experience.

[**On flexport.com, if you open the “Developers” menu item**](https://developers.flexport.com/s/?_ga=2.155367484.687997278.1658178205-206804922.1658178205) – everything listed there is built by, or supported by my team in partnership with teams across our entire Technology organization.

## APIs At Flexport

**_What are the key goals for the API platform team?_**

A key goal for the company is to become more API-first – as our Executive leadership has said, “The vision is to build the technology platform to make global trade easier. We will open up more and more capabilities for anybody in the world to use”; [**we are building the API platform for internal developers, partner developers, as well as 3rd party developers**](https://www.freightwaves.com/news/flexport-to-open-platform-for-third-party-app-developers). That’s the strategic product vision my team seeks to achieve, so internal and external application developers can build on Flexport’s tech platform. I think this story will sound familiar to many companies: APIs will only increase in importance as a lever for Flexport’s future growth and scale.

A key goal for my team is to increase end-to-end automation for internal and external API developers, which will, in turn, increase API creation and adoption. We’re always thinking about ways that we can empower these engineering teams to expose their services externally, as easily and quickly as possible – while giving them access to the critical functionality needed to run a great API. That means taking care of everything from observability, telemetry, monitoring, testing, scalability and reliability, through to hosting developer portals, documentation, developer tutorials, code samples and OpenAPI schema creation.

**_What is the value of having the API Platform?_**

It’s all about increasing efficiency and throughput for internal teams, our customers and ultimately growing the business. Flexport constantly launches new product lines; APIs are critical for letting teams build products quickly and allowing partner developers to create engaging apps to delight our customers. Our API platform enables the internal teams to develop and maintain APIs faster, and deliver the consistency external developers deserve.

It's worth mentioning that, because we are a large company, Flexport does make acquisitions and strategic partnerships with other tech companies. When we’re making an acquisition or building a new partnership, our APIs facilitate the integration and a frictionless customer journey.

**_You mentioned before that Flexport is ‘becoming’ API-first. Can you tell us about what Flexport is evolving from?_**

Flexport is a company that was founded in 2013. We started off delivering much of our service via web apps, though our public API usage has steadily increased over time. As our customer-base has grown, we’ve added more clients and partners who require enterprise-level platform integrations via API and want to embed Flexport functionality in their own software applications, which is why APIs have become a major focus as we seek to aggressively automate all aspects of the vast global supply chain which includes many operational tasks from origin to destination.

To seed the process of evolving from the monolith to a more flexible SoA/microservice architecture, API ownership was driven by my team as a consolidated initiative. That’s why my team directly maintains a portion of Flexport’s public APIs. That was good for getting started, though we sought out a more scalable solution for the long-term. APIs are merely a different interface for accessing the same set of entities and operations that Flexport’s web apps and internal APIs offer; therefore, each product team should own their corresponding APIs in a federated model. We’re nearly done with this transition; each individual product team is starting to own the REST APIs in their vertical domain.

Going forward, my team’s role will be to focus more on their enablement, maintaining consistent REST patterns, observability, automation, testing, monitoring and providing a world-class experience to our internal and external devs who call our public API; though when business priorities necessitate, we can still quickly mobilize our resources to directly implement new APIs on behalf of domain teams to help accelerate delivery. Our goal is public API feature parity with the functionality available in our web apps.

Flexport’s own application and platform integration developers are major consumers of our public APIs currently and we will continue to build and launch customer applications on top of our public APIs; there are a number of exciting apps and new APIs in various stages of development that our customers are going to love.

**_What type of tooling has your team rolled out to the teams at Flexport_**

Yeah, good question. Networking and security optimizations were some of the first things we tackled. Then we partnered with other teams to standardize infrastructure, identity management, and so forth. Next, we focused on building a comprehensive API best practice guide: from the networking layer, down to what the routes look like along with modeling new personas. We want to make sure the look and feel of Flexport's APIs reflect REST standards. We also developed opinionated guidance about things like pagination, sorting, filtering, resource structure, URI, versioning and Documentation. We’ve launched Flexport’s [**Developer Portal**](https://developers.flexport.com), in addition to our [**API reference**](https://apidocs.flexport.com).

So now, these templates and guidance are all documented for Flexport teams, and we are turning our attention to making implementation as turnkey as possible. Self-service is our northstar; both for the internal Flexport developer publishing and consuming APIs, and also, for the external developers we serve. We have rolled out 24x7 public API service health monitoring with our [**API status page**](https://status.flexport.com) and are proud to consistently exceed our uptime availability target.

## Developer Experience

**_You mentioned your team has a larger DevEx mandate. How did you grow from API ownership to DevEx ownership?_**

This is a pattern I’ve seen at other companies I’ve worked at as well. It’s common to start with a public API enablement team that has a fairly broad charter. At my previous job, we were building a lot of those frameworks and platforms for the businesses, which the different business units and teams could all leverage, and the end goal is always to make sure that the APIs have a cohesive customer experience across all the different product lines. And then that goal naturally expands from cohesive API experience to cohesive developer experience across the whole external surface.

**_What do you think constitutes a great developer experience?_**

I've been professionally working on public API and developer platform enablement / DevEx for almost a decade. I’ve collaborated with a lot of developer advocates and partnered with many folks building incredible industry-leading developer platforms. One of the things that is essential to a great developer experience is frictionless self-service. In a given year you should not need to talk to a human being via phone or email in order to successfully build applications on a strong technology platform. You only have one chance to make that positive first impression. And if you say, ‘Hey, you must talk to a human being to get your account verified’, most developers won't come back, especially if you're not really a well-known entity. I’d also avoid having a paywall. There are metrics that show that having a paywall in front of an API will cause your developer conversion to drop significantly. I recommend using a rate-limited free tier for developers to use in order to increase adoption.

Another part of self-service is the documentation. [**You need to have very good and accurate documentation for developers**](https://apidocs.flexport.com/). You should provide code samples, SDKs, example applications, and [**status pages**](https://status.flexport.com/). My opinion is that the best you can provide is a sandbox for developers to actually interact with live. But you should make an effort to provide documentation in various formats. Written first and foremost, but some people respond better to video tutorials. We want to provide API consumers self-service access to metrics and usage dashboards, logs, errors and other configurations across production and non-production environments.

Lastly, you want to make sure the surface area is consistent and follows standards. For me personally, when I start using an API and I can see that they haven’t implemented best practices, I will question whether the company is reliable. For example, if the API isn’t rate-limited then that makes me think that they don’t really think things through. So, make sure you are embracing those best practices from the beginning.

## Improving APIs

**_What are some of the KPIs your team tracks against?_**

Of course we look at uptime, since availability of our services is table stakes. We then look at active users over a given timeframe, usually monthly or weekly for our business. We track the active users (MAUs), latency (p99, p95, p90, etc.), volume of requests, uptime availability and error rate for each endpoint, and also the emitted web hooks. Those are the most basic metrics that are universal across the teams. Every team may have its own additional KPIs that they care about as well depending on what their team’s specific business objectives are.

**_What are the main challenges facing developers building public APIs at Flexport?_**

Our public APIs are not quite at feature parity with our internal capabilities. Our push to automate API operations will help improve our API development velocity as we strive for public API feature parity.

On that topic, a lot of our tooling and processes around APIs are still more manual – multiple manual steps, reviews and synchronous activities are required to get an API exposed externally. That’s what a developer would mention as the primary opportunity. For my team, bringing automation to many different concerns, across such a large surface area of APIs is definitely a huge opportunity.

Another topic we are navigating is better defining the responsibilities of platform teams like mine vs. the domain teams that build the API business logic. Today it can be fuzzy, though for the most part ownership is clear. Who is responsible for latency, performance monitoring and load testing? How do we help domain teams even be aware of performance concerns and become more sensitive to it? Customer delight is our primary goal and we drive towards this relentlessly.

**_How about a developer integrating with Flexport APIs? What are their main challenges?_**

We frequently receive requests for enhancements to our API and developer platform experience, and due to where we are in our API journey, we get a lot of requests from customers to expose more functionality via our public API. As I said before, our APIs aren’t at parity with our web app interfaces and internal APIs yet. So that's definitely the most common request, to expose more internal functionality via our public APIs; to accomplish this we need to work broadly across our Tech org.

## API Architecture

**_If someone was designing their API architecture today, what advice would you give them? How would you approach using REST or GraphQL?_**

Yes, this is a good question, and one I've been asked ever since I joined Flexport and started managing APIs. What I would say is that there is no right answer, you need to build with your customer in mind to delight the customer and deliver high-value business impact. At Flexport we are working in freight forwarding; we're the leading technology disruptor in the supply chain logistics space. While the sector is digitizing, the rate of B2B digitization may be slower than some B2C industries, say Finance & Fintech.

Many of our partners in the space have been operating for much longer than Flexport. We do not have customers asking us for public GraphQL right now. That may happen in the future, and if there was a compelling customer use case we would consider it, though for our current customers and partners, REST endpoints and webhooks satisfy their requirements. If you’re in a space that is catering to developers who are asking for it, GraphQL might be worth considering. At my previous company we had some GraphQL public APIs though the customer demand was overwhelmingly for REST.

## Closing

**_A question we like to ask everyone: any new technologies or tools that you’re particularly excited by?_**

I'm curious about AR and VR. We have held a couple of hackathons and for one of those hackathons, I built a VR treasure hunting game. Flexport is a mix of people; some of us are from the logistics, supply chain and freight industry, while others have not actually worked in this domain. There are people at Flexport who have never had the opportunity to visit a working port, or been on a container ship. So I built a little VR game in 2 days so that people could visually explore those different locations. In the game, you are on the hunt for personal protective equipment (PPE) aboard container ships, since during that hackathon, we were at the beginning of the COVID-19 pandemic and Flexport via [**Flexport.org**](https://www.flexport.org/) was making a big push to [**ship PPE to frontline responders**](https://www.flexport.com/blog/the-drive-to-mobilize-ppe-flexport-raises-usd7-9m-to-move-medical-supplies/) where we raised over $8 million. You can play the game at [**eesee.github.io/justflexit**](https://eesee.github.io/justflexit/)


 This is the content for the doc blog/apis-vs-sdks-difference/index.mdx 

 ---
title: "APIs vs. SDKs: Key Differences, Use Cases, and Best Practices"
description: "Explore the core differences between APIs and SDKs, learn real-world use cases, and discover best practices for seamless integration and faster development."
date: 2025-01-13
image: "/media/api-vs-sdks.png"
authors:
  - name: Emre Tezisci
  - image_url: "/media/author-headshots/emre.jpeg"
tags:
  - API Advice
featured_image: "/media/api-vs-sdks.png"
---
# APIs vs. SDKs: Understanding the Differences and Practical Applications

In the interconnected world of modern software, APIs (Application Programming Interfaces) and SDKs (Software Development Kits) are indispensable tools. APIs act as the bridges that allow different applications to communicate and share data, while SDKs provide developers with the toolkits they need to build upon these APIs efficiently. Choosing whether to use an API directly or leverage an SDK is a crucial decision that can significantly impact a project's timeline and overall success. This guide will clarify the distinctions between APIs and SDKs, explore their common use cases, and outline best practices for both.

---

## Quick-Reference Summary

Here’s a brief table that highlights the fundamental differences between APIs and SDKs at a glance:

| **Aspect**               | **API (Application Programming Interface)**                                       | **SDK (Software Development Kit)**                                         |
|--------------------------|-----------------------------------------------------------------------------------|----------------------------------------------------------------------------|
| **Definition**           | A set of rules and protocols for communication between software components        | A bundle of tools, libraries, and documentation to accelerate development  |
| **Scope**                | Focuses on how to send and receive data (often via HTTP/HTTPS)                    | Provides prebuilt code, testing frameworks, and platform-specific support  |
| **Implementation Detail**| Requires developers to handle requests, responses, and error handling manually    | Abstracts complexities with prewritten methods and classes                 |
| **Platform Dependency**  | Typically platform- and language-agnostic (REST, GraphQL, gRPC, etc.)             | Often tied to a specific language or ecosystem (Android SDK, iOS SDK, etc.)|
| **Use Case**             | Ideal for lightweight integration, direct control, or cross-platform scenarios    | Best for rapid development, built-in best practices, and platform-specific features |

---

## What Are APIs?

An **Application Programming Interface (API)** is a set of rules, protocols, and definitions that enable different software components to communicate. It acts as a “contract,” specifying how requests and data exchanges occur between systems, such as a client application and a remote server.

APIs serve as fundamental building blocks in modern software. They allow developers to leverage sophisticated services (e.g., payment gateways, location services) without building them from scratch. Internally, APIs make it easier for teams to create modular, scalable applications by standardizing communication between different components and services.

### Popular API Approaches

- **REST (Representational State Transfer):** REST is the most widely used approach for creating APIs, primarily due to its simplicity and compatibility with HTTP. It dictates structured access to resources via well-known CRUD (Create/Read/Update/Delete) patterns. A common pattern in modern web development is to create a front-end written in React or a similar framework, which fetches data from and communicates with a back-end server via a REST API.
- **GraphQL:** GraphQL is a newer API technology that enables API consumers to request only the data they need. This reduces bandwidth required and improves performance, and is particularly suitable in situations where a REST API returns large amounts of unnecessary data. However, GraphQL is more complex to implement and maintain, and users need to have a deeper understanding of the underlying data models and relationships in order to construct the right queries.
- **gRPC (Google Remote Procedure Call):** gRPC is a high-performance, open-source framework designed for low-latency and highly-scalable communication between microservices. gRPC is strongly-typed, which helps catch errors earlier in the development process and improves reliability. However, gRPC ideally requires support for HTTP/2 and protocol buffers, which many web and mobile clients may not support natively. Also note that far fewer developers are familiar with gRPC than REST, which can limit adoption. For these reasons, gRPC is mainly used for internal microservice communications.

In summary, REST remains the most popular API technology due to its simplicity and widespread adoption. GraphQL and gRPC are popular for specific use cases.

---

## What Are SDKs?

A **Software Development Kit (SDK)** is a comprehensive collection of tools, libraries, documentation, and code samples that streamline application development on a specific platform or for a specific service. While an API defines how to interact with a service, an SDK provides ready-made resources to speed up that interaction.

Key components of SDKs include:
- **Pre-Written Libraries**: Reduce boilerplate by offering out-of-the-box methods and classes  
- **Development Utilities**: Provide testing frameworks and debugging tools  
- **Platform-Specific Resources**: Include documentation, guides, and environment setup instructions

For example, the **Android SDK** includes compilers, emulators, libraries, and tutorials, allowing developers to build Android apps with minimal friction.

---

### Why Do SDKs Add Value to API Integrations?

Without an SDK, you must manually handle HTTP requests, parse responses, implement error handling, manage authentication, and maintain the correct sequence of API calls. SDKs solve many of these pain points by:

- **Development Efficiency**: Simplify method calls (e.g., `client.placeOrder(...)` instead of manually constructing endpoints and payloads).  
- **Type Safety & Consistency**: Strongly-typed interfaces reduce integration errors.  
- **Maintenance Benefits**: Common patterns and best practices are baked into the libraries.  
- **Change Management**: Many SDKs transparently handle minor API updates under the hood.

---

## How Do APIs Compare With SDKs in Practice?

### Example: Direct API Integration

To highlight these differences, let’s look at an example of what integrating with an e-commerce API might look like, first without an SDK and then with one. The use case will be enabling a new customer to place an order. This requires fetching information about the product being ordered, creating a new customer, and creating the order itself.

**First, here’s what integrating might look like without an SDK:**

```typescript
const fetch = require('node-fetch');

const apiKey = 'your_api_key';
const baseUrl = 'https://api.ecommerce.com/v1';
const headers = {
  'Authorization': `Bearer ${apiKey}`,
  'Content-Type': 'application/json'
};

const productName = 'Awesome Widget';
const customer = {
  firstName: 'John',
  lastName: 'Doe',
  email: 'john.doe@example.com'
};
const quantity = 2;

async function placeOrder(productName, customer, quantity) {
  try {
    // Step 1: Get product information
    const productResponse = await fetch(`${baseUrl}/products`, { headers });

    if (productResponse.status !== 200) {
      throw new Error(`Could not fetch products. Status code: ${productResponse.status}`);
    }

    const productData = await productResponse.json();
    const product = productData.products.find(p => p.name === productName);

    if (!product) {
      throw new Error(`Product '${productName}' not found.`);
    }

    // Step 2: Create a new customer
    const customerResponse = await fetch(`${baseUrl}/customers`, {
      method: 'POST',
      headers,
      body: JSON.stringify({ customer })
    });

    if (customerResponse.status !== 201) {
      throw new Error(`Could not create customer. Status code: ${customerResponse.status}`);
    }

    const customerData = await customerResponse.json();
    const customerId = customerData.customer.id;

    // Step 3: Place the order
    const orderResponse = await fetch(`${baseUrl}/orders`, {
      method: 'POST',
      headers,
      body: JSON.stringify({
        order: {
          customerId,
          items: [
            {
              productId: product.id,
              quantity
            }
          ]
        }
      })
    });

    if (orderResponse.status !== 201) {
      throw new Error(`Could not place order. Status code: ${orderResponse.status}`);
    }

    console.log('Order placed successfully!');
  } catch (error) {
    console.error(`Error: ${error.message}`);
  }
}

placeOrder(productName, customer, quantity);
```

Note that the API consumer would need to construct all this code themself. They would need to refer to the API documentation to figure out which APIs should be called, what the response data structures look like, which data needs to be extracted, how to handle auth, what error cases might arise and how to handle them.

**What You Manage Manually:**
- Constructing requests and headers  
- Parsing responses  
- Handling errors for each call  
- Managing authentication  
- Sequencing the calls to ensure proper workflow

**Now here’s the SDK version of this code. Using an SDK, the same functionality can be achieved with much greater ease:**

```typescript
const { EcommerceClient } = require('ecommerce-sdk');

const apiKey = 'your_api_key';
const client = new EcommerceClient(apiKey);

const productName = 'Awesome Widget';
const customer = {
  firstName: 'John',
  lastName: 'Doe',
  email: 'john.doe@example.com'
};
const quantity = 2;

async function placeOrder(productName, customer, quantity) {
  try {
    await client.placeOrder(productName, customer, quantity);
    console.log('Order placed successfully!');
  } catch (error) {
    console.error(`Error: ${error.message}`);
  }
}

placeOrder(productName, customer, quantity);
```

Notice how much simpler and concise it is. Authentication is handled automatically with the developer just needing to copy in their key. Pre-built functions mean the developer doesn’t need to parse through pages of API docs to stitch together the required calls and associated data extraction themselves. Error handling and retries are built-in.

Overall, a far easier and superior experience.

**Advantages of Using an SDK:**
- **Dramatically Reduced Code Complexity**: Fewer lines of code and clearer logic flow  
- **Automatic Authentication and Error Handling**: The SDK’s internal routines handle retries, rate limits, and token refreshes  
- **Built-in Best Practices**: Consistent data structures and naming conventions  
- **Faster Onboarding**: Less time spent referencing raw API docs

---

## What’s the difference between SDKs and APIs?
APIs and SDKs serve distinct yet complementary roles in software development. **APIs** provide the underlying communication protocols and offer broad flexibility, while **SDKs** wrap these protocols with ready-to-use libraries and best practices that make development faster and more consistent. In summary, APIs & SDKs are symbiotic. Let’s talk about coffee to draw the analogy better.

You can think of APIs as the fundamental, bare metal interfaces that enable applications or services to communicate. In our analogous example, APIs are like going to a coffee shop and getting a bag of beans, a grinder, a scale, filter paper, a coffemaker/brewer, kettle, and an instruction guide. Good luck making a delicious brew!

SDKs on the other hand are critical to enabling APIs to reach their full potential, by providing a rapid, ergonomic way to access the API’s underlying functionality. In our coffee example, SDKs are more akin to telling a skilled barista “I’d like a latte please”. The barista does all of the work of assembling the ingredients, and you get to focus on the end result.

## API and SDK best practices

Now we know what APIs and SDKs do, what should you keep in mind as you’re building them, to ensure they fulfill the promises we’ve outlined above?

Here are some “gotchas!” to watch out for when building awesome APIs:

- **Design carefully:** It can be extremely difficult to get users to change how they use an API once it’s in production. Avoiding unnecessary breaking changes, where possible, will save you many headaches and irate users later.
- **Documentation:** In addition to an “API reference” that details every endpoint and response, consider creating a “usage guide” that walks users through how to use APIs in sequence to accomplish certain tasks.
- **Authentication:** Creating and sending users API keys manually works fine for an MVP, but has obvious security and scalability challenges. An ideal solution is to offer a self-service experience where end-users can generate and revoke keys themselves. For more on API auth, [check out our guide](/post/api-auth-guide).
- **Troubleshooting and support:** Users will inevitably run into issues. It’s easy for members of the team to quickly get inundated with support requests. Try to provide self-service tools for troubleshooting API issues, such as logging and monitoring, and community support channels.

Building great SDKs presents a different set of considerations. Keep these in mind if you want to offer a great SDK to your users:

- **How stable is the underlying API?** If the API is undergoing frequent changes, it might be particularly challenging to manually keep the SDKs up-to-date and in sync with the API.
- **Creation and maintenance cost:** Creating native language SDKs for all your customers’ preferred languages can be a huge hiring and skills challenge. Each language SDK also has to be updated every time the API changes – ideally in lockstep to avoid the SDK and API being out of sync. This is time-consuming and costly. Many companies have deprecated or scaled back their SDKs after misjudging the work required.
- **Testing and validation:** Plan for thorough testing of the SDKs across different platforms and languages, including unit tests, integration tests, and end-to-end tests, to ensure the SDKs are reliable and compatible with the API.
- **Documentation:** Provide clear examples and code snippets in each language to make the SDKs easy to use and understand.
---

## Simplify SDK Generation with Speakeasy

While the benefits of providing SDKs are clear, creating and maintaining them across multiple languages can be a significant undertaking. It requires specialized skills, substantial development time, and ongoing effort to keep SDKs in sync with API changes. This is where Speakeasy comes in.

Speakeasy is a platform that **automatically generates high-quality, idiomatic SDKs** from your API specification. Our solution helps you:

*   **Reduce Development Time and Costs:** Eliminate the need to manually write and maintain SDKs. Speakeasy handles the heavy lifting, freeing up your team to focus on core product development.
*   **Ensure SDK Quality and Consistency:** Our generated SDKs are built to follow industry best practices. Speakeasy offers comprehensive, automated testing, ensuring reliability and a consistent developer experience across all supported languages. Each generated SDK comes with:
    *   **Comprehensive Test Coverage:** We provide a wide range of tests, including unit, integration, and end-to-end tests, to validate every aspect of the SDK's functionality.
    *   **Automated Test Execution:** Our platform automatically runs these tests whenever your API specification changes, providing immediate feedback on any potential issues.
*   **Keep SDKs in Sync with API Changes:** Speakeasy automatically regenerates your SDKs whenever your API specification is updated, guaranteeing that your SDKs are always up-to-date.
*   **Improve Developer Experience:** Provide developers with easy-to-use, well-documented SDKs that accelerate integration and enhance their overall experience. Each generated SDK comes with extensive, ready-to-publish documentation:
    *   **Interactive Code Examples:** Developers can see real code examples in their preferred language, making it easier to get started.
    *   **Clear and Concise Explanations:** Our documentation is designed to be easy to understand, even for complex API interactions.
    *   **Automatically Updated:** Documentation is regenerated alongside the SDKs, ensuring consistency and accuracy.
*   **API Insights:** Speakeasy provides detailed insights into your API's usage and performance. Our platform helps you track key metrics, identify areas for improvement, and ensure the reliability of your API.

**How it Works:**

1.  **Provide Your API Specification:** Share your OpenAPI or other supported API specification with Speakeasy.
2.  **Configure Your SDKs:** Select the languages you want to support and customize the look and feel of your SDKs, including configuring authentication methods.
3.  **Generate and Publish:** Speakeasy automatically generates your SDKs, runs comprehensive tests, creates detailed documentation, and makes them available for download or through package managers.

Stop spending valuable time and resources on manual SDK development. Let Speakeasy handle the complexities of SDK generation, testing, and documentation so you can focus on building great APIs and delivering exceptional developer experiences. [Learn more about Speakeasy's SDK generation platform](https://www.speakeasy.com/docs/introduction).

**Get started today, [book a demo with us](https://www.speakeasy.com/book-demo).**


 This is the content for the doc blog/auth-for-embedded-react-components/index.mdx 

 ---
title: "How to Handle Auth for Embedded React Components"
description: "Learn how to handle Auth for products that are being delivered as react embeds."
image: "/media/auth-for-embedded-react-components.png"
date: 2022-10-20
authors:
  - name: Nolan Sullivan
  - image_url: https://uploads-ssl.webflow.com/62ccd7b208cab0723d026273/62cdfe8c07de4e15f4df107d_5-alexa-headshot.jpg
tags:
  - Building Speakeasy
featured_image: "/media/auth-for-embedded-react-components.png"
---

## Use case & requirements

More and more developer products are being delivered as react embeds. We recently built out our infrastructure to support delivery of Speakeasy as a series of embedded react components. Why embedded components? The embedded components enable our customers to include Speakeasy functionality in their existing developer portals, surfacing their API request data to their users.

Because our embeds are exposing API data to our customers’ users, authentication is a critically important feature to get right. In this blog post I will walk through how we decided to implement authentication for our embedded react components, in the hope it helps someone else building react embeds that handle data.

The elevator-pitch for this feature is: our users should be able to safely embed _our_ widgets into _their_ web applications. The list of requirements for this feature looks something like this:

1. **Data Segmentation:** Because our customers’ web applications may be user-facing, our customers need to be able to dictate _what_ portion of the data gets rendered into the components.
2. **Manageable:** Customers need to be able to restrict access to someone who was previously authenticated.
3. **Read Only:** Embeds might be exposed to people who shouldn’t be able to modify the existing state (upload an OpenAPI schema or change API labels, for example), so they should have restricted permissions.

The **TL;DR** is that we evaluated two authentication options: API Keys, and Access Tokens. We ended up going with access tokens. Although they were more work to implement we felt they provided superior flexibility. Read on for the details of the discussion.

## How to Solve - Option 1: API Key?

At the point that we were initially discussing this feature, we already had the concept of an api-key, which permits programmatic access to our API. An obvious first question is whether or not we could re-use that functionality to authenticate users of the embedded resources. Let’s imagine what that might look like in the use case of our react embeds.

![Speakeasy react embeds example.](./assets/auth-for-embedded-react-components-image-01.png)

That was easy! But does it meet our requirements?

1. **Data Segmentation:** Maybe, but it would take some work. We’d need to have some means of relating an api-key with a customer, but that’s not too bad.
2. **Manageable:** Again, maybe. What happens when an api key needs to be revoked: unless a separate api key is produced for each user (imagine that one customer has multiple people using the embedded component), everyone using the newly-revoked token loses access. If a separate api key _is_ produced for each user, then the api-key becomes state that needs to be held, somewhere, and since Speakeasy isn’t a CRM, that’s now a table in _your_ database that _you_ need to manage.
3. **Read Only:** Not yet. We’d need to add some functionality to the api keys to distinguish between read-only and write-capable api keys.

### How does this live up to our requirements?

Could work, but would require some work to meet the requirements. Maybe we can do better.

## How to Solve - Option 2: Access Token

What would a separate “embed access token” implementation look like? Let’s take a look at what tools we’ve already built that we might take advantage of.

- We know that our sdk is in your API
- We know we have the concept of a customer ID
- We also know that you know how we’re identifying your customers (you assigned the customerID to each request)
- We know that you probably already have an auth solution that works for you.

From the looks of it, we don’t really need to understand who your users are or how you assign customerIDs to requests, and chances are, you don’t want another service in which to manage your customers, because you probably have that logic somewhere already. Since the authentication logic _for_ your API is probably reliably available _from within_ your API, we just need to make it easy for you to give that authentication result to us.

This is where we can take advantage of the Speakeasy server-side sdk that is already integrated into your API code, because it’s authenticated with the Speakeasy API by means of the api key (which is workspace specific).

```typescript
// Endpoint in api server with speakeasy sdk integration
controller.get("/embed-auth", (req, res)  => {
    const authToken = req.headers["x-auth"];
    const user = userService.getByAuthHeader(authToken);

    const filters = [{
      key: "customer_id",
      operator: "=",
      value: user.companyId
    }];

    const accessToken = speakeasy.getEmbedAccessToken(filters)
    res.end(JSON.stringify({ access_token: accessToken}));
});
```

That takes care of understanding _how_ to segment the data, but how do we actually make that work? There are myriad reasons that you might want to control the scope of data exposed to your customers. We already have a filtering system for the Speakeasy Request Viewer. If we build the access token as a JWT, we can bake those filters _into_ the JWT so that they cannot be modified by the end-user, and we can coalesce the filters from the JWT and the URL, maintaining the existing functionality of the Speakeasy Request Viewer filtering system.

Putting this all together, the resulting flow looks like:

1. You authenticate the user
2. You translate the result of authentication into a series of filters
3. You pass the filters into some method exposed by the Speakeasy sdk
4. We encode those filters into a JWT on our server
5. You return that JWT to your React application.

### How does this live up to our requirements?

1. **Data Segmentation:** Yeah, beyond even just by customer ID.
2. **Manageable:** JWTs are intentionally ephemeral. We already have logic to refresh JWTs for our Saas platform that we can re-use for the embed access tokens.
3. **Read Only:** This can be implemented with an additional JWT claim alongside the filters.

### Loose ends

Requiring a new endpoint in your API to get the embedded components (option 2) working _is_ more work (potentially split over teams) than api-key based authentication. The consequence is that the endpoint has to be deployed _before the embedded component can even be tested._ To ameliorate this disadvantage, we added a **Create Access Token** button directly in our webapp, which generates a short-lived access token that can be hard-coded in the react code for debugging, testing, or previewing.

![Create embed tokens direclty in the webapp.](./assets/auth-for-embedded-react-components-image-02.png)

## Final Conclusion

Access Tokens take a little more setup, but it’s considerably more flexible than the original api key idea. It also works with whatever authentication flow you already have, whether it’s through a cloud provider or your hand-rolled auth service. Additionally, because JWTs are ephemeral and can’t be modified, this solution is more secure than the api-key method, which would require manual work to revoke, whereas the moment that a user can’t authenticate using your existing authentication, they can no longer authenticate with a Speakeasy embedded component.


 This is the content for the doc blog/build-terraform-providers/index.mdx 

 ---
title: "Building a SaaS API? Don't Forget Your Terraform Provider"
description: "Speakeasy is ready to help SaaS platforms reach Terraform users"
keywords: [terraform, terraform provider, api, openapi, swagger, sdk generation, go, golang, go sdk, golang sdk, developer experience, devex, dx]
image: "/media/build-terraform-providers-2.png" 
date: 2023-07-04
authors:
  - name: Thomas Rooney
  - image_url: "/media/author-headshots/thomas.jpeg"
tags:
  - Terraform
featured_image: "/media/build-terraform-providers-2.png"
---

# Building a SaaS API? Don't Forget Your Terraform Provider

Replacing custom integrations with Terraform cuts down on errors, simplifies infrastructure management, makes infrastructure changes easier to version, and saves developers hours of repeatedly clicking around hundreds of different dashboards.

Most users are familiar with Terraform DevOps functions: Launching web servers, managing databases, and updating DNS records on AWS, GCP, and Azure.

But Terraform's adoption is growing far beyond infrastructure and it's time we free this sleeping giant from its reputation as a cost center. Instead, we see Terraform as a strategic ally and driver of revenue for most SaaS platforms through the offering of a provider.

Terraform with its provider plugin ecosystem can serve as a robust interface between your SaaS API and your users, allowing for a more integrated, efficient, and scalable developer experience. It's not just about deploying servers and managing databases anymore; it's about creating a unified and streamlined workflow for your users, reducing complexities, and unlocking new use cases.

Terraform already helps your users solve their infrastructure problems:

| Infrastructure Problem | Terraform Solution |
|---|---|
| Inconsistent environments | Promotes consistency across dev, staging, and production environments. |
| Managing multicloud infrastructure | Provides a unified framework for managing GCP, AWS, Azure, and more. |
| Scalability challenges | Automates provisioning and management of resources, aiding scalability. |
| Wasted cloud resources | Includes modules to estimate infrastructure cost and scale down unused resources. |
| Resource orchestration | Automatically handles dependencies between infrastructure resources. |
| Auditing and versioning of infrastructure | Enables version control and auditing of infrastructure. |

Speakeasy makes it straightforward to create API surface areas that provide exceptional developer experience to your users. That includes SDKs in 7+ popular languages, but it also means [Terraform providers](/docs/create-terraform). We help you meet your users where they already are - the Terraform registry.

But first, let's start with some background.

## What is Terraform?

Terraform is an open-source infrastructure-as-code (IaC) tool developed by HashiCorp. IaC is essential in modern DevOps practices as it allows for consistent and repeatable deployments, minimizing the risks and tedium of manual configurations.

By describing their desired infrastructure in a declarative language called HCL (HashiCorp Configuration Language), teams can version, share, and apply infrastructure definitions using Terraform.

The most basic building blocks of Terraform configurations are Terraform providers.

## What Is a Terraform Provider?

A Terraform provider is a plugin that allows Terraform to manage a given category of resources. Providers usually correspond to specific platforms or services, such as AWS, Azure, GCP, or GitHub. Each provider defines and manages a set of resource types—for example, an AWS provider might handle resources like an AWS EC2 instance or an S3 bucket.

When you're writing your Terraform configuration files, you define what resources you want and which provider should manage those resources. The provider is then responsible for understanding API interactions with the given service and exposing resources for use in your Terraform scripts.

In practical terms, this means that providers translate the HCL code that users write into API calls to create, read, update, delete, and otherwise manage resources on these platforms. By using providers, Terraform users can manage a wide variety of service types.

The most widely used Terraform plugin registry is the [HashiCorp Terraform registry](https://registry.terraform.io/). Launched in 2017, the registry [has now surpassed 3,000 published providers](https://www.hashicorp.com/blog/hashicorp-terraform-ecosystem-passes-3-000-providers-with-over-250-partners).

## Terraform Beyond Infrastructure

While Terraform's primary function remains infrastructure management, its use cases extend beyond the traditional scope. In addition to managing servers, databases, and networks, Terraform can be used to manage higher-level services and applications, including SaaS products.

Let's look at a few examples of Terraform providers for SaaS platforms:

### LaunchDarkly Terraform Provider

[LaunchDarkly](https://launchdarkly.com/) is a continuous delivery platform that enables developers to manage feature flags and control which users have access to new features. For each new feature your team builds, you add a feature flag and gradually release the feature to your users based on certain criteria. Now imagine a situation where you want to test a feature flag in development, staging, QA, and then finally release it in production. Your team would need to log in to the LaunchDarkly dashboard each time to manage these flags across the different environments, which can be a time-consuming process.

The [LaunchDarkly Terraform provider](https://registry.terraform.io/providers/launchdarkly/launchdarkly/latest/docs) allows developers to automate the process of creating, updating, and deleting feature flags across different environments, reducing manual effort, minimizing human error, and increasing efficiency in their workflows. It's a clear win for developer productivity and reliability of the deployment process.

### Checkly Terraform Provider

[Checkly](https://www.checklyhq.com/) enables developers to monitor their websites and APIs, offering active monitoring, E2E testing, and performance metrics. It provides essential insights into uptime, response time, and the correctness of your web services.

Imagine a situation where your organization has multiple services, each with different endpoints. Managing these services and ensuring they all maintain a high level of performance can be a daunting task. You may also require your developers to use Checkly headless browser testing or screenshot features in development, staging, and other environments. You'd need to log in to the Checkly dashboard each time to set up or adjust monitoring configurations, a process that could become tedious and time-consuming, especially in large-scale environments.

The [Checkly Terraform provider](https://registry.terraform.io/providers/checkly/checkly/latest/docs) simplifies this process by allowing developers to automate the configuration and management of checks and alerts. Instead of manually configuring each check via the Checkly dashboard, developers can define them directly in their Terraform configurations. This means checks can be versioned, shared, and managed just like any other piece of infrastructure.

### Other SaaS Terraform Providers

-  [PagerDuty](https://registry.terraform.io/providers/PagerDuty/pagerduty/latest/docs): Developers benefit from the PagerDuty Terraform provider by being able to automate the set up and management of incident response procedures, providing a consistent, repeatable, and efficient way to manage complex operational environments.
-  [Salesforce](https://registry.terraform.io/providers/hashicorp/salesforce/latest/docs): The Salesforce Terraform provider allows administrators to programmatically manage Salesforce users. This provider is released by Hashicorp and may have been released to scratch their own itch while administering Salesforce, but it is a clear indication that Terraform has at least some usage outside of development teams.
-  [Fivetran](https://registry.terraform.io/providers/fivetran/fivetran/latest/docs): With the Fivetran Terraform provider, users can automate the set up and management of data connectors, allowing for easy and efficient integration of various data sources with their data warehouse in a version-controlled manner.
-  [GitHub](https://registry.terraform.io/providers/integrations/github/latest/docs): GitHub users benefit from its Terraform provider by being able to automate repository management, actions, teams, and more, streamlining workflows and improving efficiency across their development teams.

Admittedly, the lines between SaaS and infrastructure are blurred for some of these, but the examples above are only a fraction of what's available on the registry.

## Why Users Choose Terraform Over API Integration

All the SaaS platforms in our examples above have full-featured and well-documented APIs with accompanying SDKs. Chances are, if you're not using Terraform in your organization yet, you may come across some utility code in your repositories performing regular calls to one of these ubiquitous platforms' APIs.

We touched on a couple of examples where Terraform enables teams to configure services for use with different environments in the development lifecycle. This unlocks new use cases, and may even increase many users' spending after automation negates the labor cost of configuring ephemeral environments.

Furthermore, it seems the availability of good Terraform providers is already starting to factor into SaaS purchasing decisions.

Organizations that are audited as part of certification often have strict disaster recovery requirements. Terraform providers with full API coverage could enable these organizations to recover their SaaS configurations without any manual work.

This benefit also applies when organizations are required to keep track of configuration updates. If a SaaS platform does not have strict auditing built in, a client could use Terraform with version control, thereby creating an audit trail.

## Which SaaS Products Are Good Candidates for Terraform?

With the benefits we mentioned in mind, we can try to identify traits that would make a SaaS platform a suitable candidate for Terraform integration.

-  **Configurable resources:** SaaS platforms that offer configurable resources like users, roles, policies, projects, or servers are good candidates. If a resource's configuration can be described in code, it can be managed by Terraform.
-  **Multiple environments:** Platforms that need to maintain consistent configurations across multiple environments (like dev, test, staging, and production) are well-suited for Terraform, as it simplifies the creation and management of these environments.
-  **Frequent changes:** If your SaaS product requires frequent changes to its configuration, it may benefit from a Terraform provider. With its plan/apply cycle, Terraform allows users to preview changes before applying them, reducing the risk of unintentional modifications.
-  **Scaling needs:** SaaS platforms that need to manage resources at scale can also benefit from Terraform. It allows for managing many resources consistently and efficiently, reducing the risk of manual errors and inconsistencies.
-  **Automatable tasks:** If your platform's tasks can be automated via API calls, then a Terraform provider would be a good fit. Terraform excels in automating infrastructure management tasks and reduces the need for manual intervention.
-  **Security and compliance needs:** For SaaS platforms that need to enforce certain security configurations or compliance standards, Terraform can ensure these requirements are consistently met across all resources.
-  **Integrations with other cloud services:** If your SaaS platform frequently integrates with other cloud services, having a Terraform provider can make these integrations easier and more efficient by leveraging Terraform's extensive provider ecosystem.

Even if a SaaS platform doesn't check all these boxes, there might still be significant value in offering a Terraform provider, especially as Terraform's adoption is growing rapidly.

## What About Internal Tools and Interfaces?

Terraform providers are especially useful for building internal tools, where they provide your internal developers an AWS-like experience while managing internal resources.

By offering a Terraform provider to internal users, organizations can provide a consistent, standardized interface for managing internal resources. This standardization reduces the learning curve and complexity for developers, as they can use the same Terraform commands and concepts they're already familiar with from managing public cloud resources.

The benefits we mentioned earlier also apply to internal tools: Better collaboration, version control, reduced errors, and less time configuring services using manual interfaces.

## Why SaaS Companies Don't Publish Terraform Providers

Clearly, publishing a Terraform provider could benefit most SaaS providers, so why don't we see more companies maintaining Terraform providers for their APIs?

We won't mince words here: Publishing a Terraform provider is difficult.

Telling SaaS development teams to _“just publish a Terraform provider”_ would be misguided at best.

![If only it was as easy as drawing an owl](./assets/owl.jpg)

Developing and maintaining a Terraform provider requires a significant investment in terms of time, resources, and expertise. You need an in-depth understanding of the SaaS platform's API, the Terraform ecosystem, and the Go programming language.

Add to this the fact that many APIs change significantly over time. If an API changes frequently, it can require a significant effort to keep its Terraform provider up to date.

Even if creating and maintaining a provider is within a SaaS company's abilities, there might be hesitance to take on an additional support commitment. We understand that it could feel like you're adding another layer to an already complex problem and users will expect some manual help. We argue that the simplicity of HCL lends itself to much easier support engineering, as declarative configuration is simpler to lint automatically, read, debug, and rewrite.

Terraform is well suited for self-help users, as the documentation for Terraform providers is standardized and hosted by the registry. Nevertheless, some platforms such as LaunchDarkly choose to support Terraform integration only for users on pro or enterprise pricing tiers—presumably to offset anticipated support cost.

## Speakeasy Generates and Maintains Terraform Providers

With Speakeasy, you can generate a Terraform provider based on your OpenAPI spec. This means you don't need to be a Terraform expert or write any custom Go code. Speakeasy also makes sure your provider stays updated with your API by pushing a new branch to your provider's repository when your API spec changes.

To generate a Terraform provider, you map OpenAPI objects and operations to Terraform entities and actions by annotating your OpenAPI specification.

![Workflow diagram showing Speakeasy's Terraform generation](./assets/tf-workflow.png)

To get started, you can follow our [documentation on annotating your spec for the Terraform provider](/docs/create-terraform/extensions) generator.

After generating a provider, updating the provider becomes as straightforward as merging a PR from the update branch Speakeasy creates.

## Case Study: Airbyte Terraform Provider

On 22 June, [Airbyte launched their Terraform provider](https://airbyte.com/blog/terraform-provider-launched-for-airbyte-cloud) for Airbyte cloud users. The Airbyte [Terraform provider](https://registry.terraform.io/providers/airbytehq/airbyte/latest/docs) was generated by Speakeasy, based entirely on Airbyte's OpenAPI specification.

This release came after months of collaboration between Airbyte and Speakeasy, and we are delighted to have played a role in Airbyte's continued success.

> We looked for the best options in API tooling, so we didn’t have to build everything ourselves. We focus on what we do best: ensuring data is accessible everywhere it has value. For our API needs; we have Speakeasy.
>
> -- <cite>Riley Brook, Product @ Airbyte</cite>

Under the hood, the Airbyte Terraform provider uses a Go SDK generated by Speakeasy. Since the [generated provider is open source](https://github.com/airbytehq/terraform-provider-airbyte), we can take a look at what's in the repository.

We ran [Sloc Cloc and Code (scc)](https://github.com/boyter/scc/blob/master/README.md) on the repository and this is what we found:

```bash
$ scc terraform-provider-airbyte
```

| Language  | Files | Lines  | Blank | Comment | Code   | Complexity | Bytes    |
|-----------|------:|-------:|------:|--------:|-------:|-----------:|---------:|
| Go        | 3024  | 267623 | 35706 | 10727   | 221190 | 37484      | 9886221  |
| Markdown  | 240   | 17985  | 6760  | 0       | 11225  | 0          | 802305   |
| YAML      | 5     | 65577  | 8     | 24      | 65545  | 0          | 2170810  |
| JSON      | 1     | 6      | 0     | 0       | 6      | 0          | 83       |
| Makefile  | 1     | 9      | 3     | 0       | 6      | 0          | 138      |
| Terraform | 1     | 85     | 10    | 0       | 75     | 2          | 2106     |
| gitignore | 1     | 4      | 0     | 0       | 4      | 0          | 44       |
| Total     | 3273  | 351289 | 42487 | 10751   | 298051 | 37486      | 12861707 |

* Estimated Cost to Develop (organic) $10,705,339
* Estimated Schedule Effort (organic) 33.86 months
* Estimated People Required (organic) 28.09

Airbyte connects with more than 250 data sources and destinations, all with unique configuration parameters, which adds to this enormous Terraform provider.

Even if the scc estimation is off by a few orders of magnitude, it is clear that the Speakeasy Terraform provider generator saved Airbyte valuable development time and will continue to save time on maintenance in the future.

You can read more about [how Airbyte launched their SDKs and Terraform provider](/post/case-study-airbyte) on our blog.

## Summary

We believe that Terraform is poised to simplify SaaS configuration at scale and expect to see the continued growth of the Terraform registry.

However, navigating the Terraform ecosystem without deep expertise is daunting, and successfully publishing and maintaining a Terraform provider is no small undertaking.

Speakeasy is ready to help SaaS platforms expand their reach in this exciting ecosystem.

To get started with your Terraform provider, [follow our documentation](/docs/create-terraform).

[Join our Slack community](https://join.slack.com/t/speakeasy-dev/shared_invite/zt-1cwb3flxz-lS5SyZxAsF_3NOq5xc8Cjw) for expert advice on Terraform providers or get in touch to let us know how we can best help your organization.


 This is the content for the doc blog/building-php-sdks/index.mdx 

 ---
title: "How to Build an SDK in PHP"
description: "A detailed guide on how to build an SDK in PHP, including considerations for developer experience, designing resources, working with parameters, building HTTP requests, pagination, and more."
image: "/media/how-to-build-php-sdks.png"
date: 2024-05-01
authors:
  - name: Steve McDougall
  - image_url: '/media/author-headshots/steve.jpeg'
tags:
  - API Advice
featured_image: "/media/how-to-build-php-sdks.png"
---

import YouTube from 'react-youtube';

<div className="mt-10 flex justify-center items-center" >
  <YouTube
    videoId="O4xyEVSyS_s"
  />
</div>

The ability to streamline and simplify the integration process between systems is getting more and more invaluable. Everything, and I mean everything, is starting to come with its own API. If we want to implement new features, the chances are we will need to work with and external API of some description. Sometimes they offer SDKs, sometimes they don’t. The chances of them supporting your language, or framework, in the way that you need it …. Need I say more?

So learning how to build an SDK in PHP is a skillset you should definitely consider picking up. If you are building your own API and want people to use your service, you will want to provide an SDK for them to use.

In this tutorial, we are going to walk through the decisions you will take when designing an SDK in PHP:

- [What are we building](./building-php-sdks#what-are-we-building)
- [Thinking about Developer Experience](./building-php-sdks#thinking-about-developer-experience)
- [Designing our Resources](./building-php-sdks#designing-our-resources)
- [Working with Parameters](./building-php-sdks#working-with-query-parameters)
- [Building our HTTP Request](./building-php-sdks#building-our-http-request)
- [PSR-18 and sending HTTP Requests](./building-php-sdks#psr-18-and-sending-http-requests)
- [Working with Responses](./building-php-sdks#working-with-responses)
- [Pagination](./building-php-sdks#pagination)
- [Sending data through our SDK](./building-php-sdks#sending-data-through-our-sdk)
- [Summary](./building-php-sdks#summary)

There are many ways to skin a cat, I mean build an SDK. One of the first questions you need to answer is how much opinionation you want to bake into your SDK at the expense of flexibility. My approach will be unopinionated about dependencies, but more opinionated when it comes to the architecture and implementation. That’s because dependencies can be a sticking point for a lot developers, who may feel strongly about Guzzle vs. Symfony or have strict procedures in place for external dependencies. We want to ensure maximum compatibility with the PHP ecosystem. So we need to learn how to build SDKs that work no matter what. Now, let’s walk through how we might go about building an SDK.

## What are we building?

We are going to be building an SDK for a fictional e-commerce start up. The primary focus for their API is to allow their customers to sell products. Quite a common use case I am sure we can all agree.

When it comes to building an SDK, the first thing you want to think about is access. What do you want to enable access to, what resources are going to be available, and how should this work. Do we want full access? Do we want partial access, maybe read only access? This is typically tied directly to the abilities of your API.

For the SDK we are going to build, we want to be able to do the following:

- List all products, allowing the filtering and sorting of results.
- List a customers order history, and understanding the status of each order.
- Allowing customers to start creating an order, and progressing it to payment.
- Generate invoices for orders.

We won’t be handling any payment intents or actual payments in our fictional API, as there are enough payment providers out there already.

At this point we want to start thinking about the configurable options that we might have in our SDK. We can pull out the resources from the list above with relative ease. We will then need an authentication token to be passed in so that our SDK can be authorized to perform actions for us. We will also want some level of store identifier, which will indicate a specific customer’s account. How the API is set up, will depend on how the store identification works. We could use a subdomain identifier for our store, a query parameter, or a header value. It depends on how the API has been implemented. For this let’s assume we are using the subdomain approach, as it is the most common method I have seen.

## Thinking about Developer Experience

The DX is something that is important, frustrations with an SDK is the quickest way to lose the adoption you are trying to grow. Bad developer experience signals to developers that your focus isn’t on making their lives easier.

Some common things you should focus on that I find works well for developer experience are:

- Ensuring compatibility with as many implementations as possible
- Limiting third-party dependencies that could change behaviour, or break with updates
- Handling serialization effectively, nobody wants a JSON string to work with - they want objects
- Supporting pagination for paging through long result sets
- Providing programatic control over filtering query parameters,

This can tell us a lot about how to start our SDK, as we now know the parameters we need to pass to the constructor. The main thing we want to think about when it comes to our SDK, other than the core functionality, is developer experience.

So let’s start with some code, and I can walk you through the next steps:

```php
declare(strict_types=1);

namespace Acme;

final readonly class SDK
{
  public function __construct(
    private string $url,
    private string $token,
  ) {}
}
```

At this point we have an SDK class that we can use to start integrating with. Typically what I like to do is test the integration as I am building, to make sure that I am not going to be creating any pain points that I can solve early on.

```php
$sdk = new Acme\SDK(
  url: 'https://acme.some-commerce.com',
  token: 'super-secret-api-token',
);
```

This would typically be loaded in through environment variables and dependency injection, so we wouldn’t construct the SDK directly very often. However, we cannot rely on the assumptions here. In Laravel this would be declared in the following way:

```php
final class IntegrationServiceProvider extends ServiceProvider
{
  public function register(): void
  {
    $this->app->singleton(
      abstract: Acme\SDK::class,
      concrete: fn () => new Acme\SDK(
        url: config('services.commerce.url'),
        token: config('services.commerce.token'),
      ),
    );
  }
}

```

We should always test this too, I know I know, why test a constructor? Honestly, it is more of a habit right now than anything. Getting into the practice of testing your code is never a bad thing!

```php
it('can create a new sdk', function (): void {
  expect(
    new Acme\SDK(
      url: 'https://acme.some-commerce.com',
      token: 'super-secret-api-token',
    ),
  )->toBeInstanceOf(Acme\SDK::class);
});
```

As you can see here, I am using Pest PHP for testing. It’s less verbose and I think it’s actually fun to write! I find if you enjoy how you write tests, you are more likely to actually write the tests themselves.

## A Note on Authentication

In the example above you’ll notice that I am assuming that you will provide API Tokens for your users to use for their integrations. However, when it comes to APIs there are multiple options available. What is best depends on your usage patterns. You could use OAuth, HTTP Basic, API Token, or Personal Access Tokens. Each option has its benefits, depending on what you need to achieve and what you are providing.

A great example use case of something like OAuth would be if your API or service is designed to be tightly controlled. The implementation is something that you do not want to share credentials with directly, instead you want to proxy the control of this to the service you are authenticating with, which then provides an Access Token that the SDK/implementation can use on the users behalf.

Using HTTP Basic auth is something you see less and less of these days. It used to be extremely popular with government services, where you use you credentials directly to have access remotely. The core principle here is that the application doesn’t care if it is a first or third party, they should all have the same level of control and access.

That leaves API Tokens or Personal Access Tokens. This is my preferred method of authentication. You, as a user, create an API token that you want to use to gain access to the API. You can scope this to specific abilities and permissions, which then allows you to do exactly what you need nothing more. Each token is typically tied directly to a user, or entity. Using this token then also ties any actions you are trying to take directly to the entity the token belongs to. You can quickly and easily revoke these tokens, and you can cascade the deletion of the entity out to the tokens themselves. This is very similar to OAuth, but without as many hoops which makes it a great choice - at least until you actually need OAuth of course.

## Designing our Resources

From our testing above, we know the instantiation works. What’s next? Up to this point we have thought about the developer experience, and figured out how we want users to authenticate the SDK. Next we want to start defining the interface for our resources, starting with the Product resources. How I imagine this working is the following:

```php
$sdk->products()->list(); // this should return a collection of products
$sdk->products()->list('parameters'); // We should be able to filter based on parameters
```

To start building the Product resource out properly, we want to understand the potential options that we will be able to filter based on but also sorting. Personally I like enums for some of this, as it makes the most sense. Using an Enum allows you to tightly control what would be floating constants in your source code, it also gives you control over potential typos from the end user.

```php
enum Sort: string
{
  case Price = 'price';
  case Age = 'created_at';
  // other options you may want to sort on...
}
```

This would be used like the following:

```php
$sdk->products()->list(
  sort: Sort::price,
  direction: 'desc|asc',
);
```

This allows us to easily sort programmatically, giving as much control to the person implementing your SDK as possible.

## Working with Query Parameters

So, filtering. Filtering is an interesting one. There are a few different approaches that we could take here, with no clear winner. The option I personally like is passing in a list of filters to iterate over:

```php
$sdk->products()->list(
  filters: [
    Filter::make(
      key: 'brand',
      value: 'github',
    ),
  ],
);
```

This allows us to programmatically build up our request exactly as we want it. In theory this is perfect, how about in practice though? Is this going to cause frustrations?

```php
final readonly class IndexController
{
  public function __construct(
    private SDK $sdk,
    private Factory $factory,
  ) {}

  public function __invoke(Request $request): View
  {
    $products = $this->sdk->products();

    $sort = [];
    if ($request->has('sort')) {
      $sort['on'] = Sort::from(
        value: $request->string('sort')->toString(),
      );

      $sort['direction'] = $request->has('direction')
        ? $request->string('direction')->toString()
        : 'desc';
    }

    $filters = [];

    if ($request->has('filters')) {
      foreach ($request->get('filters') as $filter) {
        $filters[] = Filter::make(
          key: $filter['key'],
          value: $filter['value'],
        );
      }
    }

    try {
      $response = $products->list(
        filters: $filters,
        sort: $sort['sort'] ?? null,
        direction: $sort['direction'] ?? null,
      );
    } catch (Throwable $exception) {
      throw new ProductListException(
        message: 'Something went wrong fetching your products.',
        previous: $exception,
      );
    }

    return $this->factory->make(
      view: 'pages.products.index',
      data: [
        'products' => $response,
      ],
    );
  }
}
```

So, this isn’t perfect. We start by getting the products resource from the SDK, then process our request to programmatically change how we want to send the request to the API. Now, this is ok, but it is very long winded, which opens it up for issues and user error. We want to control things a little tighter, while still providing that flexibility. It does give me the approach I want and need to get exactly the data I want, but in a way that I personally wouldn’t want to have to use. In reality this would have been wrapped in a Service class to minimize breaking changes.

If we go with this approach, we can now start implementing the resource itself.

```php
final readonly class ProductResource
{
  public function __construct(
    private SDK $sdk,
  ) {}

  public function list(array $filters = [], null|Sort $sort = null, null|string $direction = null): Collection
  {
    // build initial request
    // build up request with filters and sorting
    // send request
    // capture response
    // throw error if response failed
    // return transformed response as a collection
  }
}
```

For now I am just commenting the steps here, because we’ll be stepping through each of these parts one by one.

## Building our HTTP Request

Building up the initial request. We want to make sure that we aren’t making any decisions for the user when it comes to their HTTP client. Luckily there is a PSR for that ([PSR-17](https://www.php-fig.org/psr/psr-17/)), which also allows us to leverage auto-discovery.

All of our resources are going to be required to create requests to send. We could either create an abstract resource, or a trait. I personally prefer composition over inheritance, so I would typically lean towards a trait here. The main benefit of composition is that we know that each resource is going to implement similar functionality - however, if we need tighter control over just one thing we can partially pull in a trait or simply not use it. Also, when it comes to testing, testing traits is a lot easier than testing abstract classes.

```php
trait CanCreateRequests
{
  public function request(Method $method, string $uri): RequestInterface
  {
    return Psr17FactoryDiscovery::findRequestFactory()->createRequest(
      method: $method->value,
      uri: "{$this->sdk->url()}/{$uri}",
    );
  }
}
```

This trait allows us to access the discovered Request Factory that implements PSR-17, to then create a request using a passed in method and url. The method here is a simple Enum that allows programatic choice of method, instead of using static variables like `GET` or `POST`.

As you can see we need to extend our base SDK class right now, to provide accessors to the private properties of `url` and later on `token`.

```php
final readonly class SDK
{
  public function __construct(
    private string $url,
    private string $token,
  ) {}

  public function url(): string
  {
    return $this->url;
  }

  public function token(): string
  {
    return $this->token;
  }
}
```

The first step is done, we can now create the request we need to so that we can build the request as required. The next step is to add the trait to the resource class, so we can implement the required logic.

```php
final readonly class ProductResource
{
  use CanCreateRequests;

  public function __construct(
    private SDK $sdk,
  ) {}

  public function list(array $filters = [], null|Sort $sort = null, null|string $direction = null): Collection
  {
    $request = $this->request(
      method: Method::GET,
      uri: '/products',
    );

    // build up request with filters and sorting
    // send request
    // capture response
    // throw error if response failed
    // return transformed response as a collection
  }
}
```

As you can see from the above, to build the request all we need to do is interact with the trait we have created. This will use the PSR-17 factory discovery to find the installed Request Factory, and create the request based on the parameters we sent through. The chances are that we will want to build up our requests in a lot of our resources, so we will need to extend our trait.

```php
trait CanCreateRequests
{
  public function request(Method $method, string $uri): RequestInterface
  {
    return Psr17FactoryDiscovery::findRequestFactory()->createRequest(
      method: $method->value,
      uri: "{$this->sdk->url()}/{$uri}",
    );
  }

  public function applyFilters(RequestInterface $request, array $filters): RequestInterface
  {
    foreach ($filters as $filter) {
      // now we need to work with the filter itself
    }
  }
}
```

But, before we work with the filters on the request we need to understand the options for the filters. They are using query parameters to build the query parameters, which is supported in PSR-7. Let’s look at the filter class, and add a method for working with the filters.

```php
final readonly class Filter
{
  public function __construct(
    private string $key,
    private mixed $value,
  ) {}

  public function toQueryParameter(): array
  {
    return [
      $this->key => $this->value,
    ];
  }

  public static function make(string $key, mixed $value): Filter
  {
    return new Filter(
      key: $key,
      value: $value,
    );
  }
}
```

We just need a way to take the content passed into the filter class, and turn it into an array that we can work with.

```php
trait CanCreateRequests
{
  public function request(Method $method, string $uri): RequestInterface
  {
    return Psr17FactoryDiscovery::findRequestFactory()->createRequest(
      method: $method->value,
      uri: "{$this->sdk->url()}/{$uri}",
    );
  }

  public function applyFilters(RequestInterface $request, array $filters): RequestInterface
  {
    $parameters = $request->getQueryParameters();

    foreach ($filters as $filter) {
      $parameters = array_merge($parameters, $filter->toQueryParameter());
    }

    return $request->withQueryParameters($parameters);
  }
}
```

In the snippet above, we are extracting the query parameters that may already be in place on our request, then merging them with the passed through filter query parameters, before returning back our modified request. A thing to note here is that the PSR-7 request is typically immutable by default, so you need to make sure that your logic is applied in the way that you expect.

```php
final readonly class ProductResource
{
  use CanCreateRequests;

  public function __construct(
    private SDK $sdk,
  ) {}

  public function list(array $filters = [], null|Sort $sort = null, null|string $direction = null): Collection
  {
    $request = $this->request(
      method: Method::GET,
      uri: '/products',
    );

    $request = $this->applyFilters(
      request: $request,
      filters: $filters,
    );

    // send request
    // capture response
    // throw error if response failed
    // return transformed response as a collection
  }
}
```

We can now work on sending the request, and how we might want to achieve that using PSRs too.

## PSR-18 and sending HTTP Requests

We’ve already seen PSR-17, but how about [PSR-18](https://www.php-fig.org/psr/psr-18/). It allows a level of interoperability between HTTP clients, so that you aren’t stuck using Guzzle v7.0 or Symfony HTTP Client. Instead, like all good software, you build your implementation to an interface and rely on dependency injection or similar to tell the application exactly what should be used when resolving the interface. This is clean, very testable, and great for building PHP code that isn’t going to break from a random `composer update`.

How can we implement it though? It can be pretty confusing to try and implement PSRs on their own, the documentation is aimed at library authors who are typically used to reading specification documents. Let’s look at a quote from the specification so you can understand what I mean.

> Note that as a result, since [PSR-7 objects are immutable](https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-7-http-message-meta.md#why-value-objects), the Calling Library MUST NOT assume that the object passed to `ClientInterface::sendRequest()` will be the same PHP object that is actually sent. For example, the Request object that is returned by an exception MAY be a different object than the one passed to `sendRequest()`, so comparison by reference (===) is not possible.
>

Now if you read it, it makes sense! But if you are trying to build against it, along with other PSRs - things can get complicated quickly as the rules pile up in-front of you. This is why we use a library such as [PHP HTTP](https://docs.php-http.org/en/latest/), which allows us to auto-discover everything that we need.

Using this library, we are able to discover the HTTP client installed and use it directly. However, I prefer (and recommend) a different approach. The [PHP-HTTP library](https://docs.php-http.org/en/latest/) offers a Plugin Client that we can use. This offers more of a composable approach to building up our HTTP client. Let’s look at how we can use this in isolation before how we might implement this into our SDK.

```php
use Http\Discovery\HttpClientDiscovery;
use Http\Client\Common\PluginClient;

$client = new PluginClient(
  client: HttpClientDiscovery::find(),
  plugins: [],
);
```

So, our plugin client will use the discovered client, but also accept an array of plugins the can be applied on each request. You can see all of the requirements for this [here](https://docs.php-http.org/en/latest/plugins/introduction.html) but I will walk you through a standard approach that I like to use:

```php
use Http\Discovery\HttpClientDiscovery;
use Http\Client\Common\PluginClient;
use Http\Client\Common\Plugin\AuthenticationPlugin;
use Http\Client\Common\Plugin\ErrorPlugin;
use Http\Client\Common\Plugin\RetryPlugin;
use Http\Message\Authentication\Bearer;

$client = new PluginClient(
  client: HttpClientDiscovery::find(),
  plugins: [
    new RetryPlugin(),
    new ErrorPlugin(),
    new AuthenticationPlugin(
      authentication: new Bearer(
        token: 'YOUR_API_TOKEN',
      ),
    ),
  ],
);
```

You can add as many plugins as you need here, there are cache plugins, history plugins, decoding plugins. The list goes on, and is well documented in case you want to build your own plugins. But retry, errors, and authentication is a good list to start with.

Now we know how to build it, we can look at how we might want to implement this. All of our resources are going to want to send requests. But, we don’t want to overload them with traits. To me, the perfect place for this is on the client itself.

```php
final class SDK
{
  public function __construct(
    private readonly string $url,
    private readonly string $token,
    private array $plugins = [],
  ) {}

  public function withPlugins(array $plugins): SDK
  {
    $this->plugins = $plugins;

    return $this;
  }

  public function url(): string
  {
    return $this->url;
  }

  public function token(): string
  {
    return $this->token;
  }
}
```

First we need to start by removing the `readonly` from the class, and add it to the `url` and `token` properties. The reason for this is because our plugins property that we need to add, we want to be able to override, or at least have the option to should we need to. As this will be your SDK, we can customize this a little past this point.

```php
final class SDK
{
  public function __construct(
    private readonly string $url,
    private readonly string $token,
    private array $plugins = [],
  ) {}

  public function withPlugins(array $plugins): SDK
  {
    $this->plugins = array_merge(
      $this->defaultPlugins(),
      $plugins,
    );

    return $this;
  }

  public function defaultPlugins(): array
  {
    return [
      new RetryPlugin(),
      new ErrorPlugin(),
      new AuthenticationPlugin(
        new Bearer(
          token: $this->token(),
        ),
      ),
    ];
  }

  public function url(): string
  {
    return $this->url;
  }

  public function token(): string
  {
    return $this->token;
  }
}
```

Our final step is to have a way to get and override the client that will be used to send all of the HTTP requests. At this point our client is getting pretty big, and people using our SDK may want to implement their own approach. It is important that we avoid making too many decisions for our users. The best way to achieve this, as always, is to code to an interface. Let’s design that now:

```php
interface SDKContract
{
  public function withPlugins(array $plugins): SDKContract;

  public function defaultPlugins(): array;

  public function client(): ClientInterface;

  public function setClient(ClientInterface $client): SDKContract;

  public function url(): string;

  public function token(): string;
}
```

Let’s focus in on our two new methods:

```php

final class SDK implements SDKContract
{
  public function __construct(
    private readonly string $url,
    private readonly string $token,
    private ClientInterface $client,
    private array $plugins = [],
  ) {}

  public function client(): ClientInterface
  {
    return new PluginClient(
      client: HttpClientDiscovery::find(),
      plugins: $this->defaultPlugins(),
    );
  }

  public function setClient(ClientInterface $client): SDKContract
  {
    $this->client = $client;

    return $this;
  }
}
```

Our `client` method will return a new Plugin Client, using HTTP discovery to find the client we have installed, also attaching the plugins that we want by default. But, how about if our users want to add additional plugins? How would this look in Laravel?

```php
final class AppServiceProvider extends ServiceProvider
{
  public function register(): void
  {
    $this->app->singleton(
      abstract: SDKContract::class,
      concrete: fn () => new SDK(
        url: config('services.commerce.url'),
        token: config('services.commerce.token'),
        client: new PluginClient(
          client: HttpClientDiscovery::find(),
        ),
        plugins: [
          new CustomPlugin(),
        ],
      ),
    );
  }
}
```

## Working with Responses

At this point we are able to send the requests we need - at least for GET requests so far. Next, we want to look at how we receive and process the response data itself. There are two different approaches you could take when it comes to handling responses coming back from your API.

- Return the PSR-7 Response directly
- Transform the response into a Data Transfer Object.

There are benefits to each approach, however it mostly depends on the purpose of the SDK. If you want a completely hands free approach, then working with Data Transfer Objects is my recommended approach. Providing strongly typed, contextual objects that your customers can use to work with the data as required. The other option is of course to allow the clients to transform the response, as they see fit.

At this point we need to think back to what this SDK is for. This is an SDK that allows people to integrate with their online stores, so you want to be able to give as much freedom on implementation as possible. However for the purpose of education, let’s show how we might transform this response data anyway.

The way we do this in PHP is by designing our class, and hydrating this as we get the response. A fantastic resource for this is a package called [Serde](https://github.com/Crell/Serde) by a member of PHP-FIG, [Crell](https://github.com/Crell). Another option is to use [Object Mapper](https://github.com/thephpleague/object-mapper) which is by The PHP League. Both libraries offer a very similar functionality, the choice is more down to your personal preference. Our first step is to design the class we want to hydrate, this will typically match your API response.

```php
final readonly class Product
{
  public function __construct(
    public string $sku,
    public string $name,
    public string $description,
    public int $price,
  ) {}
}
```

This assumes that any routing is using the `sku` to lookup the product. The way I like to use these objects is to add a static method that will hydrate the class. The reason for this is because it keeps all logic about creating the object is contained within the class it is creating. There is no looking around for what class does what, it is all in one place.

```php
final readonly class Product
{
  public function __construct(
    public string $sku,
    public string $name,
    public string $description,
    public int $price,
  ) {}

  public static function make(array $data): Product
  {
    $mapper = new ObjectMapperUsingReflection();

    return $mapper->->hydrateObject(
      className: self::class,
      payload: $data,
    );
  }
}
```

As you can see, this is a neat bundle that will allow you to just send data to the class and receive the object back in a uniform way. How does this look in our SDK?

```php
final readonly class ProductResource
{
  use CanCreateRequests;

  public function __construct(
    private SDK $sdk,
  ) {}

  public function list(array $filters = [], null|Sort $sort = null, null|string $direction = null): Collection
  {
    $request = $this->request(
      method: Method::GET,
      uri: '/products',
    );

    $request = $this->applyFilters(
      request: $request,
      filters: $filters,
    );

    try {
      $response = $this->sdk->client()->sendRequest(
        request: $request,
      );
    } catch (Throwable $exception) {
      throw new FailedToFetchProducts(
        message: 'Failed to fetch product list from API.',
        previous: $exception,
      );
    }

    return new Collection(
      collectionType: Product::class,
      data: array_map(
        callback: static fn (array $data): Product => Product::make(
          data: $data,
        ),
        array: (array) json_decode(
          json: $response->getBody()->getContents(),
          associative: true,
          flags: JSON_THROW_ON_ERROR,
        ),
      ),
    );
  }
}
```

This attempts to send the request, and catches any potential exceptions. We then throw a contextual exception so that if anything does go wrong, we capture it and understand exactly what and where things broke. We then return a collection of Products, by mapping over the json response as an array. We are using the [Collection](https://github.com/ramsey/collection) library that is built by [Ben Ramsey](https://github.com/ramsey) here, not the Laravel one. We *could* just return an array, but I find it useful if you are going to go to the effort of returning objects, wrapping them in something with additional developer experience is a huge plus.

## Pagination

At some point you will need to make a decision about how you want to handle pagination - if at all. Let’s walk through the options, and figure out the what why and hows for paginating your API requests in your SDK.

The first option that most developers reach for is the `do while` approach. Which you add a do while loop within your code to just crawl the API endpoint until you get to the end of the paginated data - and then return the response. Personally I do not like this approach as it makes a few too many decisions for you. What if you don’t want to fetch all of the data, and just want to first page?

Next up, the paginator class. This will do almost the same as the do while approach, but instead you wrap the SDK call inside a pagination class which will handle the looping for you. This is a little better, as you aren’t mixing the HTTP calls with client intended logic. However to achieve this, you need to add a way to work with pages within your methods.

Finally, the programmatic approach. Much like the paginator class, your method will just accept a nullable page parameter which will request the specific page you actually want. Personally, I like this approach the most. If the client wants to paginate over the data, they have the ability to - without me forcing them into my way of doing it. Let’s have a look at a quick example.

```php
final readonly class ProductResource
{
  use CanCreateRequests;

  public function __construct(
    private SDK $sdk,
  ) {}

  public function list(
    array $filters = [],
    null|Sort $sort = null,
    null|string $direction = null,
    null|int $page = null,
  ): Collection {
    $request = $this->request(
      method: Method::GET,
      uri: '/products',
    );

    $request = $this->applyFilters(
      request: $request,
      filters: $filters,
    );

    if (null !== $page) {
      $request = $request->withQueryParameters([
        'page' => $page
      ]);
    }

    // send request
    // capture response
    // throw error if response failed
    // return transformed response as a collection
  }
}
```

If we pass through a page, we want to make sure we include it in the query parameters being sent over to the API. Your pagination may be different, for example you may use cursor pagination which will require you to pass over a specific hash. Yes the method parameters are getting long, but they all serve a purpose for control. Whoever said methods shouldn’t have more than 3 arguments has never built an SDK before.

On the client side, this is now simple to work with:

```php
$products = $sdk->products()->list(
  page: 1,
);
```

You could even wrap this in your own pagination class or provide a dedicated one with your SDK should you need it. I will show a quick high level interface so you know how this would be structured.

```php
interface Paginator
{
  public function fetch(SDK $sdk, string $method, array $parameters = []): Generator;

  public function hasNext(): bool;
}
```

Now, let’s look at an implementation:

```php
final class Pagination implements Paginator
{
  public function __construct(
    private readonly int $perPage,
    private array $pagination = [],
  ) {}

  public function fetch(SDK $sdk, string $method, array $parameters = []): Generator
  {
    foreach ($this->fetch($sdk, $method, $parameters) as $value) {
      yield $value;
    }

    while ($this->hasNext()) {
      foreach ($this->fetchNext() as $value) {
        yield $value;
      }
    }
  }

  public function hasNext(): bool
  {
    return $this->pagination['next'] !== null;
  }

  private function get(string $key): array
  {
    $pagination = $this->pagination[$key] ?? null;

    if ($pagination === null) {
      return [];
    }

    // Send the request and get the response.

    $content = ResponseMediator::getContent($response);

    if (! \is_array($content)) {
      throw new RuntimeException('Pagination of this endpoint is not supported.');
    }

    $this->postFetch();

    return $content;
  }

  private function postFetch(): void
  {
    // Get the last request from the SDK Client.
    $this->pagination = $response === null
      ? []
      : ResponseMediator::getPagination($response);
  }
}
```

You do of course so something a little simpler if you need to, but in general this should work for you. The Response Mediator class is a utility class that I would sometimes use to simplify the working with API data. Let’s move onto how we might actually send some requests now though.

## Sending Data through our SDK

One of the final stepping stones to building a good SDK, is figuring out how we want to create and update potential resources. In our example of an e-commerce API, the likelihood of creating a product object via API is extremely low. Typically you would use a provided admin dashboard. So, for this next example we are going to focus on the Customer resource. When a user registers through your platform, you want to create a customer resource on the e-commerce API, so that if the authenticated user orders anything - they will be able to link to the correct customer quickly and easily. We will look at creating a new customer next.

There are a few options, as always, when creating resources through an SDK. You can either:

- Send a validated array through to the SDK
- Send another Data Transfer Object specific to the request through to the SDK

My personal preference here is to use DTOs and then let the SDK handle sending this in the correct format. It allows a more strongly typed approach, and puts all of the control in the hands of the SDK - which minimizes potential risk.

```php
final readonly class CreateCustomer
{
  public function __construct(
    public string $name,
    public string $email,
    public string $referrer,
  ) {}

  public static function make(array $data): CreateCustomer
  {
    $mapper = new ObjectMapperUsingReflection();

    return $mapper->->hydrateObject(
      className: self::class,
      payload: $data,
    );
  }
}
```

Just like the Product DTO, we add a static make method using the object mapper to create the object itself. Let’s now design the resource.

```php
final readonly class CustomerResource
{
  use CanCreateRequests;

  public function __construct(
    private SDK $sdk,
  ) {}

  public function create(CreateCustomer $customer)
  {
    $request = $this->request(
      method: Method::POST,
      uri: '/customers',
    );

    // attach the customer as a stream.
  }
}
```

We now need to work with our trait again, so that we can work with sending and using data.

```php
trait CanCreateRequests
{
  // Other method...

  public function attachPayload(RequestInterface $request, string $payload): RequestInterface
  {
    return $request->withBody(
      Psr17FactoryDiscovery::findStreamFactory()->createStream(
        content: $payload,
      );
    );
  }
}
```

What we are doing here is passing through the request we are building, and the stringified version of the payload. Again, we can use auto-discovery to detect what HTTP Stream Factory is installed, then create a stream from the payload and attach it to the request as its body.

We need a way to quickly and easily serialize the data from our DTOs to send through to create a stream. Let’s look at the DTO for creating a customer again.

```php
final readonly class CreateCustomer
{
  public function __construct(
    public string $name,
    public string $email,
    public string $referrer,
  ) {}

  public function toString(): string
  {
    return (string) json_encode(
      value: [
        'name' => $this->name,
        'email' => $this->email,
        'referrer' => $this->referrer,
      ],
      flags: JSON_THROW_ON_ERROR,
    );
  }

  public static function make(array $data): CreateCustomer
  {
    $mapper = new ObjectMapperUsingReflection();

    return $mapper->->hydrateObject(
      className: self::class,
      payload: $data,
    );
  }
}
```

Now let’s go back to the implementation.

```php
final readonly class CustomerResource
{
  use CanCreateRequests;

  public function __construct(
    private SDK $sdk,
  ) {}

  public function create(CreateCustomer $customer)
  {
    $request = $this->request(
      method: Method::POST,
      uri: '/customers',
    );

    $request = $this->attachPayload(
      request: $request,
      payload: $customer->toString(),
    );

    try {
      $response = $this->sdk->client()->sendRequest(
        request: $request,
      );
    } catch (Throwable $exception) {
        throw new FailedToCreateCustomer(
          message: 'Failed to create customer record on the API.',
          previous: $exception,
        );
    }

    // Return something that makes sense to your use case here.
  }
}
```

So, we can quickly and easily create and send data. A typical usage of this in a Laravel application, would be to leverage the events system - listening for something like the `Registered` event to be fired:

```php
final readonly class CreateNewCustomer
{
  public function __construct(
    private SDK $sdk,
  ) {}

  public function handle(Registered $event): void
  {
    try {
      $this->sdk->customers()->create(
        customer: CreateCustomer::make(
          data: [
            'name' => $event->name,
            'email' => $event->email,
            'referrer' => $event->referrer,
          ],
        ),
      );
    } catch (Throwable $exception) {
      Log::error('Failed to create customer record on API', ['event' => $event]);
      
      throw $exception;
    }
  }
}
```

Quite clean and easy to use I am sure you would agree. The only improvement I would potentially suggest here is to use a dispatchable job or event-sourcing style system here, something that would allow you to replay the attempt - giving you the opportunity to fix and retry.

## Summary

As you can see from this tutorial, building an SDK for your API isn’t overly tricky - but there are a lot of things to think about. With developer experience being a key factor in the success of your SDK, you need to make sure you think about that alongside the technical requirements that your SDK has.

At Speakeasy we have carefully designed how SDKs should work in each language we support, allowing you to follow a similar approach to the above without having to write a single line of code. Instead it will use your OpenAPI specification to generate a robust, well tested, and developer friendly SDK for your API. Even better, it will take less time than waiting for a pizza delivery. Now, I have always been against auto-generated SDKs, especially when you see some of the examples out there. However, what Speakeasy does is a completely different approach that guarantees better success and developer experience. Instead you can focus on building the best API and OpenAPI specification you can - and let us focus on providing you with a great SDK in multiple languages.


 This is the content for the doc blog/categories/[category].mdx 

 ---
title: "Blog"
description: "The Speakeasy Blog offers actionable advice for creating SDKs and Terraform providers from an OpenAPI / Swagger spec and tips on how to improve the developer experience of your API."
breadcrumb: false
---

import { Category, getStaticPaths as getCategoryStaticPaths, getStaticProps as getCategoryStaticProps } from '~/features/blog/category';

<Category />

export const getStaticPaths = getCategoryStaticPaths;
export const getStaticProps = getCategoryStaticProps;


 This is the content for the doc blog/choosing-your-framework-python/index.mdx 

 ---
title: "Choosing your Python REST API framework"
description: "We compare the most popular Python frameworks for building REST APIs."
image: "/media/choosing-your-framework-python.png"
date: 2024-12-09
authors:
  - name: Nolan Di Mare Sullivan
  - image_url: "/media/author-headshots/nolan.jpeg"
tags:
  - API Advice
featured_image: "/media/choosing-your-framework-python.png"
---

import { IconGrid } from "~/features/shared/recipes";
import { pickingAPythonFrameworkData } from "~/data/post/picking-a-python-framework";

We're fortunate to live in a time when a wide selection of Python API frameworks is available to us. But an abundance of choice can also be overwhelming. Do you go for the latest, trending option or stick with the tried-and-tested framework that offers security and control? 

Whether you're a startup founder who needs to deliver an MVP in a few weeks while taking scale and performance into consideration, or part of a large organization running hundreds of microservices needing reliable and robust technologies, choosing the right API framework is a critical decision. The key is recognizing that every framework choice involves trade-offs, which shift based on your project's unique needs. Failing to account for this can lead to frustration down the road.

In this post, we discuss the factors to consider when choosing a REST API framework and explores popular options, highlighting each framework's strengths and weaknesses. At the end of the article, we'll suggest a pragmatic approach you can take to make an informed decision.

<IconGrid {...pickingAPythonFrameworkData} />

## Factors to consider when choosing a Python API framework

### Iteration speed

For startups or fast-moving teams, the pressure to ship an MVP or new features quickly can outweigh concerns about the project's long-term architecture. But this short-term focus can lead to technical debt, making it harder to scale or adapt the API later. 

To strike the right balance between speed and maintainability, it helps to understand when speed is essential and when it's worth investing time in a more robust foundation. The solution lies in using tools that offer the flexibility to write code quickly while setting aside some initial scalability or performance concerns, with the option to refactor and evolve your architecture as your project grows.

Start with a simple, script-like setup for exposing endpoints without committing to a solid architecture upfront. Once the business is stable, you can take advantage of the framework's features to transition to a more complex and robust architecture.

### Enterprise needs: Scale and security

Your MVP has succeeded, and your project now serves a significant user base. Or maybe you're operating in an enterprise environment, building a service that must handle thousands or even millions of daily requests. While flexibility is still appealing at this stage, relying on tools that prioritize flexibility over structure is no longer wise. Instead, focus on well-structured frameworks designed to help with scalability, simplify complex processes, and abstract away the challenges introduced by your growing needs.

When choosing a framework for mature or large-scale projects, you need to consider:

- **Request volume:** The number of requests your application needs to handle.
- **Authorization:** How to manage user permissions securely and efficiently.
- **Database optimization:** Ensuring database queries are performant and scalable.
- **Logging:** Implementing proper logging for monitoring and debugging.
- **Performance:** Maintaining responsiveness under heavy traffic and load.

While lightweight frameworks can handle these challenges with careful implementation, your top priorities should shift to performance, robustness, and security.

When evaluating frameworks for these needs, consider these three critical factors:

- **Framework maturity and adoption:** A framework with wide industry adoption can be a sign of reliability. A strong community and long-standing development history often reflect a framework's stability and available support.
- **Security:** A framework with many built-in features may introduce security vulnerabilities. Assess the framework's history of handling security issues, its track record with security updates, and the quality of its documentation.
- **Robustness:** Evaluate the framework's architecture for its ability to abstract complex tasks effectively, ensuring scalability and maintainability over time.

### Async support

Asynchronous programming is known for its performance benefits, especially in non-blocking operations. For example, imagine an API that handles file uploads: The user doesn't need the upload to finish immediately or receive a download link right away. They just want confirmation that the process has started and that they'll be notified of its success or failure later. This is where async frameworks shine, allowing the API to respond without waiting for the file upload to complete.

Synchronous frameworks like Flask or Django can still handle asynchronous-like tasks using background job libraries like Celery paired with tools like Redis or RabbitMQ. While these frameworks have introduced partial async support in their architectures, they are not fully asynchronous yet. Background job solutions like Celery, Redis, and RabbitMQ are robust for task delegation, but they come with additional setup complexity and don't achieve proper non-blocking behavior within the API. 

Frameworks built with async programming in mind, like Tornado and FastAPI, provide a more intuitive coding experience for async tasks.

## Popular Python API frameworks

### Flask-RESTX: Familiar, lightweight, and flexible

Flask alone is sufficient to build a REST API. However, to add important REST API features like automatic Swagger documentation, serialization, and error handling, [Flask-RESTX](https://flask-restx.readthedocs.io/) offers tools that simplify additional parts of your workflow.

Here's an example that creates an application to list payments: 

```python app.py
from flask import Flask
from flask_restx import Api, Resource, fields

app = Flask(__name__)
api = Api(app, doc="/docs")  # Swagger UI documentation available at /docs

ns = api.namespace('payments', description="Payment operations")

payment_model = api.model('Payment', {
    'id': fields.Integer(description="The unique ID of the payment", required=True),
    'amount': fields.Float(description="The amount of the payment", required=True),
    'currency': fields.String(description="The currency of the payment", required=True),
    'status': fields.String(description="The status of the payment", required=True),
})

# Sample data
payments = [
    {'id': 1, 'amount': 100.0, 'currency': 'USD', 'status': 'Completed'},
    {'id': 2, 'amount': 50.5, 'currency': 'EUR', 'status': 'Pending'},
    {'id': 3, 'amount': 200.75, 'currency': 'GBP', 'status': 'Failed'},
]

@ns.route('/')
class PaymentList(Resource):
    @ns.marshal_list_with(payment_model)
    def get(self):
        return payments

api.add_namespace(ns)

if __name__ == "__main__":
    app.run(debug=True)
```

This code snippet creates an application that runs on port **5000** and provides two endpoints:

- `/payments`, for listing payments.
- `/docs`, for automatically documenting the payments endpoint.

The Flask-RESTX marshaling feature is noteworthy for how it automatically maps the results – whether from a database, file, or API request – to a defined schema and sends a structured response to the client. This functionality ensures consistency and reduces boilerplate code for formatting responses. 

The Flask ecosystem gives you the flexibility to create your application in the way that suits your needs. When the time comes to scale, Flask combined with [Flask-RESTX](https://flask-restx.readthedocs.io/) provides you with the features you need to handle larger, more complex projects effectively.

### Sanic: For lightweight and production-ready real-time APIs

Sanic (not to be confused with Sonic the Hedgehog, though it's just as speedy) is a lightweight, asynchronous Python web framework designed for high-performance and real-time applications. While these characteristics might suggest complexity, writing an application that serves both an HTTP endpoint and a WebSocket server is surprisingly straightforward.

```python app.py
from sanic import Sanic
from sanic.response import json

app = Sanic("ConfigAPI")

configs = {
    "app_name": "My App",
    "version": "1.0.0",
    "debug": True,
    "max_connections": 100,
    "allowed_hosts": ["localhost", "127.0.0.1"],
}


@app.get("/configs")
async def get_configs(request):
    return json(configs)


if __name__ == "__main__":
    app.run(host="127.0.0.1", port=8000, debug=True)
```

Sanic intuitively handles static files, making it a user-friendly alternative to popular frameworks like [Django, which can require more complex configurations for similar tasks](https://www.quora.com/Why-is-Django-making-handling-static-files-so-difficult).   

```python app.py
app = Sanic("ConfigAPI")

app.static('/static', './static')
```

Another point in Sanic's favor is its interesting approach to handling TLS, a process that can be complicated to understand and set up. With Sanic, you can start your server using your certificate files, or even better, let it automatically set up local TLS certificates, enabling secure access with little configuration. 

```bash
sanic path.to.server:app \--dev \--auto-tls
```

### FastAPI: Build modern and highly typed REST APIs

FastAPI's excellent developer experience has made it one of the most popular Python frameworks. By combining async programming, type hints, and automatic OpenAPI document generation, FastAPI enables you to create highly documented APIs with minimal effort.

FastAPI's design is also async-first, making it an excellent choice for real-time APIs, high-concurrency workloads, and systems needing rapid prototyping with built-in tools. FastAPI offers modern convenience and a healthy ecosystem of complementary tooling without compromising on performance. 

The following code example demonstrates creating a REST API for listing and creating invoices.

```python app.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List

app = FastAPI()

class Invoice(BaseModel):
    id: int
    customer_uid: str
    amount: float
    status: str

# In-memory storage for invoices
invoices = [
    Invoice(id=1, customer_uid="4r3dd", amount=250.50, status="Paid"),
    Invoice(id=2, customer_uid="f3f3f3f", amount=150.00, status="Pending"),
]

@app.get("/invoices", response_model=List[Invoice])
async def list_invoices():
    return invoices
```

### Django REST framework

If what you care about is security, reliability, and maturity, [Django REST framework (DRF)](https://www.django-rest-framework.org/) is what you want. Django is the most mature Python framework and rose to prominence thanks to its abstractions of the tedious but essential parts of backend development: authentication, authorization, logging, multiple database connections, caching, testing, and much more. 

However, this abstraction comes with trade-offs. Django is not especially flexible or lightweight, and its enforced Model-View-Template (MVT) structure can feel verbose and rigid compared to more modern frameworks. However, if you embrace its design principles, Django can be one of the most stable and effective frameworks you've ever used.

When it comes to async support, DRF does not currently support async functionality. This limitation means you cannot create async API views or viewsets using DRF, as its core features – like serializers, authentication, permissions, and other utilities – are not designed to work asynchronously. 

Third-party package [ADRF (Async DRF)](https://github.com/em1208/adrf) adds async support, but it's not officially supported and may not be stable for production. That undermines the core value of Django REST framework: stability.

To create an API with DRF, you need to define a model first.   

```python models.py
from django.db import models

class Item(models.Model):
    name = models.CharField(max_length=255)
    description = models.TextField()
    price = models.DecimalField(max_digits=10, decimal_places=2)
    created_at = models.DateTimeField(auto_now_add=True)

    def __str__(self):
        return self.name
```

Then, you need to define a serializer that will convert the Python object retrieved from the Django ORM to a JSON object and vice versa.   

```python serializers.py
from rest_framework import serializers
from .models import Item

class ItemSerializer(serializers.ModelSerializer):
    class Meta:
        model = Item
        fields = ['id', 'name', 'description', 'price', 'created_at']
``` 
 
Next, you need to write a view (or in standard terms, controller) to handle the API logic, in this case, listing.

```python views.py
from rest_framework.generics import ListCreateAPIView
from .models import Item
from .serializers import ItemSerializer

class ItemListCreateView(ListCreateAPIView):
    queryset = Item.objects.all()
    serializer_class = ItemSerializer
```   

Finally, you need to register the view in a `urls.py` file.  

```python
from django.urls import path
from .views import ItemListCreateView

urlpatterns = [
    path('items/', ItemListCreateView.as_view(), name='item-list-create'),
]
```

This example illustrates how verbose Django can be. But by following its well-documented architecture, you ensure your application is robust and scalable while following proven design principles. 

### Tornado: Pure async logic

Tornado is a lightweight framework built entirely around asynchronous programming, making it ideal for building APIs where non-blocking I/O is critical, like WebSocket-based applications or systems with high-concurrency needs. If you don't have the immediate pressure of needing an extensive feature set or an existing ecosystem, Tornado can be an excellent choice for applications requiring pure async workflows.  

```python app.py
from tornado.ioloop import IOLoop
from tornado.web import Application, RequestHandler
import json

# In-memory storage
orders = []

# Handler to list all orders
class OrderListHandler(RequestHandler):
    async def get(self):
        self.set_header("Content-Type", "application/json")
        self.write(json.dumps(orders))

# Initialize the Tornado app
def make_app():
    return Application([
        (r"/orders", OrderListHandler),    # Endpoint to list all orders
    ])

if __name__ == "__main__":
    app = make_app()
    app.listen(8888)
    print("Server is running on http://127.0.0.1:8888")
    IOLoop.current().start()
```

However, Tornado lacks some of the built-in tools and abstractions found in more modern frameworks like FastAPI, meaning you might spend more time building features available out of the box elsewhere.

## Making pragmatic choices

The Python API frameworks we've discussed each have distinct strengths and trade-offs, but choosing the right framework for your project might still be a daunting task.

To help you select a framework, we've created a flowchart that simplifies the decision-making process and a table that maps use cases to recommended frameworks. To use these resources, start with the flowchart to narrow your options based on your project's stage, requirements, and priorities. Then, consult the table to match your use case and requirements to recommended frameworks. 

![A flowchart for choosing a Python framework](./assets/framework-decision-diagram.png)


| **Use case**                | **Requirements**                             | **Recommended frameworks**          |
|-----------------------------|---------------------------------------------|--------------------------------------|
| **MVP with limited resources** | Quick setup, simplicity, flexibility         | Flask-RESTX, FastAPI                      |
| **Complex project**          | Scalability, structure, robust tools         | Django + DRF                        |
| **Secure enterprise application** | Strong security, maintainability, scalability | Django + DRF                        |
| **Fully async workload**     | High concurrency, non-blocking performance   | FastAPI, Tornado                    |
| **Real-time application**    | WebSocket support, low latency               | Tornado, Sanic                      |
| **Existing project**         | Gradual migration to async or scaling needs  | Django (with ASGI), FastAPI         |

Consider:

1. What does your project need most — stability or speed?
2. Are you starting fresh or scaling an existing application?
3. Does the framework support your required features without adding unnecessary risk?  
4. How well does the framework align with your team's expertise?

If your team has extensive experience with one framework, that might be your go-to for creating a REST API. If stability, reliability, and enterprise-grade features are your priorities, then [Django REST framework (DRF)](https://www.django-rest-framework.org/) probably makes sense. If your priorities are a modern developer experience, performance, or emerging async capabilities, then a cutting-edge framework like **FastAPI** is a great choice.


 This is the content for the doc blog/contract-testing-with-openapi/index.mdx 

 ---
title: "Contract testing with OpenAPI"
description: "Learn how to implement contract testing with OpenAPI to ensure consistency across distributed systems and catch breaking changes early in development."
image: "/media/contract-testing-with-openapi.png"
date: 2024-09-30
authors:
  - name: Brian Flad
  - image_url: "/media/author-headshots/brian.jpg"
tags:
  - API Advice
featured_image: "/media/contract-testing-with-openapi.png"
---

import { ScrollyCoding } from "~/components/codehike/Scrollycoding";
import { Callout } from "~/components";

We've all heard that infernal phrase, "It works on my machine." Scaling any solution to work across many machines can be a challenge. The world of APIs with disparate consumers is no different. What if there was a well-defined contract between them?

But APIs and consumers change over time, which begs the question: How do we ensure our systems stick to their agreements? The answer is contract testing.

In this article, we'll explore contract testing, how it fits into the testing pyramid, and why it's important. We'll also walk through how to implement contract testing using OpenAPI, a popular API specification format.

Finally, we'll discuss how you can generate contract tests automatically using OpenAPI and Speakeasy.

## What is contract testing?

Contract testing verifies that two parties who interact via an API stick to the contract they agreed upon. We'll call these parties a consumer and a provider, based on the direction of the API calls between them. The consumer is the system that makes API calls, and the provider is the system that receives and responds to the API call.

In most cases, the contract between our parties is defined by an API specification, such as OpenAPI.

Even if there is no API specification, there is always, at the very least, an implicit agreement between the consumer and the provider. The consumer expects the provider to respond in a certain way, and the provider expects the consumer to send requests in a certain way.

This explicit or implicit agreement is the contract.

Contract testing, from the consumer's perspective, is about verifying that the provider sticks to the contract. From the provider's perspective, it's about verifying that the consumer sticks to the contract.

## How contract testing differs from unit testing, integration testing, and end-to-end testing

Testing strategy is often represented as a pyramid, with unit testing at the base, followed by other testing methodologies in ascending order of complexity and scope. The idea is that the majority of tests should be at the base of the pyramid, with fewer tests at each subsequent level.

The testing pyramid is a useful model for thinking about how different types of tests fit together in a testing strategy. It helps to ensure that the right types of tests are used in the appropriate proportions, balancing the need for comprehensive testing with the need for fast feedback.

<div className="mx-auto max-w-md">
  ![Testing pyramid showing unit testing at the base, followed by contract
  testing, integration testing, and end-to-end
  testing](./assets/testing-pyramid.svg)
</div>

Let's discuss each level of the pyramid in more detail, starting from the base.

### Unit testing: Does this function work as expected?

Unit testing forms the base of the testing pyramid. These tests focus on individual components or functions in isolation, typically mocking any dependencies. They are fast to run and easy to maintain, but don't test how components work together.

In the consumer's context, a unit test might verify whether the function that deserializes a JSON object into a Python object works correctly, without making any external API calls.

Unit tests are essential for catching bugs early in the development process and providing fast feedback to developers. They are also useful for ensuring that code behaves as expected when refactoring or adding new features. However, they don't provide much confidence that the system as a whole works correctly.

### Contract testing: Do we honor the API specification?

Moving up the pyramid, we have contract tests. Contract testing sits between unit testing and integration testing. It focuses specifically on the interactions between the provider and consumer for a given call, ensuring that the API contracts are honored. Contract tests are more complex than unit tests but less complex than integration tests.

A contract test verifies that a consumer can create requests with specific data and correctly handle the provider's expected responses or errors. This might be accomplished with mocked request or response data based on the contract. For example, an API contract test for an order creation endpoint might verify that request data correctly maps integer item IDs to quantities and that the response decodes to an expected success with an integer order ID.

Contract tests are useful for catching issues that arise when the consumer or provider strays from the agreed-upon contract. They provide a level of confidence that the system works as expected when the consumer and provider interact. They are also useful for catching breaking changes early in the development process.

### Integration testing: Do systems work together?

Further up, we find integration tests. These verify that different components of a system work together correctly. They are more complex than unit tests and contract tests, and may involve multiple components or services.

An integration test might verify that the consumer can successfully make an API call to the provider and receive a valid response. This test would involve both the consumer and provider, and would typically run in a test environment that mirrors the production environment.

Because integration tests involve multiple systems, they are useful for catching issues that arise when components interact, such as network issues between two services.

### End-to-end testing: Does the user flow work as expected?

At the top of the pyramid are end-to-end tests. These test the entire user flow and supporting systems from start to finish. They provide the highest level of confidence but are also the slowest to run and most difficult to maintain.

In our API context, an end-to-end test might involve making a series of API calls that represent a complete user journey, verifying that the system behaves correctly at each step. This could include creating a resource, updating it, retrieving it, and finally deleting it, all through the API.

When end-to-end tests fail, it can be challenging to identify the root cause of the failure, as the problem could be in any part of the system. Due to their complexity and cost, they are often used as a final check before deploying to production, rather than as part of the regular development process.

### Testing pyramid summary

Here's a summary of the different types of tests and how they compare:

| Aspect           | Unit testing     | Contract testing | Integration testing     | End-to-end testing |
| ---------------- | ---------------- | ---------------- | ----------------------- | ------------------ |
| Scope            | Functional logic | Interface logic  | Provider implementation | User journeys      |
| Speed            | Very fast        | Fast             | Moderate                | Slow               |
| Complexity       | Low              | Medium           | Medium to high          | High               |
| Isolation        | High             | Medium           | Low                     | Very low           |
| Typical quantity | Many             | Several          | Some                    | Few                |

## Why contract testing is important

Over time, APIs change in response to changing requirements, sometimes in subtle and imperceptible ways. For example, a provider may change the format of a field in a certain response from a string to an integer. This change may seem innocuous to the provider but could have catastrophic effects for the consumer.

Contract testing mitigates this risk by ensuring that any changes to the API contract are detected early in the development process. When the provider updates the API, corresponding contract tests will fail if the update is not backward compatible. This failure acts as an immediate signal that the change needs to be reviewed, preventing breaking changes from reaching production.

Consider this example: A subscription management platform (the provider) has an endpoint `/plan/{id}` that returns a subscription plan based on the plan ID. The consumer expects the response to include an `amount` field, which is an integer representing the cost of the plan. If the provider changes the `amount` field from an integer to a string, the consumer's contract test will fail, alerting the consumer to the breaking change.

In this example, a contract test would catch the breaking change early in the development process, before it reaches production. The consumer and provider can then work together to resolve the issue, ensuring that the API contract is honored.

## How to implement contract testing with OpenAPI

Let's walk through the process of implementing contract testing using [OpenAPI](/openapi/), focusing on both the consumer and provider perspectives.

### Step 1: Create a new project

We'll start by creating a new project for our consumer and provider code. We'll use a simple subscription management API as an example.

In the terminal, run:

```bash
mkdir contract-example
cd contract-example
```

Let's create a new TypeScript project for our SDK and tests:

In the terminal, run:

```bash
npm install --save-dev typescript
npx tsc --init
```

Select the default options when prompted.

### Step 2: Define the OpenAPI specification

We'll start by writing an OpenAPI specification for our API. In most cases, the provider will define the OpenAPI specification, as they are responsible for the implementation of the API.

<ScrollyCoding className="ch-scrollycoding-full-height ch-scrollycoding-force-focus-scroll" fullHeight>

## !!steps

Here's a basic example of an OpenAPI specification. Save it as `subscriptions.yaml` in the root of your project.

```yaml ! subscriptions.yaml
openapi: 3.1.0
info:
  title: Subscription Management API
  version: 1.0.0
servers:
  - url: http://127.0.0.1:4010
    description: Local server
tags:
  - name: subscription
    description: Subscription management
security:
  - api_key: []
paths:
  /plan/{id}:
    get:
      operationId: getPlanById
      tags:
        - subscription
      summary: Get a subscription plan by ID
      parameters:
        - name: id
          in: path
          required: true
          schema:
            type: integer
          examples:
            basic:
              value: 1
            premium:
              value: 2
      responses:
        "200":
          description: Successful response
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Subscription"
              examples:
                basic:
                  value:
                    id: 1
                    name: "Basic"
                    amount: 100
                    currency: "USD"
                premium:
                  value:
                    id: 2
                    name: "Premium"
                    amount: 200
                    currency: "USD"
components:
  schemas:
    Subscription:
      type: object
      properties:
        id:
          type: integer
        name:
          type: string
        amount:
          type: integer
        currency:
          type: string
      example:
        id: 1
        name: "Basic"
        amount: 100
        currency: "USD"
  securitySchemes:
    api_key:
      type: apiKey
      name: X-API-Key
      in: header
```

---

## !!steps

This OpenAPI specification defines a single endpoint `/plan/{id}`.

```yaml ! subscriptions.yaml
# !focus(13:37)
# !mark(14)
```

---

## !!steps

The `/plan/{id}` endpoint has a single `GET` operation that retrieves a subscription plan by ID. It expects an integer `id` parameter in the path.

```yaml ! subscriptions.yaml
# !focus(13:37)
# !mark(15:25)
```

---

## !!steps

We'll only focus on the `200` response for now. The response should be a JSON object with the subscription plan details, as defined in the `Subscription` schema.

```yaml ! subscriptions.yaml
# !focus(31:37)
# !mark(37)
```

---

## !!steps

The `Subscription` schema defines the structure of the response object. It includes fields for `id`, `name`, `amount`, and `currency`.

```yaml ! subscriptions.yaml
# !focus(53:68)
```

</ScrollyCoding>

This example OpenAPI specification only defines the happy path for the `/plan/{id}` endpoint. In a real-world scenario, you would define additional paths, operations, and error responses to cover all possible scenarios.

### Step 3: Create an SDK with Speakeasy

We'll use Speakeasy to create a TypeScript SDK from the OpenAPI specification. If you don't have Speakeasy installed, you can install it from the [Introduction to Speakeasy](/docs/introduction/introduction#getting-started) guide.

With Speakeasy installed, run:

```bash
speakeasy quickstart
```

When prompted, select the `subscriptions.yaml` file and choose TypeScript as the target language. We decided on `Billing` as the SDK name, and `billing` as the package name.

### Step 4: Add tests

Now that we have an SDK, we can write tests to verify that the SDK handles the API responses correctly.

Let's install the necessary dependencies:

```bash
npm i --save-dev vitest
```

Create a new `tests` directory in the root of your project. Then, create a new file, `tests/subscription.test.ts`:

```typescript tests/subscription.test.ts
import { expect, test } from "vitest";
import { Billing } from "../billing-typescript/src/index.ts"

test("Subscription Get Plan By Id Basic", async () => {
  const billing = new Billing({
    apiKey: process.env["BILLING_API_KEY"] ?? "",
  });
  const result = await billing.subscription.getPlanById({
    id: 1,
  });
  expect(result).toBeDefined();
  expect(result).toEqual({
    id: 1,
    name: "Basic",
    amount: 100,
    currency: "USD",
  });
});
```

Add the following script to your `package.json`:

```json package.json
// !focus(2:4)
{
  "scripts": {
    "test": "vitest run"
  }
}
```

Now you can run the tests:

```bash
npm run test
```

This should run the test and verify that the SDK correctly handles the API responses, but since we haven't started a server yet, the test will fail.

### Step 5: Start a mock server

We'll use Prism to start a mock server that serves responses based on the OpenAPI specification.

Add Prism as a dev dependency:

```bash
npm install --save-dev @stoplight/prism-cli
```

Then, add a new script to your `package.json`:

```json package.json
// !focus(4)
{
  "scripts": {
    "test": "vitest run",
    "mock": "prism mock subscriptions.yaml"
  }
}
```

In a new terminal window, run:

```bash
npm run mock
```

This will start a mock server at `http://127.0.0.1:4010`.

### Step 6: Run the tests

Now that the mock server is running, you can run the tests again:

```bash
npm run test
```

This time, Prism returns a `401` status code because we haven't provided an API key. Let's run the test with the `BILLING_API_KEY` set to `test`:

```bash
export BILLING_API_KEY=test
npm run test
```

```txt
$ vitest run
 
 RUN  v2.1.1 /Users/speakeasy/contract-example
 
 ✓ tests/subscription.test.ts (1)
   ✓ Subscription Get Plan By Id Basic
 
 Test Files  1 passed (1)
      Tests  1 passed (1)
   Start at  07:28:38
   Duration  630ms (transform 191ms, setup 0ms, collect 199ms, tests 150ms, environment 0ms, prepare 86ms)
```

The test should now pass, verifying that the SDK correctly handles the API response.

### Step 7: Test for correctness

We've validated that the SDK can correctly handle an API response by interacting with a mock server, but we haven't confirmed whether the response conforms to the contract. To make this a true contract test, let’s verify that both the consumer and provider behaviors align with the agreed-upon OpenAPI specification.

We'll add a contract-validation step to the test, then use Ajv, a JSON Schema validator, to validate the response against the OpenAPI schema.

```bash
npm install --save-dev ajv ajv-errors ajv-formats yaml
```

Create a new file, `validateSchema.ts`:

```typescript validateSchema.ts
import Ajv from "ajv";
import addFormats from "ajv-formats";
import addErrors from "ajv-errors";
import { readFileSync } from "fs";
import yaml from "yaml";

// Load and parse the OpenAPI specification
const openApiSpec = yaml.parse(readFileSync("./subscriptions.yaml", "utf8")) as any;

// Initialize Ajv with formats and error messages
const ajv = new Ajv({ allErrors: true, strict: false });
addFormats(ajv);
addErrors(ajv);

// Compile the schema for the Subscription response
const subscriptionSchema = {
  ...openApiSpec.components.schemas.Subscription
};

const validate = ajv.compile(subscriptionSchema);

export const validateSubscription = (data: any) => {
  const isValid = validate(data);
  if (!isValid) {
    console.error(validate.errors);
    throw new Error("Validation failed");
  }
};
```

Update the test to include the contract-validation step:

```typescript tests/subscription.test.ts mark=3,19
import { expect, test, expectTypeOf } from "vitest";
import { Billing } from "../billing-typescript/src/index.ts";
import { validateSubscription } from "../validateSchema.ts";

test("Subscription Get Plan By Id Basic", async () => {
  const billing = new Billing({
    apiKey: process.env["BILLING_API_KEY"] ?? "",
  });
  const result = await billing.subscription.getPlanById({
    id: 1,
  });
  expect(result).toBeDefined();
  expect(result).toEqual({
    id: 1,
    name: "Basic",
    amount: 100,
    currency: "USD",
  });
  validateSubscription(result); // Contract validation
});
```

Now when you run the tests, the contract validation will ensure that the response from the mock server matches the OpenAPI specification.

```bash
npm run test
```

## Generating contract tests automatically with OpenAPI

Manually writing contract tests can be a time-consuming and error-prone process. If you're starting with an OpenAPI document as your contract, you may be able to automatically generate tests that conform to your contract.

By generating contract tests, you reduce the risk of human error, save significant development time, and ensure that tests are always kept up to date.

The biggest advantage of automated test generation is the assurance that your tests are based on the API specification. This means that all aspects of the API contract, from endpoint paths and methods to data types and constraints, are accurately represented in the generated tests.

A drawback of basing tests on OpenAPI documents is that the OpenAPI Specification does not currently have built-in support for test generation. Although examples of requests and responses allow test case generation, there are still challenges in linking request and response pairs to each other. These are problems we're working hard to overcome at Speakeasy.

## Speakeasy test generation

<Callout title="EARLY ACCESS" variant="info">
  Speakeasy API test generation is in beta. Join the [early
  access](/product/api-testing) program to give it a try.
</Callout>

At Speakeasy, we enable developers to automatically test their APIs and SDKs by creating comprehensive test suites. Shipping automated tests as part of your SDKs will enable your team to make sure that the interfaces your users prefer, your SDKs, are always compatible with your API. We ensure your APIs and SDKs stick to the contract so that you can focus on shipping features and evolving your API with confidence.

The process of adding tests with Speakeasy is straightforward: Add detailed examples to your OpenAPI document, or describe tests in a simple and well-documented YAML specification that lives in your SDK project. Speakeasy will regenerate your tests when they need to change, and you can run the tests as part of development or CI/CD workflows.

Speakeasy's new [automated API testing](/product/api-testing) platform is in early access and currently supports Python, Go, TypeScript, and Java.

## Speakeasy test generation roadmap

Looking ahead, Speakeasy's testing roadmap includes broader language support, advanced server mocking, ability to run contract tests on past versions of the API and SDK, and using the Arazzo specification to string together multiple contract tests. With these features, you'll be able to monitor the health of all your SDKs and APIs in one place.

We're also working on support for behavior-driven development (BDD) and end-to-end (E2E) testing by embracing OpenAPI and the recently published Arazzo specification for complex testing workflows.

To join the Speakeasy automated API testing early access program, follow the signup link on the [testing page](/product/api-testing).


 This is the content for the doc blog/create-a-terraform-provider-a-guide-for-beginners.mdx 

 ---
title: "How To Create a Terraform Provider — a Guide for Absolute Beginners"
description: "Learn the basics of creating a Terraform provider from scratch"
keywords: [go, golang, HashiCorp, Terraform, tutorial, provider]
image: "/media/api-advice-how-to-create-a-TF-provider.png"
date: 2024-03-13
authors:
  - name: Tristan Cartledge
  - image_url: "https://uploads-ssl.webflow.com/62ccd7b208cab0723d026273/635ff12733f46637e91ced22_1516859875198.jpg"
tags:
  - API Advice
featured_image: "/media/api-advice-how-to-create-a-TF-provider.png"
---

This tutorial shows you how to create a simple Terraform provider for your web service. 

Terraform is a large, complicated piece of software, and the Terraform tutorials on creating a Terraform provider are lengthy and intimidating. But creating a provider doesn't have to be complicated. 

In this guide, we strip away many of the unnecessary functions that Terraform demonstrates and create a provider that does nothing but create, read, update, and delete a resource via an API. You don't need any experience using Terraform to follow along — we'll explain everything as we go.

## Prerequisites

You need [Docker](https://docs.docker.com/get-docker) to run the code provided here. You can install Terraform and Go locally if you prefer, but you'll need to adjust the commands we provide to suit your operating system.

## Set Up Your System

Create a folder on your computer to work in. Open a terminal in the folder and run the commands below to create a basic project structure.

```sh
touch Dockerfile
mkdir 1_webservice
mkdir 2_customer
mkdir -p 3_provider/internal/provider
```

The `1_webservice` folder represents the service that your company sells online. In this example, we'll have an API that can add and remove users. This service can be in any programming language.

The `2_customer` folder represents how your users will call Terraform to talk to your service. This folder will hold a Terraform resource configuration file.

The `3_provider` folder is the custom Terraform provider that will let Terraform talk to your web service. This provider will have three files in Go (Terraform uses only Go for plugins).


Add the text below to the `Dockerfile`.

```bash
FROM --platform=linux/amd64 alpine:3.19

WORKDIR /workspace

RUN apk add go curl unzip bash sudo nodejs npm vim

ENV GOPATH=/root/go
ENV PATH=$PATH:$GOPATH/bin

# install terraform:
RUN curl -O https://releases.hashicorp.com/terraform/1.7.0/terraform_1.7.0_linux_amd64.zip && \
    unzip terraform_1.7.0_linux_amd64.zip && \
    mv terraform /usr/local/bin/ && \
    rm terraform_1.7.0_linux_amd64.zip
```

Now build the Docker image and start working in it using the commands below. Your current folder will be shared with the Docker container as `/workspace`, so you can edit the files on your computer while running Go in the container.

```sh
docker build -t timage .
docker run -it --volume .:/workspace --name tbox timage
# if you stop the container and want to restart it later, run: docker start -ai tbox
```

## Create the Web Service

Since we'll write the Terraform provider in Go, let's create a basic web service in Go, too. 

Terraform uses CRUD (create, read, update, and delete) operations to manage any resource in any system, from AWS and Azure to your company's finance, software, or healthcare product.

In the `1_webservice` folder, create a single-file service that allows a customer to create users with an ID and name, and update and delete users. Run the commands below in the Docker container terminal:

```sh
cd /workspace/1_webservice
go mod init main
go get github.com/go-chi/chi/v5
touch main.go
```

These commands create a `go.mod` file in the folder and add the `chi` web framework dependency.

In a text editor, copy the code below into `main.go`.

```go
package main

import (
	"fmt"
	"io"
	"net/http"
	"sync"

	"github.com/go-chi/chi/v5"
)

var users = make(map[string]string) // Map to store users with id as key and name as value
var mutex = &sync.RWMutex{}         // Mutex to protect access to the map as server is multithreaded

func main() {
	router := chi.NewRouter()

	router.Post("/{id}", func(response http.ResponseWriter, request *http.Request) {
		id := chi.URLParam(request, "id")
		name, err := io.ReadAll(request.Body)
		if err != nil {
			http.Error(response, "Failed to read request body", http.StatusBadRequest)
			return
		}
		mutex.Lock()
		defer mutex.Unlock()
		users[id] = string(name)
		fmt.Fprintf(response, "%s", string(name))
		fmt.Println("POST: ", id, " ", string(name))
	})
```

This code imports parts of the Go standard library related to HTTP and the `chi` web framework.

The code then makes a variable called `users` to hold the IDs and names of users. As we aren't using a database here, this will work only as long as the service is running, and all users will be lost when the service stops.

We then create a mutex to handle safe writing to the users lists. Go is multithreaded, so we need a mutex to allow each HTTP handler to check that no other thread is trying to update the users list at the same time.

Finally, we have an HTTP `POST` handler to set a user in the list. To keep this guide short, we do no fancy checks for existing users or errors. The service overwrites items in the list with `users[id] = string(name)`. The function returns plain text (not JSON) to the caller with `fmt.Fprintf(response, "%s", string(name))`. The last line prints to the console to show that it's working.

So far, we have only a `Create` handler with the `Post` method. Let's add read, update, and delete. Append the code below to `main.go`.

```go
  router.Get("/{id}", func(response http.ResponseWriter, request *http.Request) {
		id := chi.URLParam(request, "id")
		mutex.RLock()
		defer mutex.RUnlock()
		name, ok := users[id]
		if !ok {
			http.NotFound(response, request)
			return
		}
		fmt.Fprintf(response, "%s", name)
		fmt.Println("GET: ", id, " ", name)
	})

	router.Put("/{id}", func(response http.ResponseWriter, request *http.Request) {
		id := chi.URLParam(request, "id")
		name, err := io.ReadAll(request.Body)
		if err != nil {
			http.Error(response, "Failed to read request body", http.StatusBadRequest)
			return
		}
		mutex.Lock()
		defer mutex.Unlock()
		if _, ok := users[id]; !ok {
			http.NotFound(response, request)
			return
		}
		users[id] = string(name)
		fmt.Fprintf(response, "%s", string(name))
		fmt.Println("PUT: ", id, " ", users[id])
	})

	router.Delete("/{id}", func(response http.ResponseWriter, request *http.Request) {
		id := chi.URLParam(request, "id")
		mutex.Lock()
		defer mutex.Unlock()
		name, ok := users[id]
		if !ok {
			http.NotFound(response, request)
			return
		}
		delete(users, id)
		fmt.Fprintf(response, "%s", name)
		fmt.Println("PUT: ", id, " ", users[id])
	})

	http.ListenAndServe(":6251", router)
}
```

Test the service by running the commands below.

```sh
go run main.go &
curl -X POST -d "Jane" http://localhost:6251/2
curl http://localhost:6251/2
```

This series of commands starts the service in the background, creates a user `[2,Jane]`, and retrieves the user from the service.

To stop and restart the background service, run the command below:

```
killall main; go run main.go &
```

## Create a Terraform Configuration File

So you have a web service, and in reality, you might even have an SDK in Python, Go, Java, and other languages that your customers could use to call your service. Why do you need Terraform, too?

We answer this question in detail in [our blog post about using Terraform as a SaaS API interface](/post/build-terraform-providers). In summary, Terraform allows your customers to manage multiple environments with a single service (Terraform) through declarative configuration files that can be stored in Git. This means that if one of your customers wants to add a new user or a whole new franchise, they can copy a Terraform resource configuration file from an existing franchise, update it, check it into GitHub, and get it approved. Then Terraform can run it automatically using continuous integration. This has benefits for your customers in terms of speed, safety, repeatability, auditing, and correctness.

Let's create a Terraform configuration file to demonstrate this now. Run the commands below:

```sh
cd /workspace/2_customer
touch main.tf
```

Paste the code below into `main.tf`:

```hcl
# load the provider
terraform {
  required_providers {
    myuserprovider = {
      source  = "example.com/me/myuserprovider"
      # version = "~> 1.0"
    }
  }
}

# configure the provider
provider "myuserprovider" {
  endpoint = "http://localhost:6251/"
}

# configure the resource
resource "myuserprovider_user" "john_doe" {
  id   = "1"
  name = "John Doe"
}
```

In the first section, we tell Terraform that it will need to use a custom provider to interact with our service, `example.com/me/myuserprovider`. We name the service `myuserprovider`.

In the second section, we configure this provider with the URL of the web service.

The final section is what your customers will use most. Here we create a resource (a user) with an ID and a name. You could create hundreds of users here. Once the users are created, you can also change their names or delete them, and Terraform will automatically make the appropriate calls to your service to ensure that the API matches the state it recorded locally.

This `main.tf` file is all your customers need to work with once you've created a provider. Let's create the provider now.

## Create a Custom Terraform Provider

Run the commands below:

```sh
cd /workspace/3_provider
touch go.mod
```

Here we create `go.mod` manually because a Terraform provider needs a lot of dependencies. (The dependencies come from the [Terraform provider scaffolding project](https://github.com/hashicorp/terraform-provider-scaffolding-framework).)

Add the text below to `go.mod`.

```go
module example.com/me/myuserprovider

go 1.21

require (
	github.com/hashicorp/go-version v1.6.0
	github.com/hashicorp/terraform-plugin-docs v0.18.0
	github.com/hashicorp/terraform-plugin-framework v1.6.1
	github.com/hashicorp/terraform-plugin-go v0.22.0
	github.com/hashicorp/terraform-plugin-log v0.9.0
	github.com/hashicorp/terraform-plugin-testing v1.7.0
)

require (
	github.com/Kunde21/markdownfmt/v3 v3.1.0 // indirect
	github.com/Masterminds/goutils v1.1.1 // indirect
	github.com/Masterminds/semver/v3 v3.2.0 // indirect
	github.com/Masterminds/sprig/v3 v3.2.3 // indirect
	github.com/ProtonMail/go-crypto v1.1.0-alpha.0 // indirect
	github.com/agext/levenshtein v1.2.2 // indirect
	github.com/apparentlymart/go-textseg/v15 v15.0.0 // indirect
	github.com/armon/go-radix v1.0.0 // indirect
	github.com/bgentry/speakeasy v0.1.0 // indirect
	github.com/cloudflare/circl v1.3.7 // indirect
	github.com/fatih/color v1.16.0 // indirect
	github.com/golang/protobuf v1.5.3 // indirect
	github.com/google/go-cmp v0.6.0 // indirect
	github.com/google/uuid v1.4.0 // indirect
	github.com/hashicorp/cli v1.1.6 // indirect
	github.com/hashicorp/errwrap v1.1.0 // indirect
	github.com/hashicorp/go-checkpoint v0.5.0 // indirect
	github.com/hashicorp/go-cleanhttp v0.5.2 // indirect
	github.com/hashicorp/go-cty v1.4.1-0.20200414143053-d3edf31b6320 // indirect
	github.com/hashicorp/go-hclog v1.6.2 // indirect
	github.com/hashicorp/go-multierror v1.1.1 // indirect
	github.com/hashicorp/go-plugin v1.6.0 // indirect
	github.com/hashicorp/go-uuid v1.0.3 // indirect
	github.com/hashicorp/hc-install v0.6.3 // indirect
	github.com/hashicorp/hcl/v2 v2.20.0 // indirect
	github.com/hashicorp/logutils v1.0.0 // indirect
	github.com/hashicorp/terraform-exec v0.20.0 // indirect
	github.com/hashicorp/terraform-json v0.21.0 // indirect
	github.com/hashicorp/terraform-plugin-sdk/v2 v2.33.0 // indirect
	github.com/hashicorp/terraform-registry-address v0.2.3 // indirect
	github.com/hashicorp/terraform-svchost v0.1.1 // indirect
	github.com/hashicorp/yamux v0.1.1 // indirect
	github.com/huandu/xstrings v1.3.3 // indirect
	github.com/imdario/mergo v0.3.15 // indirect
	github.com/mattn/go-colorable v0.1.13 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	github.com/mattn/go-runewidth v0.0.9 // indirect
	github.com/mitchellh/copystructure v1.2.0 // indirect
	github.com/mitchellh/go-testing-interface v1.14.1 // indirect
	github.com/mitchellh/go-wordwrap v1.0.0 // indirect
	github.com/mitchellh/mapstructure v1.5.0 // indirect
	github.com/mitchellh/reflectwalk v1.0.2 // indirect
	github.com/oklog/run v1.0.0 // indirect
	github.com/posener/complete v1.2.3 // indirect
	github.com/russross/blackfriday v1.6.0 // indirect
	github.com/shopspring/decimal v1.3.1 // indirect
	github.com/spf13/cast v1.5.0 // indirect
	github.com/vmihailenco/msgpack v4.0.4+incompatible // indirect
	github.com/vmihailenco/msgpack/v5 v5.4.1 // indirect
	github.com/vmihailenco/tagparser/v2 v2.0.0 // indirect
	github.com/yuin/goldmark v1.6.0 // indirect
	github.com/yuin/goldmark-meta v1.1.0 // indirect
	github.com/zclconf/go-cty v1.14.3 // indirect
	golang.org/x/crypto v0.21.0 // indirect
	golang.org/x/exp v0.0.0-20230809150735-7b3493d9a819 // indirect
	golang.org/x/mod v0.15.0 // indirect
	golang.org/x/net v0.21.0 // indirect
	golang.org/x/sys v0.18.0 // indirect
	golang.org/x/text v0.14.0 // indirect
	golang.org/x/tools v0.13.0 // indirect
	google.golang.org/appengine v1.6.8 // indirect
	google.golang.org/genproto/googleapis/rpc v0.0.0-20231106174013-bbf56f31fb17 // indirect
	google.golang.org/grpc v1.61.1 // indirect
	google.golang.org/protobuf v1.32.0 // indirect
	gopkg.in/yaml.v2 v2.3.0 // indirect
)
```

Note the module name at the top of the file, `module example.com/me/myuserprovider`. This name consists of an example URL to make the module globally unique, and the name used for the provider in the `main.tf` file — `myuserprovider`.

There are only three code files that are essential to create a provider. They are each presented in a subsection below.

### The `main.go` File

The first file you need is `main.go`. Create it in `/workspace/3_provider/main.go` and add the code below to it:

```go
package main

import (
	"context"
	"log"

	"example.com/me/myuserprovider/internal/provider"
	"github.com/hashicorp/terraform-plugin-framework/providerserver"
)

func main() {
	opts := providerserver.ServeOpts{
		Address: "example.com/me/myuserprovider",
	}
	err := providerserver.Serve(context.Background(), provider.New(), opts)
	if err != nil {
		log.Fatal(err.Error())
	}
}
```

This file creates a `providerserver`, a server that hosts the provider plugin that Terraform can connect to and use. When Terraform looks for your plugin to load it, this `main` function is what Terraform calls to get access to the provider, created with `provider.New()`.

Providers are structured like a Go web service. Functions receive a `context`, which holds state, a request, and a response. Functions can add data to the `context` that Terraform will use when the function exits. We'll see an example of this when we create the resource file.

### The `provider.go` File

Create a `3_provider/internal/provider/provider.go` file and add the code below to it:

```go
package provider

import (
	"context"
	"net/http"

	tfdatasource "github.com/hashicorp/terraform-plugin-framework/datasource"
	tffunction "github.com/hashicorp/terraform-plugin-framework/function"
	tfprovider "github.com/hashicorp/terraform-plugin-framework/provider"
	tfschema "github.com/hashicorp/terraform-plugin-framework/provider/schema"
	tfresource "github.com/hashicorp/terraform-plugin-framework/resource"
	tftypes "github.com/hashicorp/terraform-plugin-framework/types"
)

type UserProviderModel struct {
	Endpoint tftypes.String `tfsdk:"endpoint"`
}

type UserProvider struct {
	endpoint string
	client   *http.Client
}

var _ tfprovider.Provider = &UserProvider{}
var _ tfprovider.ProviderWithFunctions = &UserProvider{}

func New() func() tfprovider.Provider {
	return func() tfprovider.Provider {
		return &UserProvider{}
	}
}
```

This code does the following:
- Imports the Terraform Go framework.
- Defines a `UserProviderModel` struct with an `endpoint`. This endpoint will come from the `main.tf` configuration file (the URL of your web service).
- Defines a `UserProvider` struct that holds any data the provider needs throughout its life. In our case, we need only the web service URL and an HTTP client that we can pass to the resource manager (created in the next section).
- Checks that `UserProvider` correctly implements all the functions Terraform needs in `var _ tfprovider.Provider = &UserProvider{}`. It creates a discarded `_` variable and assigns it the type `tfprovider.Provider` so that the Go compiler can verify it.
- Defines a `New()` function to return an instance of our provider. This function was called in the previous file in the provider server.

Next, add the functions below to the `provider.go` file:

```go
func (p *UserProvider) Metadata(ctx context.Context, req tfprovider.MetadataRequest, resp *tfprovider.MetadataResponse) {
	resp.TypeName = "myuserprovider" // matches in your .tf file `resource "myuserprovider_user" "john_doe" {`
}

func (p *UserProvider) Schema(ctx context.Context, req tfprovider.SchemaRequest, resp *tfprovider.SchemaResponse) {
	resp.Schema = tfschema.Schema{
		Attributes: map[string]tfschema.Attribute{
			"endpoint": tfschema.StringAttribute{
				MarkdownDescription: "Endpoint of the API, e.g. - http://localhost:6251/",
				Required:            true,
			},
		},
	}
}

func (p *UserProvider) Configure(ctx context.Context, req tfprovider.ConfigureRequest, resp *tfprovider.ConfigureResponse) {
	var data UserProviderModel
	resp.Diagnostics.Append(req.Config.Get(ctx, &data)...)
	if resp.Diagnostics.HasError() {
		return
	}
	p.endpoint = data.Endpoint.ValueString()
	p.client = http.DefaultClient
	resp.DataSourceData = p // will be usable by DataSources
	resp.ResourceData = p   // will be usable by Resources
}

func (p *UserProvider) Resources(ctx context.Context) []func() tfresource.Resource {
	return []func() tfresource.Resource{
		NewUserResource,
	}
}

func (p *UserProvider) DataSources(ctx context.Context) []func() tfdatasource.DataSource {
	return []func() tfdatasource.DataSource{}
}

func (p *UserProvider) Functions(ctx context.Context) []func() tffunction.Function {
	return []func() tffunction.Function{}
}
```

- `Metadata()` contains the name of the provider.
- `Schema()` must match the `main.tf` file so that Terraform can get the configuration settings for the provider.
- `Configure()` gets the settings from the configuration file, creates an HTTP client, saves the settings to the `UserProvider` struct, and adds them to the method's response type. We set `ResourceData` so that the resource manager has access to all the fields of the `UserProvider` struct.
- `Resources()` creates a single `NewUserResource` instance. The `NewUserResource` function returns a `UserResource` type, which is what interacts with the users in the web service, and we create it in the next subsection. Since our provider doesn't manage any `DataSources`, we don't create any.

### The `userResource.go` File

Create a `3_provider/internal/provider/userResource.go` file and add the code below to it:

```go
package provider

import (
	"bytes"
	"context"
	"fmt"
	"io"
	"net/http"

	tfpath "github.com/hashicorp/terraform-plugin-framework/path"
	tfresource "github.com/hashicorp/terraform-plugin-framework/resource"
	tfschema "github.com/hashicorp/terraform-plugin-framework/resource/schema"
	tftypes "github.com/hashicorp/terraform-plugin-framework/types"
)

var _ tfresource.Resource = &UserResource{}
var _ tfresource.ResourceWithImportState = &UserResource{}

type UserResource struct {
	client   *http.Client
	endpoint string
}

type UserModel struct {
	Id   tftypes.String `tfsdk:"id"`
	Name tftypes.String `tfsdk:"name"`
}

func NewUserResource() tfresource.Resource {
	return &UserResource{}
}
```

This code is similar to the code in the previous file we created. It loads dependencies, checks the interfaces compile, and defines the struct the resource will use.

Note the `UserModel`. This struct is what will communicate between the web service and Terraform core. Terraform will save the values here for `Id` and `Name` into a local state file that mimics what Terraform thinks the web service state is. Terraform uses its own types to do this, `terraform-plugin-framework/types`, not plain Go types.

Next, add the code below to allow the resource to configure itself:

```go

func (r *UserResource) Metadata(ctx context.Context, req tfresource.MetadataRequest, resp *tfresource.MetadataResponse) {
	resp.TypeName = req.ProviderTypeName + "_user" // matches in main.tf: resource "myuserprovider_user" "john_doe" {
}

func (r *UserResource) Schema(ctx context.Context, req tfresource.SchemaRequest, resp *tfresource.SchemaResponse) {
	resp.Schema = tfschema.Schema{
		MarkdownDescription: "User resource interacts with user web service",
		Attributes: map[string]tfschema.Attribute{
			"id": tfschema.StringAttribute{
				MarkdownDescription: "The user ID",
				Required:            true,
			},
			"name": tfschema.StringAttribute{
				MarkdownDescription: "The name of the user",
				Required:            true,
			},
		},
	}
}

func (r *UserResource) Configure(ctx context.Context, req tfresource.ConfigureRequest, resp *tfresource.ConfigureResponse) {
	if req.ProviderData == nil { // this means the provider.go Configure method hasn't been called yet, so wait longer
		return
	}
	provider, ok := req.ProviderData.(*UserProvider)
	if !ok {
		resp.Diagnostics.AddError(
			"Could not create HTTP client",
			fmt.Sprintf("Expected *http.Client, got: %T", req.ProviderData),
		)
		return
	}
	r.client = provider.client
	r.endpoint = provider.endpoint
}
```

Again, this code looks similar to the code in the previous file.
- Note how the `Metadata()` function combines the provider and resource names with `_` in `myuserprovider_user`. This matches the name in `main.tf` and is a Terraform naming standard.
- `Schema()` defines what Terraform will remember about the remote resource in local state.
- `Configure()` gets the information from the provider we configured in the `provider.go` file in the `Configure()` method, `resp.ResourceData = p`. It receives an HTTP client and URL from the provider to use in the resource manager.

The `if req.ProviderData == nil` line is essential. Terraform can load the resource manager before the provider, so when the `Configure()` function is called, there may not yet be a provider to get configuration data from. In this case, the function will exit, and Terraform will call it again later when the provider has been loaded. It seems strange that Terraform would call the resource manager before the provider since it seems that the provider **owns** the resource manager, but that's just how it is.

The last code you need to add to `userProvider.go` is the heart of the provider: Calling the web service with CRUD functions and returning the response to Terraform to update its state. This code is also the easiest to understand. We'll explain the `Create` function after you've added the code below. The other functions are similar.

```go
func (r *UserResource) Create(ctx context.Context, req tfresource.CreateRequest, resp *tfresource.CreateResponse) {
	var state UserModel
	resp.Diagnostics.Append(req.Plan.Get(ctx, &state)...)
	if resp.Diagnostics.HasError() {
		return
	}
	response, err := r.client.Post(r.endpoint+state.Id.ValueString(), "application/text", bytes.NewBuffer([]byte(state.Name.ValueString())))
	if err != nil {
		resp.Diagnostics.AddError("Client Error", fmt.Sprintf("Error sending request: %s", err))
		return
	}
	defer response.Body.Close()
	if response.StatusCode != http.StatusOK {
		resp.Diagnostics.AddError("HTTP Error", fmt.Sprintf("Received non-OK HTTP status: %s", response.Status))
		return
	}
	body, err := io.ReadAll(response.Body)
	if err != nil {
		resp.Diagnostics.AddError("Failed to Read Response Body", fmt.Sprintf("Could not read response body: %s", err))
		return
	}
	state.Name = tftypes.StringValue(string(body))
	resp.Diagnostics.Append(resp.State.Set(ctx, &state)...)
}

func (r *UserResource) Read(ctx context.Context, req tfresource.ReadRequest, resp *tfresource.ReadResponse) {
	var state UserModel
	resp.Diagnostics.Append(req.State.Get(ctx, &state)...)
	if resp.Diagnostics.HasError() {
		return
	}
	response, err := r.client.Get(r.endpoint + state.Id.ValueString())
	if err != nil {
		resp.Diagnostics.AddError("Client Error", fmt.Sprintf("Unable to read user, got error: %s", err))
		return
	}
	defer response.Body.Close()
	if response.StatusCode == http.StatusNotFound {
		resp.State.RemoveResource(ctx)
		return
	}
	if response.StatusCode == http.StatusOK {
		bodyBytes, err := io.ReadAll(response.Body)
		if err != nil {
			resp.Diagnostics.AddError("Error reading response body", err.Error())
			return
		}
		state.Name = tftypes.StringValue(string(bodyBytes))
		resp.Diagnostics.Append(resp.State.Set(ctx, &state)...)
		return
	}
	resp.Diagnostics.AddError("HTTP Error", fmt.Sprintf("Received bad HTTP status: %s", response.Status))
}

func (r *UserResource) Delete(ctx context.Context, req tfresource.DeleteRequest, resp *tfresource.DeleteResponse) {
	var data UserModel
	resp.Diagnostics.Append(req.State.Get(ctx, &data)...)
	if resp.Diagnostics.HasError() {
		return
	}
	request, err := http.NewRequest(http.MethodDelete, r.endpoint+data.Id.ValueString(), nil)
	if err != nil {
		resp.Diagnostics.AddError("Request Creation Failed", fmt.Sprintf("Could not create HTTP request: %s", err))
		return
	}
	response, err := r.client.Do(request)
	if err != nil {
		resp.Diagnostics.AddError("Client Error", fmt.Sprintf("Unable to delete user, got error: %s", err))
		return
	}
	defer response.Body.Close()
	if response.StatusCode != http.StatusOK {
		resp.Diagnostics.AddError("HTTP Error", fmt.Sprintf("Received non-OK HTTP status: %s", response.Status))
		return
	}
	data.Id = tftypes.StringValue("")
	data.Name = tftypes.StringValue("")
	resp.Diagnostics.Append(resp.State.Set(ctx, &data)...)
}

func (r *UserResource) Update(ctx context.Context, req tfresource.UpdateRequest, resp *tfresource.UpdateResponse) {
	var state UserModel
	resp.Diagnostics.Append(req.Plan.Get(ctx, &state)...)
	if resp.Diagnostics.HasError() {
		return
	}
	webserviceCall, err := http.NewRequest("PUT", r.endpoint+state.Id.ValueString(), bytes.NewBuffer([]byte(state.Name.ValueString())))
	if err != nil {
		resp.Diagnostics.AddError("Go Error", fmt.Sprintf("Error sending request: %s", err))
	}
	response, err := r.client.Do(webserviceCall)
	if err != nil {
		resp.Diagnostics.AddError("Client Error", fmt.Sprintf("Error sending request: %s", err))
		return
	}
	defer response.Body.Close()
	if response.StatusCode != http.StatusOK {
		resp.Diagnostics.AddError("HTTP Error", fmt.Sprintf("Received non-OK HTTP status: %s", response.Status))
		return
	}
	body, err := io.ReadAll(response.Body)
	if err != nil {
		resp.Diagnostics.AddError("Failed to Read Response Body", fmt.Sprintf("Could not read response body: %s", err))
		return
	}
	state.Name = tftypes.StringValue(string(body))
	resp.Diagnostics.Append(resp.State.Set(ctx, &state)...)
}

func (r *UserResource) ImportState(ctx context.Context, req tfresource.ImportStateRequest, resp *tfresource.ImportStateResponse) {
	tfresource.ImportStatePassthroughID(ctx, tfpath.Root("id"), req, resp)
}
```

The `Create` function looks like a web handler, with a context, request, and response. As mentioned earlier, Terraform uses the web metaphor to structure its plugins. Like the other three functions, `Create()` does three things:
- Loads the Terraform state for the resource with `req.Plan.Get(ctx, &state)`. This represents what Terraform thinks the remote resource is, or what it wants it to be.
- Calls the web service and gets the response with `r.client.Post(r.endpoint+state.Id.ValueString()`.
- Saves the response to the local Terraform state with `resp.State.Set(ctx, &state)`.

Note that you don't have to write any logic to reason about changing the remote state, for example, adding or updating the user if the response from the web service is not what you anticipated. That's what Terraform Core is for. Terraform will call the correct sequence of CRUD functions to work out how to change the remote users based on your desired users in the configuration file.

Be careful to use only `ValueString()` when working with Terraform string types. There are similar functions, like `String()` and `Value()`, that can add extra `"` marks to your fields. You'll encounter confusing errors with infinite update loops calling Terraform if you don't notice that you're adding extra string quotes to every web service call when you use the wrong method.

## Run the Provider

Let's recapitulate. You've:
- Created a one-file web service to manage users that represents your company's product that you sell to customers.
- Created a `main.tf` Terraform configuration file to say that you want to use the `myuserprovider` provider to create a user called "John Doe" using the web service.
- Created a Terraform provider with three files: a provider server, a provider, and a user resource manager.

Now it's time to run Terraform pretending that you're one of your customers calling your web service and check that your provider works with the configuration file.

Because your provider isn't hosted on the online Terraform registry, you need to tell Terraform to use the local project.

Create a file called `.terraformrc` in the `workspace` folder:

```sh
cd /workspace
touch .terraformrc
```

Insert the text below:

```hcl
provider_installation {
    dev_overrides {
        "example.com/me/myuserprovider" = "/workspace/3_provider/bin"
    }
    direct {} # For all other providers, install directly from their origin provider.
}
```

In the Docker terminal, run the command below to copy this Terraform settings file to the container home folder (where you're user `root`), so that Terraform knows where to look for your provider.

```sh
cp /workspace/.terraformrc /root/
```

Now let's run the provider and test it. Run the commands below.

```sh
cd /workspace/3_provider
go mod tidy # download dependencies
go build -o ./bin/terraform-provider-myuserprovider

cd /workspace/2_customer
terraform plan
terraform apply -auto-approve
```

Terraform should return:

```sh
Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # myuserprovider_user.john_doe will be created
  + resource "myuserprovider_user" "john_doe" {
      + id   = "1"
      + name = "John Doe"
    }

Plan: 1 to add, 0 to change, 0 to destroy.
myuserprovider_user.john_doe: Creating...
POST:  1   John Doe
myuserprovider_user.john_doe: Creation complete after 0s [id=1]

Apply complete! Resources: 1 added, 0 changed, 0 destroyed.
```

(If you've used Terraform before and are used to running `terraform init`, that won't work with the `dev_overrides` setting. The `Init` command isn't necessary because there's no need to download any plugins.)

If you need to do any debugging while working on the provider, set the environment variable for logging in the terminal with `export TF_LOG=WARN`, and ask Terraform to write information to the terminal in your `userResource.go` with:
```go
import "github.com/hashicorp/terraform-plugin-log/tflog" // at the top
tflog.Info(ctx, "We are inside CREATE\n") // in a function
```

Notice that Terraform created `/workspace/2_customer/terraform.tfstate`. This state file holds what Terraform thinks the remote state is. Never alter this file manually. If you need to update Terraform state because you added users directly through the web service, you'll need to implement the Terraform `import` command.

Experiment to see how Terraform calls the CRUD functions depending on how you change your state. Add more users to the `main.tf` file, change their names, call `curl -X POST -d "Jane" http://localhost:6251/1` to try to confuse Terraform, and see how it handles the changes.

## Limitations and Further Reading

You're done with writing code for this guide and now have a working minimal example of a Terraform provider that you can enhance. But this provider isn't ready for production use yet. There are features you'll probably want to add, for example:

- Markup responses (JSON or XML). This simplistic web service currently returns either a 404 or a string containing a user name directly in the response body. In reality, you'll use a markup language. You may even want to have your `userResource` call a Go SDK for your service instead of making web calls directly.
- Versioning and continuous integration. Your web service will change over time. The provider will need to change to match it. Your customers will need to use the correct versions of each. You will also want to automatically build and release your provider from GitHub, using GitHub actions.
- Testing. A real web service is complex, and you will need to write a lot of integration tests to ensure that every provider version you release does exactly what it's supposed to when calling the service.
- Documentation. Your customers want to know exactly how to set up and configure your provider to manage whatever resources your service offers.
- Publishing the provider to the Terraform registry. Until you add metadata to your provider and release it in the Terraform ecosystem, no one can use it.
- You also might want to add additional functionality, like handling data sources (which are different from resources) and external imports of resources.

If you want to learn how to enhance your provider, the best place to start is the official [Terraform provider creation tutorial](https://developer.hashicorp.com/terraform/tutorials/providers-plugin-framework/providers-plugin-framework-provider). You can also clone the [provider scaffolding repository](https://github.com/hashicorp/terraform-provider-scaffolding-framework) and read through it to see how Terraform structures a provider and uses `.github` to offer continuous integration.

Once you have worked through the tutorial, we recommend reading the theory on [Terraform plugins in the documentation](https://developer.hashicorp.com/terraform/plugin). Especially promising is the 2024 HashiCorp release of an [automated provider generator](https://developer.hashicorp.com/terraform/plugin/code-generation) from an OpenAPI schema or their custom specification language. Unfortunately, the HashiCorp provider generator is not ready for production use yet — you still need to write a lot of code yourself — but it's something to watch. We have an article discussing its features [here](/post/how-to-build-terraform-providers).

## A Simpler Way

You might feel that creating and maintaining your own Terraform provider is far too much work when you're busy trying to run a business and provide your core service. Luckily, there is a much easier way. We at Speakeasy are passionate about and dedicated to making web APIs easy for customers to use. Our service can automatically generate a complete Terraform provider with documentation that's ready to offer to your customers on the Terraform registry. All you need is an OpenAPI schema for your service and a few custom attributes.

Read about how you can create a Terraform provider with us in a few clicks in this [article](/post/how-to-build-terraform-providers) and see how we can massively reduce your workload.


 This is the content for the doc blog/definitive-guide-to-devex-portals.mdx 

 ---
title: "Definitive Guide to API DevEx Portals"
description: "A definitive guide to building an API DevEx Portal for your API users."
keywords: [api, api portal, developer portal, developer experience, devex, dx, api documentation, api key management, sdk]
image: "/media/definitive-guide-to-devex-portals.png"
date: 2022-11-18
authors:
  - name: Sagar Batchu
  - image_url: 'https://uploads-ssl.webflow.com/62ccd7b208cab0723d026273/62cdf303b28bf9598d7a6b63_sagar_headshot-p-500.jpeg'
tags:
  
  - API Advice
featured_image: "/media/definitive-guide-to-devex-portals.png"
---
import { ReactPlayer } from "~/recipes/Player/ReactPlayer";

## Why You Need A Portal If You Want to Serve Developers

When you’re teaching a kid to drive, you don’t hand them a car key, the operation manual for the car and then leave them to figure it out. Of course you could, but not everyone would get the car moving, and there might be some easily avoided accidents along the way.

But this is exactly how many companies teach people to use their APIs: they hand over a stack of documentation, an API key and call it a day. Of course, those tools are important; without them, your users are stuck at ‘Go’. But these tools alone don’t really deliver any sort of experience to users. It’s basic. It’s lackluster. It’s _an_ experience, but probably not **_THE_** developer experience you want to give your users. It’s certainly not a delightful experience or memorable enough to tell others about. And that lackluster experience has a material impact on the adoption and usage of your API.

If developer experience has been underinvested in, a common set of issues begin to pop up:

- The average time gap between user signup and first successful request is longer than one day.
- Your team spends hours every week troubleshooting client integrations.
- The average number of API calls & endpoints made by users isn’t expanding over time.

Resulting in:

- Decreased API adoption and usage
- A higher cost to support for each user.
- A reduced LTV for each user.

To address these issues you need to vastly improve your API’s DevEx. **The key to a great user experience is providing your users with an API DevEx Portal** which makes your API:

- **Accessible**: there is zero friction to begin sending API requests.
- **Understandable**: users are able to get immediate feedback on their API usage – and to self-service troubleshoot issues when they do occur.
- **Usable**: It is trivially easy for users to discover and test out new use cases for your API that naturally expand their usage.

But what tooling gets you to this point? Let’s walk through each of the above criteria and discuss the specific tools that can help you give your API users the DevEx they deserve.

## What Tooling Does Your API Portal Need

### Accessible

Making the API accessible means making it as easy as possible for users to make that first API call. 99% of companies are still stuck somewhere in this first stage.

**Key Capabilities**:

- **API Documentation** - There has been a pervasive view that documentation equals  great DevEx. We think that documentation is only the first step: it’s critical, but on its own it will still leave your API users wanting. Documentation should be comprehensive, easy to navigate / search, and have code snippets / examples embedded. Ideally, docs should enable users to try the API without any additional tooling or configuration. API docs should also differentiate between API reference (a full list of all endpoints, parameters, response codes, etc.) and usage guides (tutorials that take users step-by-step through how they can use the API to accomplish key tasks).

- **Auth Login** - If you want to offer developers a more personalized experience, auth is the prerequisite. You need to know who someone is before you can issue them an API key, and start giving them tools to help them understand and track their usage. Login should of course be managed by a single shared identity service e.g. auth0 or other system of record – you don’t want to build a separate user management system for your application and your API for example.

- **API Key Management** - Nobody wants to have to fill in a typeform and wait for customer support to review before they can get started with an API. If there’s no way for a developer to create keys on their own, most will never convert into users of your product. By the time someone has reviewed their access request, they will have moved on to a new priority, or found an alternative solution. If the API interfaces with sensitive data, and a review process is a legal requirement for production credentials, then enable users to provision sandbox keys without review (more on Sandboxes below).
  
<div align="center">
<ReactPlayer controls={false} loop={true} playing={true} url='https://storage.googleapis.com/speakeasy-design-assets/videos/key-management.mp4' volume={0} />
<i>Key management in Speakeasy’s API Portal</i>
<br /><br />
</div>

### Understandable

Even companies where APIs are the primary surface area often struggle to make the investment required to advance their developer portal to being understandable.

**Key Capabilities:**

- **API Request Viewer** - When debugging, there’s no substitute for being able to step through things one at a time. A request viewer makes it possible for your users to view the full list of requests they’ve sent to your API – without creating additional work for your team to pull logs, send screenshots via email or Slack, or jump on a Zoom call. Without a self-service request viewer, broken integrations create poor API experience and leads to churned clients. A request viewer should provide users the ability to filter by time, response code, endpoint and more, and ideally allow users to edit and replay the request for quick debugging.

<div align="center">
<ReactPlayer controls={false} loop={true} playing={true} url='https://storage.googleapis.com/speakeasy-design-assets/videos/Rrequest-viewer.mp4' volume={0} />
<i>Request Viewer in Speakeasy’s API Portal</i>
<br /><br />
</div>

- ‍**API Usage Metrics** - A request viewer is only useful to developers if they know there’s an issue to investigate. That is why it’s important to surface key usage metrics in real time – so that users know the overall health of their integration. Usage metrics should place an emphasis on error reporting and significant changes in usage so that your users can take corrective action to any errors or unintended changes.

<div align="center">
<ReactPlayer controls={false} loop={true} playing={true} url='https://storage.googleapis.com/speakeasy-design-assets/videos/usage-dashboard.mp4' volume={0} />
<i>Usage Dashboard in Speakeasy’s API Portal</i>
<br /><br />
</div>

- **API Status Page** - Developers need a place to check if APIs are experiencing downtime. Nothing is more frustrating than having to email a company, “is your API working?” An API status page brings transparency, and transparency is important for building trust with your users.

### Usable

Usability tooling is focused on making it easy to test out new API use cases and also making those new use cases easy to find. Usability tooling shines as APIs become larger. Early on an API will serve a single use case, and documentation will focus on supporting that use case. As the API’s surface area grows, documentation becomes denser, and isolating relevant instructions becomes challenging. Usability tooling will help insulate users against this by providing structure for the documentation, and making it easier to test out new use cases.

**Key Capabilities:**

- **Client SDKs** - You need to meet your developers where they already are. Providing client SDKs makes it easier for developers to get started with your API by grounding them in the familiarity of their favorite language, and significantly reducing the amount of boilerplate they need to write. This is especially true if your SDKs can handle auth, pagination, and retries and others. They are therefore great at helping maximize your audience while minimizing support costs. But it’s not enough to have SDKs, it’s critical that the SDKs are developer-friendly, meaning that they are language idiomatic and human readable. Unfortunately, creating client SDKs is prohibitively expensive for most API teams, since they need to be created and updated by hand. While open source generators exist, the SDKs they output are often buggy and not ergonomic.
- **API Runbooks** - We think of runbooks as live usage guides. They take users step-by-step through the process of using your API to accomplish specific tasks, but also show relevant, live API requests in real-time. This helps to focus developers on the key use cases required to complete API integrations. Your customers can use them to grow their usage of your API. As an API builder, runbooks also help you understand the maturity of your customer base: you can begin to understand your API usage as a customer funnel, and start to measure where and why users drop out of the funnel.  
- **API Sandbox** - Probably nothing helps more with adoption than giving developers an easy way to play with your API. A sandbox can give prospective users a way to use your APIs without needing to sign up for an account. Developers are more likely to trust an API if they’ve seen it working before needing to hand over their information. And a sandbox can give existing users a way to learn by doing, and without any risk of corrupting production workflows. This enables users to easily expand their use cases for your API.

## How to get to Best-In-Class: Build or Buy?

The list above is presented as a rough roadmap. To improve your DevEx, just build each of the tools listed above in order, and you can progress from having no tooling, to having a great Developer Portal.

But as any PM or engineer will tell you, knowing what to build is only the beginning. Finding the resources required to build is the real battle. Great DevEx is extremely important for a successful API, but building all of the above is a huge commitment of resources, requires significant ongoing maintenance, and likely isn’t a core competency for your organization. As a result, investing in Developer Experience continues to be the project that is slated for next quarter.

For almost every company therefore, investing in a best-of-breed solution makes more sense. With an API DevEx Portal from a company like Speakeasy, your customers get a world-class API Developer Experience in days instead of quarters, your product roadmap has negligible impact, and your eng teams don’t need to reinvent the wheel.

Furthermore, our product is designed with maximum flexibility in mind, giving you the best of both worlds. Every tool in our API DevEx Portal is a React component, and can be customized, branded and extended as you need. Similarly, our platform can be self-hosted or run in our Speakeasy Cloud depending on your requirements.

## Final Thoughts

For a long time, companies have been able to get by with substandard developer experiences, but that is beginning to change. Developer Experience is now getting the attention it deserves, and we are rapidly reaching an inflection point. What has been previously considered great DevEx is becoming table stakes for developer products.

We know that DevEx isn’t ignored because companies don’t see the value. Rather, it’s the result of painful prioritization decisions. That’s why Speakeasy exists. We don’t want anyone to have to ever make that tradeoff. With Speakeasy you can get a best-in-class, composable developer portal up and running in a matter of minutes. If you want to learn more, [come chat with us in our Slack](https://join.slack.com/t/speakeasy-dev/shared_invite/zt-1cwb3flxz-lS5SyZxAsF_3NOq5xc8Cjw)!


 This is the content for the doc blog/design-responses.md 

 ---
title: "Designing REST APIs: Responses Your Users Expect"
description: "Practical strategies for designing efficient, informative, and user-friendly API responses."
image: "/media/design-responses.png"
date: 2024-08-06
authors:
 - name: Nolan Sullivan
 - image_url: "/media/author-headshots/nolan.jpeg"
tags:
  - API Advice
featured_image: "/media/design-responses.png"
---

This article shows you how to use your OpenAPI specification to design API responses that are efficient, informative, and user-friendly.

## User-First Design

API requests are only as useful as their responses. So, to design a useful API, you must consider how to structure responses that make sense for your users. A good user experience requires an API that either responds with appropriate data or tells users how to recover from errors.

One way to enhance user experience is to take the _design-first approach_: Write your OpenAPI specification before you code. Starting from a specification puts the user experience at the first step of the development process. The specification has well-defined properties for schemas and responses, which helps you consider how to structure your design. Designing first also helps contain development scope, since developers can code to a precise specification.

This article is a guide to using the OpenAPI specification to design responses. Regardless of whether your API returns a successful ‘200’ status code or an error with a ‘500’ status code, OpenAPI provides you with the tools and methodologies to meticulously design your API responses for optimal performance and seamless user experiences. In the following sections, we will delve into the key principles and best practices to ensure your API responses meet the highest standards of usability, efficiency, and effectiveness.

## Principles of Good API Style

Before you start writing responses, consider the following guiding principles of API design.
From data structure to error messages, these principles should inform every aspect of how you approach responses.

### Be as Explicit as Possible

Describe the data the API returns as explicitly as possible. The more explicit and precise you are, the easier it is for users and their systems to interpret returned values.

The OpenAPI Specification has many properties to describe a data field. Every field requires a defined data type. Beyond this, when possible, also provide additional information about the data format and range, for example, a string may have `minLength` and `maxLength` values. Precise and predictable information helps users handle responses.

Also be explicit in how you name your data properties. Overly general names (like `type`) describe many things. More specific names (like `userType`) immediately provide context about their data.

Explicitness also helps users interpret response descriptions. For example, `List of users` is not as helpful as `List of users with active subscriptions`.

### Be Consistent

Most schemas in OpenAPI can be referenced and reused. Reuse adds consistency and makes writing easier.

Consistent naming conventions make it easier for computers and humans to process information. Consistent response descriptions with parallel grammar and coherent phrasing make response bodies easier to read and interpret.

### Prefer Flat... Until You Need to Nest

As the [Google JSON Style Guide](https://google.github.io/styleguide/jsoncstyleguide.xml) recommends, "data should not be arbitrarily grouped." Group data in objects only when the grouping makes semantic sense and is more convenient.

Each nested object in the specification requires another level of indentation. Be mindful of this indentation – too much might indicate that it's time to refactor.

Of course, information hierarchies often do help categorize and sort information. Use your judgment to determine whether a grouping is worth the added complexity.
 

In the following sections, we'll provide illustrative descriptions of common status codes for better understanding. For the full list, review the [HTTP response status codes](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) reference. Now, let's continue with the response descriptions that will help us craft precise and informative responses in the OpenAPI specification.

1. In the response object, provide a description.

   Some descriptions are reusable for all status codes.
   Other descriptions may depend on the specific operation.

   ```yaml
   responses:
     "200":
      description: A new user was created.
   ```

   Be as specific as is reasonable.
   Often, `200` statuses use only `OK` for the description, and that might be sufficient.

1. Define content type and schema.

   In the `content` object, describe the content type and the returned schema.
   Often the schema needs only to reference a structure described in the `components` object of your specification.

   REST APIs often return JSON content, `application/json`.
   But other formats, like `text/plain` or `img/png`, are common, too.

All together, a full `responses` object looks something like this:

```yaml
paths:
  /v1/user:
    post:
      operationId: createUserv1
      summary: Create user
      ###
      responses:
        "200":
          content:
            application/json:
              schema:
                ## Reuses the User object
                $ref: "#/components/schemas/User"
          description: OK
        "400":
          description: Bad request. User name must be a string.
        default:
          ## A default response for cases that are undescribed
          $ref: "#/components/responses/default"
```

> **Note:** Each response may also include response headers providing additional context about things like rate limits.

Status, description, and content: that's all you need to describe a response.
However, the best way to structure this content is highly contextually dependent.

The rest of this article recommends how you should describe this content for successful and unsuccessful requests.

## Make Success Feel Successful

Broadly, users make requests for two reasons:

- To get resources
- To create or modify the status of a resource

So, your success responses must tell the user what succeeded and provide the resources they requested.

### Give GETs Their Resources

Users typically expect GET requests to return some data.
The structure and content of the data depend entirely on the resources the application offers.
For specifics about how to describe this data, read our
[guide to data type formats](https://speakeasyapi.dev/post/openapi-tips-data-type-formats/).

However, no matter the specifics, the response to a GET is likely either:

- An object for a specifically requested item
- An array of these items

Many, if not most, of the items returned by a GET are composed of data structures that the API reuses in other requests and responses.
To keep your interface consistent and avoid tedious repetition, describe all reusable items in `components/schemas`.
Then reference the object in your response.

For example, a GET request to a `/users/{user_id}` probably returns a single user object:

```YAML
"200":
  content:
    application/json:
      schema:
        $ref: "#/components/schemas/User"
```

A GET request to just `/users` likely returns an array of these objects.
To describe this array, your schema might reference a `Users` schema that contains an array of `user` objects.
And for list operations like this, remember to paginate.
Large response bodies can become performance bottlenecks. Even if the listed objects are few at first, the number will grow with your user base.

In this schema, pagination is described by the `offset` property.

```yaml
users:
  description: A list of users to return.
  items:
    $ref: "#/components/schemas/User"
  type: array
offset:
  type: integer
  description: The page to return
  default: 1
```

### Define Payloads for Other Methods by Use Case

For other methods, like PUT and POST, the payloads are more variable.

For example, if a POST creates an object, users may find it convenient for the API to return the created object.
However, returning the full object may not always be useful or practical, especially when there are performance constraints to consider.
In these cases, it may be sufficient to return only the newly created ID with a link, or a link to its resource (refer to the `201` status described in the subsequent section).

Similar advice applies to PUT and PATCH requests.
If returning the object isn't necessary, your response might require only a description that informs the user of the new status.

```yaml
put:
  "200":
    description: User was updated.
```

> Besides, POST requests often do more than create, since the flexible syntax lends itself to custom operations.

### Going Beyond 200

`200` is the most common success status, but not the only one.
If appropriate, consider including the following codes as responses for some operations.

- `201`: Lets users know the resource is created. Returns a link instead of an object.
- `204`: Indicates an empty response body (and informs that an empty body is expected). Some APIs use the `204` status to indicate success for operations that don't require additional information or resources. For example, a successful DELETE request operates on resources that, by definition, no longer exist when its response is sent. A `204 No Content` can indicate that the resource was successfully deleted and that the empty body is part of the defined behavior.

## Return Errors That Inform and Help Recover

By definition, errors mean the API did not return a requested resource.
If a call returns an error, tell users what happened and how they can recover.

Your specification should describe all known responses to codify errors and suggest appropriate recovery.
For some errors, a status code and terse description provide enough information.
Other errors may require different bodies for different operations.

### Reuse Descriptions for Standard Errors

Some error statuses always have the same causes and the same response bodies.
For example, `429` errors always indicate that a rate limit was reached, and `418` errors always indicate that the user should [make a cup of tea](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/418).

To minimize writing and to maintain message consistency for users,
define these responses in your `components/responses` object and reuse them across definitions.

Here's an example of a `responses` object whose properties all refer to reusable error bodies:

```yaml
responses:
  "401":
    $ref: "#/components/responses/401"
  "403":
    $ref: "#/components/responses/403"
  "404":
    $ref: "#/components/responses/404"
  "429":
    $ref: "#/components/responses/429"
```

The content for these errors is described in the `components` part of the specification:

```YAML
components:
  responses:
    "401":
      description: Unauthorized. The request did not have a valid API key.
    "403":
      description: Forbidden. This API key doesn't have necessary permissions.
    "404":
      description: Not Found. Server cannot find the requested resource.
    "429":
      description: Too Many Requests. The rate limit has been reached for this API key.
```

### Provide Details To Help Fix Bad Requests

Other status codes may need more than a generic message.
For example, requests that receive a `400 Bad request` status often have invalid fields.
For these cases, consider giving each relevant operation a unique `400` description that describes its necessary fields.

```YAML
/v1/user:
   post:
    summary: Create user
    responses:
      "400":
          description: Bad request. Operation requires valid `username` and `email` fields.
          content:
            schema:
                $ref: "#/components/schemas/FailedUserCreation"
```

A caveat for this recommendation is that the number of response bodies can be very large, especially if responses are dynamically generated based on field-validation errors.

It may be impractical to describe all error cases.
As a workaround, some specification authors write only a specific `description` (as in the preceding snippet), then refer to a generic `400` schema with placeholder values.

## Multi-Purpose Response Descriptions

When referencing a reusable component is insufficient, the OpenAPI Specification has a few features to make descriptions more flexible.
These features provide ways to join sets of schemas, define responses across a range of statuses, or provide default responses.

### Join Schemas With `allOf`

You might want to compose a response from separate schemas.
For example, a request for a resource about an administrative user might include the basic `user` object along with additional properties particular to administrators.
In this case, describe your schemas with the `allOf` operator, putting each schema as an item in an array.

```yaml
Admin:
  description: A user with admin privileges
  allOf:
  - $ref: '#/components/schemas/User'
  - type: object
    properties:
      super_admin:
        type: boolean
        description: Whether the user has super admin privileges
```

For another use of `allOf`, the API may have basic and extended error models, as given in the [example from the specification](https://spec.openapis.org/oas/latest.html#schemaObject).

### Select Schemas With `anyOf` and `oneOf`

These operators describe responses that might contain some combination of schemas.
Similar to `allOf`, these operators are defined in an array.

For example, a request to an endpoint called `/one-binary-number` might return one of two possible schemas:

```yaml
myBinarySymbol:
  oneOf:
  - $ref: '#/components/schemas/zero'
  - $ref: '#/components/schemas/one'
```

Alternatively, `anyOf` could return one or both of the preceding references.

### Status Ranges

Sometimes, a single response is enough for an entire numeric range of statuses.
For these times, use the `nXX` convention (where `n` is the number the status code starts with).

For example, you may want all 500 errors to return the same body:

```yaml
'5XX':
  description: This was our fault. Please wait a minute and try again.
```

> This `xx` description _does not_ override other defined responses for the same error class.
> So you could describe, for example, the `501` error explicitly, then use `5xx` as a catch-all for all other server-side errors.

### The Default Property

As this article has emphasized, specificity is generally preferred.
But a well-defined default can also be important.

For these times, the `responses` object also accepts a `default` property:

```yaml
paths:
  /health:
    get:
      responses:
        "200":
          description: OK
        default:
          ## reusable default
          $ref: "#/components/responses/default"

## default definition

components:
  responses:
    default:
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/Error"
      description: Default error response
```

## Conclusion

If you choose the design-first approach, the OpenAPI Specification provides a great authoring medium, with well-defined properties to structure well-defined data. It also has a robust ecosystem of tooling, which comes with its own benefits, as you can use the specification to create SDKs, contract test, mock servers, and so on.

But no matter how you write your API, the principles of design don't change. An API is as good as the value it returns. So, when you create an API, design its responses from the user's perspective.

## Further Reading

The following links are for canonical sources of information relevant to OpenAPI and HTTP responses.

- [MDN: HTTP status codes](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)
- [RFC 9110: HTTP semantics](https://www.rfc-editor.org/rfc/rfc9110.html)  
- [The OpenAPI specification](https://spec.openapis.org/oas/latest.html)


 This is the content for the doc blog/e2e-testing-arazzo/index.mdx 

 ---
title: "End-to-end API testing with Arazzo, TypeScript, and Deno"
description: "Learn how to create resilient E2E API tests using Arazzo, TypeScript, and Deno, catching issues before customers do."
image: "/media/contract-testing-with-openapi.png"
date: 2024-10-30
authors:
  - name: Brian Flad
  - image_url: "/media/author-headshots/nolan.jpeg"
tags:

featured_image: "/media/contract-testing-with-openapi.png"
---

import { ScrollyCoding } from "~/components/codehike/Scrollycoding";
import { Callout } from "~/components";

We've previously written about [the importance of building contract & integration tests](/post/contract-testing-with-openapi) to comprehensively cover your API's endpoints, but there's still a missing piece to the puzzle. Real users don't consume your API one endpoint at a time - they implement complex workflows that chain multiple API calls together.

That's why reliable **end-to-end API tests** are an important component of the testing puzzle. For your APIs most common workflows, you need to ensure that the entire process works as expected, not just the individual parts.

In this tutorial, we'll build a test generator that turns Arazzo specifications into executable end-to-end tests. You'll learn how to:

- Generate tests that mirror real user interactions with your API
- Keep tests maintainable, even as your API evolves
- Validate complex workflows across multiple API calls
- Catch integration issues before they reach production

We'll use a simple "Build-a-bot" API as our example, but the principles and code you'll learn apply to any REST API.

## Arazzo? What & Why

Arazzo is a specification that describes how API calls should be sequenced to achieve specific outcomes. Think of it as OpenAPI for workflows - while OpenAPI describes what your API can do, Arazzo describes how to use it effectively.

Arazzo was designed to bridge the gap between API reference documentation and real-world usage patterns. Fortunately for us, it also makes a perfect fit for generating end-to-end test suites that validate complete user workflows rather than isolated endpoints.

By combining these specifications, we can generate tests that validate not just the correctness of individual endpoints, but the entire user journey.

<Callout title="Arazzo?" variant="info">
  Arazzo roughly translates to "tapestry" in Italian. Get it? A tapestry of API
  calls "woven" together to create a complete user experience. We're still
  undecided about how to pronounce it, though. The leading candidates are
  "ah-RAT-so" (like fatso) and "ah-RAHT-zoh" (almost like pizza, but with a
  rat). There is a minor faction pushing for "ah-razzo" as in razzle-dazzle.
  We'll let you decide.
</Callout>

Let's look at a simplified (and mostly invalid) illustrative example. Imagine a typical e-commerce API workflow:

```yaml arazzo.yaml
arazzo: 1.0.0
workflowId: purchaseProduct
sourceDescriptions:
  - url: ./openapi.yaml
steps:
  - stepId: authenticate
    operationId: loginUser
    # post login details
    # response contains auth token
    # if successful, go to checkInventory
  - stepId: checkInventory
    operationId: getProductsStock
    # with auth token from previous step:
    # get stock levels of multiple products
    # response contains product IDs and stock levels
    # if stock levels are sufficient, go to createOrder
  - stepId: createOrder
    operationId: submitOrder
    # with auth token from first step
    # and product IDs and quantities from previous step
    # post an order that is valid based on stock levels
    # response contains order ID
    # if successful, go to getOrder
# ...
```

Arazzo allows us to define these workflows, and specify how each step should handle success and failure conditions, as well as how to pass data between steps and even between workflows.

## From specification to implementation

The example above illustrates the concept, but let's dive into a working implementation. We'll use a simplified but functional example that you can download and run yourself. Our demo implements a subset of the Arazzo specification, focusing on the most immediately valuable features for E2E testing.

We'll use the example of an API called Build-a-bot, which allows users to create and manage their own robots. You can substitute this with your own OpenAPI document, or use the Build-a-bot API to follow along.

## Arazzo deep dive

Let's examine the key components of an Arazzo specification using our Build-a-bot API example. The specification describes a workflow for creating, assembling, and activating a robot - a perfect example of a complex, multi-step process that would be difficult to test with traditional approaches.

<ScrollyCoding fullHeight>

## !!steps

The header identifies this as an Arazzo 1.0.0 specification and references the OpenAPI document. This connection between Arazzo and OpenAPI is crucial - it allows us to validate that the workflow steps match the API's capabilities.

```yaml ! arazzo.yaml focus=1:10
!from ./assets/arazzo.yaml.txt
```

---

## !!steps

Each workflow has a unique ID and can specify required inputs. In this case, we need an API key for authentication.

```yaml ! arazzo.yaml focus=12:20
!from ./assets/arazzo.yaml.txt
```

---

## !!steps

Let's look at the first step in our workflow. This step demonstrates several key Arazzo features.

```yaml ! arazzo.yaml focus=21:49
!from ./assets/arazzo.yaml.txt
```

---

## !!steps

Each step has a unique `stepId`, and a `description`. The `stepId` is particularly important, as it allows us to reference this step's outputs in subsequent steps.

```yaml ! arazzo.yaml focus=21:49 mark=22:23
!from ./assets/arazzo.yaml.txt
```

---

## !!steps

Each step also specifies an `operationId`, which corresponds to an operation in the OpenAPI document. This connection ensures that the workflow steps are always in sync with the API's capabilities.

```yaml ! arazzo.yaml
# !focus(21:49)
# !mark(24[9:32])
```

---

## !!steps

A step can define a list of `parameters` that should be passed to the operation. These parameters can be static values, references to outputs from previous steps, runtime expressions that reference many other variables, or reusable objects.

```yaml ! arazzo.yaml focus=21:49 mark=25:28
!from ./assets/arazzo.yaml.txt
```

---

## !!steps

In this example, the `x-api-key` header is required for authentication. The value for the header is gathered at runtime from the workflow's inputs, using a runtime expression: `$inputs.BUILD_A_BOT_API_KEY`. We'll explore other available expressions later.

```yaml ! arazzo.yaml
# !focus(25:28)
# !mark(28[20:46])
```

---

## !!steps

Next up, the step defines a `requestBody` object. This object specifies the request body for the operation, which is required for creating a new robot design session.

Because the request body doesn't specify a `contentType`, Arazzo will look up the `content-type` in the OpenAPI document for the operation.

```yaml ! arazzo.yaml focus=29:32
!from ./assets/arazzo.yaml.txt
```

---

## !!steps

This brings us to the meat of the step, the `successCriteria` list. This list defines the conditions that must be met for the step to be considered successful.

Each success criterion in the step serves a specific purpose.

```yaml ! arazzo.yaml focus=33:43
!from ./assets/arazzo.yaml.txt
```

---

## !!steps

First, we validate the HTTP status code. This is a basic but essential check - the robot-creation endpoint should return `201 Created`.

```yaml ! arazzo.yaml focus=34
!from ./assets/arazzo.yaml.txt
```

---

## !!steps

Next, we validate the `robotId` using a regex pattern. This ensures the ID follows the expected UUID v4 format. Notice how we use the `context` field to specify which part of the response to validate, and the `type` field to indicate we're using a regex pattern.

```yaml ! arazzo.yaml focus=35:37
!from ./assets/arazzo.yaml.txt
```

---

## !!steps

The next three criteria validate specific fields in the response body using direct comparisons. These ensure the robot is created with the correct model, name, and initial status.

```yaml ! arazzo.yaml focus=38:40
!from ./assets/arazzo.yaml.txt
```

---

## !!steps

Finally, we use JSONPath to validate the structure of the `links` array. The condition `$.length == 5` checks that exactly five links are returned.

```yaml ! arazzo.yaml focus=41:43
!from ./assets/arazzo.yaml.txt
```

---

## !!steps

After successful validation, the step defines its outputs and next action.

```yaml ! arazzo.yaml focus=44:49
!from ./assets/arazzo.yaml.txt
```

---

## !!steps

The `outputs` section extracts the `robotId` from the response body using a JSON pointer. This ID will be available to subsequent steps through the runtime expression `$steps.createRobot.outputs.robotId`.

```yaml ! arazzo.yaml
# !focus(44:49)
# !mark(49[20:42])
```

---

## !!steps

The `onSuccess` action specifies that after successful validation, the workflow should proceed to the `addParts` step. This explicit flow control helps maintain clear step sequencing.

```yaml ! arazzo.yaml focus=44:47
!from ./assets/arazzo.yaml.txt
```

</ScrollyCoding>

This combination of validation patterns, data extraction, and flow control creates the foundation for testing complex API workflows.

Each success criterion serves a specific purpose in ensuring the API behaves as expected, while the outputs and success actions enable smooth workflow progression.

We'll explore the concepts we covered in more detail in the next sections, but first, let's set up a development environment so we can see Arazzo in action.

## Setting up the development environment

First, clone the demo repository:

```bash
git clone https://github.com/speakeasy-api/e2e-testing-arazzo.git
cd e2e-testing-arazzo
```

You'll need [Deno v2](https://docs.deno.com/runtime/) installed. On macOS and Linux, you can install Deno using the following command:

```bash
curl -fsSL https://deno.land/install.sh | sh
```

The repository contains:

- A simple API server built with [@oak/acorn](https://oakserver.org/acorn) that serves as the Build-a-bot API (in `packages/server/server.ts`)
- An Arazzo specification file (`arazzo.yaml`)
- An OpenAPI specification file (`openapi.yaml`)
- The test generator implementation (`packages/arazzo-test-gen/generator.ts`)
- Generated E2E tests (`tests/generated.test.ts`)
- An SDK created by Speakeasy to interact with the Build-a-bot API (`packages/sdk`)

### Running the Demo

To run the demo, start the API server:

```bash
deno task server
```

Deno will install the server's dependencies, then start the server on `http://localhost:8080`. You can test the server by visiting `http://localhost:8080/v1/robots`, which should return a `401 Unauthorized` error:

```json
{
  "status": 401,
  "error": "Unauthorized",
  "message": "Header x-api-key is required"
}
```

Next, in a new terminal window, generate the E2E tests:

```bash
deno task dev
```

After installing dependencies, this command will generate the E2E tests in `tests/generated.test.ts` and watch for changes to the Arazzo specification file.

You can run the tests in a new terminal window:

```bash
deno task test
```

This command will run the generated tests against the API server:

```txt
> deno task test
Task test deno test --allow-read --allow-net --allow-env --unstable tests/
running 1 test from ./tests/generated.test.ts
Create, assemble, and activate a new robot ...
  Create a new robot design session ... ok (134ms)
  Add parts to the robot ... ok (2ms)
  Assemble the robot ... ok (1ms)
  Configure robot features ... ok (2ms)
  Activate the robot ... ok (3ms)
  Get the robot details ... ok (1ms)
Create, assemble, and activate a new robot ... ok (143ms)
 
ok | 1 passed (6 steps) | 0 failed (147ms)
```

Beautiful, everything works! Let's see how we got here.

## Building an Arazzo test generator

Let's start with the overall structure of the test generator.

### Project structure

The test generator is a Deno project that consists of several modules, each with a specific responsibility:

- `generator.ts`: The main entry point that orchestrates the test generation process. It reads the Arazzo and OpenAPI specifications, validates their compatibility, and generates test cases.

- `readArazzoYaml.ts` and `readOpenApiYaml.ts`: Handle parsing and validation of the Arazzo and OpenAPI specifications respectively. They ensure the specifications are well-formed and contain all required fields.

- `expressionParser.ts`: A parser for runtime expressions like `$inputs.BUILD_A_BOT_API_KEY` and `$steps.createRobot.outputs.robotId`. These expressions are crucial for passing data between steps and accessing workflow inputs.

- `successCriteria.ts`: Processes the success criteria for each step, including status code validation, regex patterns, direct comparisons, and JSONPath expressions.

- `generateTestCase.ts`: Takes the parsed workflow and generates the actual test code, including setup, execution, and validation for each step.

- `security.ts`: Handles security-related aspects like API key authentication and other security schemes defined in the OpenAPI specification.

- `utils.ts`: Contains utility functions for common operations like JSON pointer resolution and type checking.

The project also includes a `runtime-expression` directory containing the grammar definition for runtime expressions:

- `runtimeExpression.peggy`: A Peggy grammar file that defines the syntax for runtime expressions
- `runtimeExpression.js`: The generated parser from the grammar
- `runtimeExpression.d.ts`: TypeScript type definitions for the parser

Let's dive deeper into each of these components to understand how they work together to generate effective E2E tests.

### Parsing until you parse out

While our project says "test generator" on the tin, the bulk of our work will go into parsing different formats. To generate tests from an Arazzo document, we need to parse:

1. The Arazzo document
2. The OpenAPI document
3. Conditions in the Arazzo success criteria
4. Runtime expressions in the success criteria, outputs, and parameters
5. Regular expressions in the success criteria
6. JSONPath expressions in the success criteria
7. JSON pointers in the runtime expressions

We won't cover all of these in detail, but we'll touch on each to get a sense of the complexity involved and the tools we use to manage it.

### Parsing the Arazzo specification

The first step in our test generator is parsing the Arazzo specification in `readArazzoYaml.ts`. This module reads the Arazzo YAML file and should ideally validate its structure against the Arazzo specification.

For our demo, we didn't implement full validation, instead parsing the YAML file into a JavaScript object. We then use TypeScript interfaces to define the expected structure of the Arazzo document:

```typescript
export interface ArazzoDocument {
  arazzo: string;
  info: ArazzoInfo;
  sourceDescriptions: Array<ArazzoSourceDescription>;
  workflows: Array<ArazzoWorkflow>;
  components: Record<string, unknown>;
}

export interface ArazzoWorkflow {
  workflowId: string;
  description: string;
  inputs: {
    type: string;
    properties: Record<string, { type: string; description: string }>;
  };
  steps: Array<ArazzoStep>;
}

export interface ArazzoStep {
  stepId: string;
  description: string;
  operationId: string;
  parameters?: Array<ArazzoParameter>;
  requestBody?: ArazzoRequestBody;
  successCriteria: Array<ArazzoSuccessCriterion>;
  outputs?: Record<string, string>;
}
```

These TypeScript interfaces help with autocompletion, type checking, and documentation, making it easier to work with the parsed Arazzo document in the rest of our code.

The real complexity comes in validating that the parsed document follows all the rules in the Arazzo specification. For example:

- Each `workflowId` must be unique within the document
- Each `stepId` must be unique within its workflow
- An `operationId` must reference a valid operation in the OpenAPI document
- Runtime expressions must follow the correct syntax
- Success criteria must use valid JSONPath or regex patterns

We don't validate all these rules in our demo, but in production, we'd use [Zod](https://zod.dev/) or [Valibot](https://valibot.dev/) to enforce these constraints at runtime and provide helpful error messages when the document is invalid.

The OpenAPI team hasn't finalized the Arazzo specification's JSON Schema yet, but once they do, we can use it to validate the Arazzo document against the schema with tools like [Ajv](https://ajv.js.org/).

Speakeasy also provides a [command-line interface](/docs/speakeasy-cli/getting-started) for linting Arazzo documents:

```bash
# Expects arazzo.yaml in the current directory
speakeasy lint arazzo
```

### Parsing the OpenAPI specification

The OpenAPI specification's path is gathered from the Arazzo document. In our test, we simply use the first `sourceDescription` to find the OpenAPI document path. But in a production generator, we'd need to handle multiple `sourceDescriptions` and ensure the OpenAPI document is accessible.

We parse the OpenAPI document in `readOpenApiYaml.ts` and use TypeScript interfaces from the [`npm:openapi-types`](https://www.npmjs.com/package/openapi-types) package to define the expected structure of the OpenAPI document.

We won't cover the OpenAPI parsing in detail, but it's similar to the Arazzo parsing: Read the YAML file, parse it into a JavaScript object, and type check it against TypeScript interfaces.

For OpenAPI, writing a custom validator is more complex due to the specification's size and complexity. We recommend validating against the [official OpenAPI 3.1.0 JSON Schema](https://spec.openapis.org/oas/v3.1.0/schema/) using [Ajv](https://ajv.js.org/), or Speakeasy's own OpenAPI linter:

```bash
speakeasy lint openapi --schema openapi.yaml
```

### Parsing success criteria

This is where things get interesting. Success criteria in Arazzo are a list of conditions that must be met for a step to be considered successful. Each criterion can be one of the following types:

- `simple`: Selects a value with a runtime expression and compares it to an expected value
- `jsonpath`: Selects a value using a JSONPath expression and compares it to an expected value
- `regex`: Validates a value against a regular expression pattern
- `xpath`: Selects a value using an XPath expression and compares it to an expected value, used for XML documents

In our demo, we don't implement the `xpath` type, but we do cover the other three. Here's an example of a success criterion in the Arazzo document:

```yaml
successCriteria:
  - condition: $statusCode == 201
  - condition: /^[0-9A-F]{8}-[0-9A-F]{4}-[4][0-9A-F]{3}-[89AB][0-9A-F]{3}-[0-9A-F]{12}$/i
    context: $response.body#/robotId
    type: regex
  - condition: $response.body#/model == "humanoid"
  - condition: $response.body#/name == "MyFirstRobot"
  - condition: $response.body#/status == "designing"
  - context: $response.body#/links
    condition: $.length == 5
    type: jsonpath
```

The `condition` field is required, and contains the expression to evaluate, while the `context` field specifies the part of the response to evaluate. The `type` field indicates the type of validation to perform.

If no `type` is specified, the success criterion is treated as a `simple` comparison, where the `condition` is evaluated directly.

### Evaluating simple criteria

Here's an example of how we parse a `simple` success criterion:

```yaml
condition: $statusCode == 201
```

We split this simple condition into:

- Left-hand side: `$statusCode` - Runtime expression to evaluate
- Operator: `==` - Comparison operator or assertion
- Right-hand side: `201` - Expected value

We'll evaluate the runtime expression `$statusCode` and compare it to the expected value `201`. If the comparison is true, the criterion passes; otherwise, it fails.

Runtime expressions can also reference other variables, like `$inputs.BUILD_A_BOT_API_KEY` or `$steps.createRobot.outputs.robotId`, or fields in the response body, like `$response.body#/model`.

We'll cover runtime expressions in more detail later.

### Evaluating JSONPath criteria

For JSONPath criteria, we use the `jsonpath` type and a JSONPath expression to select a value from the response:

```yaml
context: $response.body#/links
condition: $.length == 5
type: jsonpath
```

Let's break down the JSONPath criterion:

- `context`: `$response.body#/links` - Runtime expression to select the `links` array from the response body
- `condition`: `$.length == 5` - JSONPath expression compared to an expected value
- `type`: `jsonpath` - Indicates the criterion type

We further need to break down the condition into:

- Left-hand side: `$.length` - JSONPath expression to evaluate
- Operator: `==` - Comparison operator
- Right-hand side: `5` - Expected value

We evaluate the JSONPath expression `$.length` and compare it to the expected value `5`. If the comparison is true, the criterion passes.

### Evaluating regex criteria

For regex criteria, we use the `regex` type and a regular expression pattern to validate a value:

```yaml
context: $response.body#/robotId
condition: /^[0-9A-F]{8}-[0-9A-F]{4}-[4][0-9A-F]{3}-[89AB][0-9A-F]{3}-[0-9A-F]{12}$/i
type: regex
```

Let's break down the regex criterion:

- `context`: `$response.body#/robotId` - Runtime expression to select the `robotId` field from the response body
- `condition`: `/^[0-9A-F]{8}-[0-9A-F]{4}-[4][0-9A-F]{3}-[89AB][0-9A-F]{3}-[0-9A-F]{12}$/i` - Regular expression pattern to validate the value as a UUID v4
- `type`: `regex` - Indicates the criterion type

We evaluate the runtime expression `$response.body#/robotId` against the regular expression pattern. If the value matches the pattern, the criterion passes.

In our implementation, we use TypeScript's `factory.createRegularExpressionLiteral` to create a regular expression literal from the pattern string. This ensures that the pattern is properly escaped and formatted as a valid JavaScript regular expression.

The generated test code looks something like this:

```typescript
assertMatch(
  response.body.robotId,
  new RegExp(/^[0-9A-F]{8}-[0-9A-F]{4}-[4][0-9A-F]{3}-[89AB][0-9A-F]{3}-[0-9A-F]{12}$/i),
  'robotId should be a valid UUID v4'
);
```

This code uses Deno's built-in `assertMatch` function to validate that the `robotId` matches the UUID v4 pattern. If the value doesn't match, the test fails with a helpful error message.

### Parsing runtime expressions

Runtime expressions are used throughout the Arazzo specification to reference variables, fields in the response body, or outputs from previous steps. They follow a specific syntax defined in the Arazzo specification as an ABNF (augmented Backus–Naur form) grammar.

To parse runtime expressions, we use a parser generated from the ABNF grammar. In our demo, this is a two-step process. First, we use the [abnf](https://www.npmjs.com/package/abnf) npm package to generate a Peggy grammar file from the ABNF grammar:

```bash
cd packages/arazzo-test-gen
abnf_gen runtime-expression/runtimeExpression.abnf
```

This generates a `runtime-expression/runtimeExpression.peggy` file that defines the syntax for runtime expressions. We then use the [peggy](https://www.npmjs.com/package/peggy) npm package to generate a parser from the Peggy grammar:

```bash
cd packages/arazzo-test-gen
peggy --dts --output runtime-expression/runtimeExpression.js --format es runtime-expression/runtimeExpression.peggy
```

This generates a `runtime-expression/runtimeExpression.js` file that contains the parser for runtime expressions. We also generate TypeScript type definitions in `runtime-expression/runtimeExpression.d.ts`.

The parser reads a runtime expression like `$response.body#/robotId` and breaks it down into tokens. We then evaluate the tokens to resolve the expression at runtime.

### Evaluating runtime expressions

Once we've parsed a runtime expression, we need to evaluate it to get the value it references. For example, given the expression `$response.body#/robotId`, we need to extract the `robotId` field from the response body.

The `evaluateRuntimeExpression` function in `utils.ts` handles this evaluation. Here's an example of how it works:

```typescript
switch (root) {
  case "$statusCode": {
    // Handle $statusCode expressions
    result = factory.createPropertyAccessExpression(
      factory.createIdentifier("response"),
      factory.createIdentifier("status"),
    );
    break;
  }
  case "$response.": {
    // Handle $request and $response expressions
    const data = factory.createIdentifier("data");
    // use parseRef to handle everything after $response.body
    const pointer = parsePointer(expression.slice(15));
    result = pointer.length > 0
      ? factory.createPropertyAccessExpression(data, pointer.join("."))
      : data;
    break;
  }
  // Handle other cases ...
}
```

Here, we handle two types of runtime expressions: `$statusCode` and `$response.body`. We extract the `status` field from the `response` object for `$statusCode`, and the `body` object from the `response` object for `$response.body`.

We use the TypeScript compiler API to generate an abstract syntax tree (AST) that represents the expression. This AST is then printed to a string and saved as a source file that Deno can execute.

### Supported runtime expressions

In our demo, we support a limited set of runtime expressions:

- `$statusCode`: The HTTP status code of the response
- `$steps.stepId.outputs.field`: The output of a previous step
- `$response.body#/path/to/field`: A field in the response body selected by a JSON pointer

Arazzo supports many more runtime expressions, for example:

| Expression                         | Description                                                         |
| ---------------------------------- | ------------------------------------------------------------------- |
| `$url`                             | The full URL of the request                                         |
| `$method`                          | The HTTP method of the request                                      |
| `$statusCode`                      | The HTTP status code of the response                                |
| `$request.header.{name}`           | The value of the specified request header                           |
| `$request.query.{name}`            | The value of the specified query parameter from the request URL     |
| `$request.path.{name}`             | The value of the specified path parameter from the request URL      |
| `$request.body`                    | The entire request body                                             |
| `$request.body#/path/to/property`  | The value of the specified JSON pointer path from the request body  |
| `$response.header.{name}`          | The value of the specified response header                          |
| `$response.body`                   | The entire response body                                            |
| `$response.body#/path/to/property` | The value of the specified JSON pointer path from the response body |
| `$inputs.{name}`                   | The value of the specified workflow input                           |
| `$outputs.{name}`                  | The value of the specified workflow output                          |
| `$steps.{stepId}.{outputName}`     | The value of the specified output from the step with ID `{stepId}`  |
| `$workflows.{id}.{inputName}`      | The value of the specified input from the workflow with ID `{id}`   |
| `$workflows.{id}.{outputName}`     | The value of the specified output from the workflow with ID `{id}`  |

### Parsing regular expressions

Regular expressions in Arazzo are used to validate string values against patterns. They're particularly useful for validating IDs, dates, and other structured strings.

In our implementation, we handle regex patterns in the `parseRegexCondition` function:

```typescript
function parseRegexCondition(
  condition: string,
  usedAssertions: Set<string>,
  context: string,
): Expression {
  usedAssertions.add("assertMatch");
  return factory.createCallExpression(
    factory.createIdentifier("assertMatch"),
    undefined,
    [
      evaluateRuntimeExpression(context),
      factory.createNewExpression(
        factory.createIdentifier("RegExp"),
        undefined,
        [factory.createRegularExpressionLiteral(condition)],
      ),
      factory.createStringLiteral(condition),
    ],
  );
}
```

This function takes three parameters:

- `condition`: The regex pattern to match against
- `usedAssertions`: A set to track which assertion functions we've used
- `context`: The runtime expression that selects the value to validate

The function generates code that:

1. Evaluates the context expression to get the value to validate
2. Creates a new RegExp object from the pattern
3. Uses Deno's `assertMatch` function to validate the value against the pattern

The generated code looks like this:

```typescript
assertMatch(
  response.body.robotId,
  new RegExp(/^[0-9A-F]{8}-[0-9A-F]{4}-[4][0-9A-F]{3}-[89AB][0-9A-F]{3}-[0-9A-F]{12}$/i),
  'robotId should be a valid UUID v4'
);
```

This approach has several advantages:

- It preserves the original pattern's flags (like `i` for case-insensitive matching).
- It provides clear error messages when validation fails.
- It integrates well with Deno's testing framework.

In a production implementation, we'd want to add:

- Validation of the regex pattern syntax
- Support for named capture groups
- Error handling for malformed patterns
- Performance optimizations like pattern caching

But for our demo, this simple implementation is sufficient to show how regex validation works in Arazzo.

### Parsing JSONPath expressions

JSONPath expressions are a powerful way to query JSON data. In Arazzo, we use them in success criteria to select objects or values from complex response structures. While JSON Pointer (which we'll cover next) is great for accessing specific values, JSONPath shines when you need to:

- Validate arrays (for example, checking array length)
- Filter elements (for example, finding items matching a condition)
- Access deeply nested data with wildcards
- Aggregate values (for example, counting matches)

Here's how our test generator handles JSONPath expressions:

```typescript
function parseJsonPathExpression(path: string, context: string): Expression {
  return factory.createCallExpression(
    factory.createIdentifier("JSONPath"),
    undefined,
    [
      factory.createObjectLiteralExpression(
        [
          factory.createPropertyAssignment(
            factory.createIdentifier("wrap"),
            factory.createFalse(),
          ),
          factory.createPropertyAssignment(
            factory.createIdentifier("path"),
            factory.createStringLiteral(path),
          ),
          factory.createPropertyAssignment(
            factory.createIdentifier("json"),
            evaluateRuntimeExpression(context),
          ),
        ],
        true,
      ),
    ],
  );
}
```

This function generates code that evaluates a JSONPath expression against a context object (usually the response body). For example, given this success criterion:

```yaml
successCriteria:
  - context: $response.body#/links
    condition: $.length == 5
    type: jsonpath
```

Our generator creates a test that:

1. Extracts the `links` array from the response body using a JSON pointer
2. Evaluates the JSONPath expression `$.length` against this array
3. Compares the result to the expected value `5`

The generated test code looks something like this:

```typescript
assertEquals(
  JSONPath({
    wrap: false,
    path: "$.length",
    json: response.body.links
  }),
  5,
  "links array should contain exactly 5 elements"
);
```

JSONPath is particularly useful for validating:

- Array operations: `$.length`, `$[0]`, `$[(@.length-1)]`
- Deep traversal: `$..name` (all name properties at any depth)
- Filtering: `$[?(@.status=="active")]` (elements where status is `active`)
- Wildcards: `$.*.name` (name property of all immediate children)

A few things to keep in mind when using JSONPath:

1. JSONPath isn't well standardized, so different implementations vary widely. Arazzo makes provisions for this by allowing us to specify the JSONPath version in the test specification.
2. Even though we can specify a version, we still need to be cautious when using advanced features. Some features might not be supported by the chosen JSONPath library.
3. Check the [JSONPath comparison](https://cburgmer.github.io/json-path-comparison/) page to see how different libraries handle various features, and decide which features are safe to use.

### Parsing JSON Pointers

While JSONPath is great for complex queries, [JSON Pointer (RFC 6901)](https://datatracker.ietf.org/doc/html/rfc6901) is perfect for directly accessing specific values in a JSON document. In Arazzo, we use JSON Pointers in runtime expressions to extract values from responses and pass them to subsequent steps.

Here's how our test generator handles JSON Pointers:

```typescript
function evaluateRuntimeExpression(expression: string): Expression {
  // ...
  case "$response.": {
    const data = factory.createIdentifier("data");
    // Parse everything after $response.body
    const pointer = parsePointer(expression.slice(15));
    result = pointer.length > 0
      ? factory.createPropertyAccessExpression(data, pointer.join("."))
      : data;
    break;
  }
  // ...
}
```

This function parses runtime expressions that use JSON Pointers. For example, given this output definition:

```yaml
outputs:
  robotId: $response.body#/robotId
```

Our generator creates code that:

1. Takes the part after `#` as the JSON Pointer (`/robotId`)
2. Converts the pointer segments into property access expressions
3. Generates code to extract the value

The generated test code looks something like this:

```typescript
// During test setup
const context = {};
// ...
// In the first test
const data = response.json();
// Generated because of outputs: { robotId: $response.body#/robotId } in the Arazzo document
// highlight-next-line
context["createRobot.outputs.robotId"] = data.robotId;
// ...
// In a subsequent test
const robotId = context["createRobot.outputs.robotId"];
const response = await fetch(`${serverUrl}/v1/robots/${robotId}/assemble`, {
  // ...
});
```

## Generating end-to-end tests

Now that we understand how to parse Arazzo documents, let's look at how we generate executable tests from them. Our generator creates type-safe test code using TypeScript's factory methods rather than string templates, providing better error detection and maintainability.

### Test structure

The generator creates a test suite for each workflow in the Arazzo document. Each step in the workflow becomes a test case that executes sequentially.

Let's explore the structure of a generated test case.

<ScrollyCoding fullHeight>

## !!steps

We start by setting up a test suite for the workflow, using the Arazzo workflow `description` as the suite name.

```typescript ! generated.test.ts focus=9
!from ./assets/generated.test.ts.txt
```

---

## !!steps

Next we define the `serverUrl`, `apiKey`, and `context` variables. The `serverUrl` points to the API server. We use the `servers` list in the OpenAPI document to determine the server URL.

We also set up the `apiKey` for authentication. In our demo, we use a hardcoded API key, but in a real-world scenario, we'd likely get this after authenticating with the API.

We'll use the `context` object to store values extracted from the response body for use in subsequent steps.

```typescript ! generated.test.ts focus=10:12
!from ./assets/generated.test.ts.txt
```

---

## !!steps

For each step in the workflow, we generate a test case that executes the step and validates the success criteria.

Our first step is to create a new robot design session.

```typescript ! generated.test.ts focus=13
!from ./assets/generated.test.ts.txt
```

---

## !!steps

The HTTP method and path are extracted from the OpenAPI document using the `operationId` from the Arazzo step.

```typescript ! generated.test.ts focus=14:15
!from ./assets/generated.test.ts.txt
```

---

## !!steps

We set up the request headers, including the `x-api-key` header for authentication.

```typescript ! generated.test.ts focus=16:19
!from ./assets/generated.test.ts.txt
```

---

## !!steps

The request body is set up using the `requestBody` object from the Arazzo step.

```typescript ! generated.test.ts focus=20
!from ./assets/generated.test.ts.txt
```

---

## !!steps

We extract the response body as JSON.

```typescript ! generated.test.ts focus=22
!from ./assets/generated.test.ts.txt
```

---

## !!steps

We assert the success criteria for the step.

```typescript ! generated.test.ts focus=23:50
!from ./assets/generated.test.ts.txt
```

---

## !!steps

Finally, we extract the outputs from the step and store them in the `context` object for use in subsequent steps.

```typescript ! generated.test.ts focus=51
!from ./assets/generated.test.ts.txt
```

</ScrollyCoding>

This structure repeats for each step in the workflow, creating a series of test cases that execute the workflow sequentially. The generated tests validate the API's behavior at each step, ensuring that the workflow progresses correctly.

## Future development and improvements

Our generated tests are a good start, but they might not be truly end-to-end if we don't consider the interfaces our users interact with to access the API.

### Testing with SDKs

In our demo, we use the `fetch` API to interact with the Build-a-bot API. While this is a common approach, it's not always the most user-friendly. Developers often prefer SDKs that provide a more idiomatic interface to the API.

To make our tests more end-to-end, we could use the SDK Speakeasy created from the OpenAPI document to interact with the API.

Since the SDK is generated from the OpenAPI document, with names and methods derived from the API's tags and operation IDs, we could use Arazzo to validate the SDK's behavior against the API's capabilities.

For example, we could:

1. Get the `operationId` from the Arazzo step and derive the corresponding SDK method import.
2. Call the SDK method with the required parameters.
3. Validate the response against the success criteria.
4. Extract the outputs from the response and store them in the `context` object.
5. Repeat for each step in the workflow.

This approach would provide a more realistic end-to-end test, validating the SDK's behavior against the API's capabilities.

### Handling authentication

In our demo, we use a hard-coded API key for authentication. In a real-world scenario, we'd likely need to authenticate with the API to get a valid API key.

OpenAPI also supports more advanced authentication schemes like OAuth 2.0, JWT, and API key in headers, query parameters, or cookies. Our test generator should handle these schemes to ensure the tests are realistic and cover all authentication scenarios.

Arazzo can point to the security schemes in the OpenAPI document, allowing us to extract the required authentication parameters and set them up in the test suite.

### Hardening the parsers against vulnerabilities

Our parsers are simple and work well for the demo, but they lack robust error handling and edge case coverage.

For example, JSONPath-plus, the library we use for JSONPath, recently fixed a remote code execution vulnerability. We should ensure our parser is up to date and secure against similar vulnerabilities, or limit the JSONPath features we support to reduce the attack surface.

This applies to parsers in general, and the risk is even higher when parsing user input and generating code from it.

Deno provides some protection by limiting access to the filesystem and network by default, but the nature of API testing means we need to access the network and read files.

## Where to next?

The Arazzo specification, although released as v1.0.0, is in active development. The OpenAPI team is working on a JSON Schema for Arazzo, which will provide a formal definition of the specification's structure and constraints.

We found the specification slightly ambiguous in places, but the team is [active on GitHub](https://github.com/OAI/Arazzo-Specification/issues) and open to feedback and contributions. If you're interested in API testing, Arazzo is a great project to get involved with.

At Speakeasy, we're building tools to make API testing easier and more effective. Our TypeScript, Python, and Go SDK generators can already generate tests from OpenAPI documents, and we're working on integrating Arazzo support. Our CLI can already lint Arazzo documents, and we'll have more to share soon.

We're excited to see how Arazzo evolves and how it can help developers build robust, end-to-end tests for their APIs.
